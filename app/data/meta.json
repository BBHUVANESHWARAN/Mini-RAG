[
  {
    "chunk_id": "cf1aad46-84ca-4d18-b1af-d0faaeeff9ce",
    "text": "# RAG-based API Service (FastAPI + LangChain + FAISS)\r\n\r\nThis repository contains a small Retrieval-Augmented Generation (RAG) API service. It supports document upload, indexing with embeddings + FAISS, and a query endpoint that retrieves relevant chunks, reranks them, runs an LLM to create an answer, and returns citations and a simple confidence indicator.\r\n\r\n> **Note:** This is a compact, educational implementation intended to demonstrate architecture and approach. It is *not* production hardened.\r\n\r\n---\r\n\r\n## Features implemented (mapped to the task)\r\n\r\n* **Document Upload** (`/upload`) — Accepts PDF / .txt / .md files, extracts text, splits into chunks, generates embeddings, and stores them in a FAISS index locally. Stores metadata (filename, chunk_id, page if PDF) in a JSON file.\r\n* *",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "eae62869-3798-42ef-bc41-da5a4e991620",
    "text": "*Query Endpoint** (`/query`) — Accepts a user query, retrieves top chunks from FAISS, reranks them by cosine similarity to the query embedding, invokes an LLM (via LangChain/ChatGoogleGenerativeAI) on the top-k chunks using a prompt template, returns: retrieved chunks, final answer, confidence score (based on similarity), and simple citations.\r\n* **Framework feature (LangChain)** — Uses LangChain's `LLMChain` and `PromptTemplate` to implement the `retriever -> reranker -> LLM -> output parser` pipeline (a simplified LCEL-like flow). The reranker is implemented as an explicit reranking step using cosine similarity of embeddings.\r\n* **Extra feature** — **Simple citations**: returned chunks include metadata and chunk IDs; the answer contains citation references.\r\n* **Minimal safety checks**:\r",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "bf8110f5-1f37-4672-8d49-6c766d62356b",
    "text": "\n\r\n  * Validates filetypes (.pdf, .txt, .md) and file size limits.\r\n  * Refuses to answer queries unrelated to the uploaded documents (checks max similarity vs threshold).\r\n  * Wraps errors and returns safe messages (no raw traceback).\r\n\r\n---\r\n\r\n## Quick repo structure\r\n\r\n```\r\nrag-api-repo/\r\n├─ README.md                # this file\r\n├─ requirements.txt\r\n├─ app/\r\n│  ├─ main.py               # FastAPI app (endpoints)\r\n│  ├─ vectorstore.py        # FAISS + embeddings store\r\n│  ├─ utils.py              # file parsing, chunking, helpers\r\n│  ├─ schemas.py            # pydantic request/response models\r\n│  └─ data/                 # persisted index and metadata (created at runtime)\r\n└─ run.sh                  # helper script to run the app locally\r\n```\r\n\r\n---\r\n\r\n## How to run (local)\r\n\r\n1. Create v",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 2
    }
  },
  {
    "chunk_id": "dce40561-344c-47a7-b8b4-248e4e8d4bcf",
    "text": "irtual env and activate it:\r\n\r\n```bash\r\npython -m venv .venv\r\nsource .venv/bin/activate   # or .venv\\Scripts\\activate on Windows\r\npip install -r requirements.txt\r\n```\r\n\r\n2. Set environment variables (OpenAI key used for LLM):\r\n\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"    # or set in your system env\r\n```\r\n\r\n3. Start the API:\r\n\r\n```bash\r\nuvicorn app.main:app --reload --port 8000\r\n```\r\n\r\n4. API endpoints:\r\n\r\n* `POST /upload` — form upload with `file` (multipart)\r\n* `POST /query` — JSON body: `{ \"query\": \"your question\", \"top_k\": 5 }`\r\n\r\n---\r\n\r\n## Requirements\r\n\r\nSee `requirements.txt` — key libraries used:\r\n\r\n* fastapi\r\n* uvicorn\r\n* langchain\r\n* openai\r\n* sentence-transformers\r\n* faiss-cpu\r\n* pypdf\r\n* python-multipart\r\n* pydantic\r\n\r\n---\r\n\r\n## Files (full source)\r\n\r\n### `requirements.txt`\r\n\r\n`",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 3
    }
  },
  {
    "chunk_id": "6db2d27c-cfdb-4435-81b9-858abcae4c4b",
    "text": "``text\r\nfastapi\r\nuvicorn[standard]\r\nlangchain\r\nopenai\r\nsentence-transformers\r\nfaiss-cpu\r\npypdf\r\npython-multipart\r\npydantic\r\ntqdm\r\n```\r\n\r\n---\r\n\r\n### `app/schemas.py`\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom typing import List, Optional\r\n\r\nclass UploadResponse(BaseModel):\r\n    success: bool\r\n    message: str\r\n    uploaded_file: Optional[str]\r\n\r\nclass RetrievedChunk(BaseModel):\r\n    chunk_id: str\r\n    text: str\r\n    filename: str\r\n    score: float\r\n\r\nclass QueryRequest(BaseModel):\r\n    query: str\r\n    top_k: int = 5\r\n\r\nclass QueryResponse(BaseModel):\r\n    answer: str\r\n    confidence: float\r\n    retrieved: List[RetrievedChunk]\r\n    safe: bool\r\n    message: Optional[str]\r\n```\r\n\r\n---\r\n\r\n### `app/utils.py`\r\n\r\n```python\r\nimport io\r\nfrom typing import List, Tuple\r\nfrom pypdf import PdfRea",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 4
    }
  },
  {
    "chunk_id": "df3c9403-bdc8-48b1-9368-8d596d598c01",
    "text": "der\r\n\r\nALLOWED = {\".pdf\", \".txt\", \".md\"}\r\n\r\n\r\ndef is_allowed(filename: str) -> bool:\r\n    return any(filename.lower().endswith(ext) for ext in ALLOWED)\r\n\r\n\r\ndef extract_text_from_pdf(file_bytes: bytes) -> List[Tuple[int, str]]:\r\n    # returns list of (page_number, text)\r\n    reader = PdfReader(io.BytesIO(file_bytes))\r\n    pages = []\r\n    for i, page in enumerate(reader.pages):\r\n        text = page.extract_text() or \"\"\r\n        pages.append((i + 1, text))\r\n    return pages\r\n\r\n\r\ndef extract_text_from_txt(file_bytes: bytes) -> List[Tuple[int, str]]:\r\n    text = file_bytes.decode(errors=\"ignore\")\r\n    return [(1, text)]\r\n\r\n\r\ndef chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:\r\n    # simple char-based chunker\r\n    chunks = []\r\n    start = 0\r\n    length = len(text)",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 5
    }
  },
  {
    "chunk_id": "a188137f-b6e0-459d-9477-dc506a2dc253",
    "text": "\r\n    while start < length:\r\n        end = min(start + chunk_size, length)\r\n        chunks.append(text[start:end])\r\n        start = max(end - overlap, end)\r\n    return chunks\r\n```\r\n\r\n---\r\n\r\n### `app/vectorstore.py`\r\n\r\n```python\r\nimport os\r\nimport json\r\nimport faiss\r\nimport numpy as np\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom typing import List, Dict, Any\r\n\r\nDATA_DIR = os.path.join(os.path.dirname(__file__), \"data\")\r\nINDEX_PATH = os.path.join(DATA_DIR, \"faiss.index\")\r\nMETA_PATH = os.path.join(DATA_DIR, \"meta.json\")\r\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\r\n\r\nos.makedirs(DATA_DIR, exist_ok=True)\r\n\r\nclass SimpleVectorStore:\r\n    def __init__(self):\r\n        self.model = SentenceTransformer(EMBED_MODEL)\r\n        self.dim = self.model.get_sentence_embedding_di",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 6
    }
  },
  {
    "chunk_id": "9b1bf67b-8097-467b-9715-3dcb58342d24",
    "text": "mension()\r\n        if os.path.exists(INDEX_PATH) and os.path.exists(META_PATH):\r\n            self.index = faiss.read_index(INDEX_PATH)\r\n            with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\r\n                self.metadata = json.load(f)\r\n        else:\r\n            self.index = faiss.IndexFlatIP(self.dim)  # cosine via normalized vectors (we'll normalize)\r\n            self.metadata = []\r\n\r\n    def _embed(self, texts: List[str]) -> np.ndarray:\r\n        embs = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\r\n        # normalize for IP-based cosine similarity\r\n        norms = np.linalg.norm(embs, axis=1, keepdims=True)\r\n        norms[norms == 0] = 1e-9\r\n        embs = embs / norms\r\n        return embs.astype('float32')\r\n\r\n    def add_documents(self, docs: List[D",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 7
    }
  },
  {
    "chunk_id": "daaad4bd-e879-4f79-bc35-5733beca003e",
    "text": "ict[str, Any]]):\r\n        # docs: list of {\"text\":..., \"metadata\":{...}, \"chunk_id\":...}\r\n        texts = [d[\"text\"] for d in docs]\r\n        embs = self._embed(texts)\r\n        if self.index.ntotal == 0:\r\n            # faiss.IndexFlatIP doesn't need training\r\n            pass\r\n        self.index.add(embs)\r\n        # store metadata aligned with index\r\n        for d in docs:\r\n            self.metadata.append({\"chunk_id\": d[\"chunk_id\"], \"text\": d[\"text\"], \"metadata\": d[\"metadata\"]})\r\n        self._persist()\r\n\r\n    def _persist(self):\r\n        faiss.write_index(self.index, INDEX_PATH)\r\n        with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\r\n            json.dump(self.metadata, f, ensure_ascii=False, indent=2)\r\n\r\n    def retrieve(self, query: str, top_k: int = 5):\r\n        q_emb = self._embed",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 8
    }
  },
  {
    "chunk_id": "579257e9-b5d1-4095-bb93-8f37312a5976",
    "text": "([query])\r\n        if self.index.ntotal == 0:\r\n            return []\r\n        scores, idxs = self.index.search(q_emb, top_k)\r\n        results = []\r\n        for score, idx in zip(scores[0], idxs[0]):\r\n            if idx < 0 or idx >= len(self.metadata):\r\n                continue\r\n            meta = self.metadata[idx]\r\n            results.append({\"chunk_id\": meta[\"chunk_id\"], \"text\": meta[\"text\"], \"metadata\": meta[\"metadata\"], \"score\": float(score)})\r\n        return results\r\n\r\n    def get_all_embeddings_for_ids(self, ids: List[int]) -> np.ndarray:\r\n        # not used often; placeholder\r\n        raise NotImplementedError\r\n\r\n    def num_docs(self):\r\n        return len(self.metadata)\r\n```\r\n\r\n---\r\n\r\n### `app/main.py`\r\n\r\n```python\r\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\r\nfro",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 9
    }
  },
  {
    "chunk_id": "e0c8ddff-e11f-44a0-8862-48c2f4f7de9f",
    "text": "m fastapi.responses import JSONResponse\r\nfrom typing import List\r\nimport uuid\r\nimport os\r\nimport traceback\r\nfrom .schemas import UploadResponse, QueryRequest, QueryResponse, RetrievedChunk\r\nfrom .utils import is_allowed, extract_text_from_pdf, extract_text_from_txt, chunk_text\r\nfrom .vectorstore import SimpleVectorStore\r\nfrom langchain import LLMChain, PromptTemplate\r\nfrom langchain.llms import OpenAI\r\nfrom pydantic import BaseModel\r\n\r\napp = FastAPI(title=\"Mini RAG API\")\r\n\r\nvs = SimpleVectorStore()\r\n\r\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB\r\n\r\n\r\n@app.post('/upload', response_model=UploadResponse)\r\nasync def upload(file: UploadFile = File(...)):\r\n    try:\r\n        filename = file.filename\r\n        if not is_allowed(filename):\r\n            raise HTTPException(status_code=400, detail=\"File ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 10
    }
  },
  {
    "chunk_id": "7b864af2-6aa8-472a-a11b-a652bd8f058b",
    "text": "type not allowed. Allowed: .pdf, .txt, .md\")\r\n        contents = await file.read()\r\n        if len(contents) > MAX_FILE_SIZE:\r\n            raise HTTPException(status_code=400, detail=\"File too large\")\r\n\r\n        docs_to_add = []\r\n        if filename.lower().endswith('.pdf'):\r\n            pages = extract_text_from_pdf(contents)\r\n            for page_no, text in pages:\r\n                if not text.strip():\r\n                    continue\r\n                chunks = chunk_text(text)\r\n                for i, c in enumerate(chunks):\r\n                    docs_to_add.append({\r\n                        \"chunk_id\": str(uuid.uuid4()),\r\n                        \"text\": c,\r\n                        \"metadata\": {\"filename\": filename, \"page\": page_no, \"chunk\": i}\r\n                    })\r\n        else:\r\n        ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 11
    }
  },
  {
    "chunk_id": "1cdf89a5-0459-4eeb-a4c7-4441b35cb0e2",
    "text": "    pages = extract_text_from_txt(contents)\r\n            for page_no, text in pages:\r\n                chunks = chunk_text(text)\r\n                for i, c in enumerate(chunks):\r\n                    docs_to_add.append({\r\n                        \"chunk_id\": str(uuid.uuid4()),\r\n                        \"text\": c,\r\n                        \"metadata\": {\"filename\": filename, \"page\": page_no, \"chunk\": i}\r\n                    })\r\n\r\n        if not docs_to_add:\r\n            return UploadResponse(success=False, message=\"No text extracted\", uploaded_file=filename)\r\n\r\n        vs.add_documents(docs_to_add)\r\n        return UploadResponse(success=True, message=f\"Indexed {len(docs_to_add)} chunks\", uploaded_file=filename)\r\n    except HTTPException as e:\r\n        raise e\r\n    except Exception as e:\r\n        #",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 12
    }
  },
  {
    "chunk_id": "e018b57d-7444-4295-a298-a5b874ecabdd",
    "text": " minimal safety: don't expose raw trace\r\n        print(\"Error in upload:\", e)\r\n        return UploadResponse(success=False, message=\"Internal server error (see logs)\", uploaded_file=None)\r\n\r\n\r\n@app.post('/query', response_model=QueryResponse)\r\nasync def query_endpoint(req: QueryRequest):\r\n    try:\r\n        # 1. retrieve\r\n        retrieved = vs.retrieve(req.query, top_k= max(5, req.top_k))\r\n\r\n        if not retrieved:\r\n            return QueryResponse(answer=\"\", confidence=0.0, retrieved=[], safe=False, message=\"No documents indexed yet\")\r\n\r\n        # 2. simple rerank by re-computing similarity (already IP scores from FAISS are cosine-like), but we re-score with the embedding model for safety\r\n        # For simplicity we reuse the indexed score, then re-sort by score desc\r\n        retrieved",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 13
    }
  },
  {
    "chunk_id": "23d0d0a9-baba-46f9-b516-d3e1e114ba51",
    "text": "_sorted = sorted(retrieved, key=lambda x: x['score'], reverse=True)[:req.top_k]\r\n\r\n        # 3. safety: if top score is below threshold, refuse\r\n        top_score = retrieved_sorted[0]['score']\r\n        CONF_THRESHOLD = 0.15  # tuned small because embeddings are normalized in [-1,1], IP in [0,1]\r\n        if top_score < CONF_THRESHOLD:\r\n            return QueryResponse(answer=\"\", confidence=top_score, retrieved=[], safe=False, message=\"Query seems unrelated to uploaded documents. Please upload relevant documents first.\")\r\n\r\n        # 4. build prompt and call LLM via LangChain\r\n        # Keep prompt small and instruct model to only use the given excerpts and to cite chunk ids.\r\n        snippets = \"\\n\\n\".join([f\"[CHUNK {r['chunk_id']}] {r['text']}\" for r in retrieved_sorted])\r\n\r\n        promp",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 14
    }
  },
  {
    "chunk_id": "6d96c2a0-6eb2-4e3d-a4a3-d6ceb506ef80",
    "text": "t_template = PromptTemplate(\r\n            input_variables=[\"snippets\", \"question\"],\r\n            template=(\"You are a helpful assistant. Use only the provided document snippets to answer the question.\\n\"\r\n                      \"If the answer is not contained in the snippets, say 'I don't know based on the provided documents.'\\n\"\r\n                      \"Cite chunks inline like (CHUNK_ID).\\n\\n\"\r\n                      \"DOCUMENTS:\\n{snippets}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\")\r\n        )\r\n\r\n        from langchain_google_genai import ChatGoogleGenerativeAI\r\n        llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\r\n        chain = LLMChain(llm=llm, prompt=prompt_template)\r\n        resp = chain.run({\"snippets\": snippets, \"question\": req.query})\r\n\r\n        # 5. Confidence: use av",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 15
    }
  },
  {
    "chunk_id": "7813957a-1c6c-4d92-b46c-4de529a32cec",
    "text": "erage of top retrieval scores as a naive confidence\r\n        avg_score = sum([r['score'] for r in retrieved_sorted]) / len(retrieved_sorted)\r\n\r\n        retrieved_out = [RetrievedChunk(chunk_id=r['chunk_id'], text=r['text'][:400], filename=r['metadata'].get('filename',''), score=r['score']) for r in retrieved_sorted]\r\n\r\n        return QueryResponse(answer=resp.strip(), confidence=float(avg_score), retrieved=retrieved_out, safe=True, message=None)\r\n    except Exception as e:\r\n        print(\"Error in query:\", e)\r\n        return QueryResponse(answer=\"\", confidence=0.0, retrieved=[], safe=False, message=\"Internal server error (see logs)\")\r\n```\r\n\r\n---\r\n\r\n## Notes on design decisions\r\n\r\n* **Embeddings**: I used `sentence-transformers/all-MiniLM-L6-v2` for local embeddings (fast, small) and FAISS ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 16
    }
  },
  {
    "chunk_id": "39b4cfbb-9efa-42d0-8af9-753461155717",
    "text": "with inner-product on normalized vectors to approximate cosine similarity.\r\n* **Reranking**: A simple reordering by similarity score is used. This keeps the pipeline explainable and fast.\r\n* **LLM**: LangChain's `OpenAI` wrapper is used. The prompt enforces using only snippets; the service additionally checks a similarity threshold to avoid answering unrelated queries.\r\n* **Persistence**: FAISS index + a `meta.json` store metadata. This is simple and sufficient for demos.\r\n* **Safety**: Tries to avoid exposing tracebacks and rejects unrelated queries.\r\n\r\n---\r\n\r\n## What to submit for the assignment\r\n\r\n* Push this repo to GitHub (create a new repo and push files).\r\n* Include a short README (this file already contains instructions).\r\n* Add a short note about functionalities (see top of this R",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 17
    }
  },
  {
    "chunk_id": "9580a893-099a-47a1-97c1-a368b5721694",
    "text": "EADME).\r\n\r\n---\r\n\r\n## Further improvements (if you'd like to expand beyond 5 hours)\r\n\r\n* Add authentication + user-scoped stores.\r\n* Use a proper DB for metadata and document ownership.\r\n* Use a more advanced reranker (cross-encoder) and/or RAG Fusion (multi-query) as the extra feature.\r\n* Add streaming of LLM tokens to the client (LangChain + server-sent events).\r\n* Add unit tests and CI.\r\n\r\n---\r\n\r\n*End of repository content.*\r\n",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 18
    }
  },
  {
    "chunk_id": "ea189d9f-87fd-4d76-af95-7fe2f3a8cbf2",
    "text": "# RAG-based API Service (FastAPI + LangChain + FAISS)\r\n\r\nThis repository contains a small Retrieval-Augmented Generation (RAG) API service. It supports document upload, indexing with embeddings + FAISS, and a query endpoint that retrieves relevant chunks, reranks them, runs an LLM to create an answer, and returns citations and a simple confidence indicator.\r\n\r\n> **Note:** This is a compact, educational implementation intended to demonstrate architecture and approach. It is *not* production hardened.\r\n\r\n---\r\n\r\n## Features implemented (mapped to the task)\r\n\r\n* **Document Upload** (`/upload`) — Accepts PDF / .txt / .md files, extracts text, splits into chunks, generates embeddings, and stores them in a FAISS index locally. Stores metadata (filename, chunk_id, page if PDF) in a JSON file.\r\n* *",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8c4fcede-d1e2-41d5-b650-1054d2ca088e",
    "text": "*Query Endpoint** (`/query`) — Accepts a user query, retrieves top chunks from FAISS, reranks them by cosine similarity to the query embedding, invokes an LLM (via LangChain/ChatGoogleGenerativeAI) on the top-k chunks using a prompt template, returns: retrieved chunks, final answer, confidence score (based on similarity), and simple citations.\r\n* **Framework feature (LangChain)** — Uses LangChain's `LLMChain` and `PromptTemplate` to implement the `retriever -> reranker -> LLM -> output parser` pipeline (a simplified LCEL-like flow). The reranker is implemented as an explicit reranking step using cosine similarity of embeddings.\r\n* **Extra feature** — **Simple citations**: returned chunks include metadata and chunk IDs; the answer contains citation references.\r\n* **Minimal safety checks**:\r",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "b34d3fa3-995d-414a-9af0-4f90301898ff",
    "text": "\n\r\n  * Validates filetypes (.pdf, .txt, .md) and file size limits.\r\n  * Refuses to answer queries unrelated to the uploaded documents (checks max similarity vs threshold).\r\n  * Wraps errors and returns safe messages (no raw traceback).\r\n\r\n---\r\n\r\n## Quick repo structure\r\n\r\n```\r\nrag-api-repo/\r\n├─ README.md                # this file\r\n├─ requirements.txt\r\n├─ app/\r\n│  ├─ main.py               # FastAPI app (endpoints)\r\n│  ├─ vectorstore.py        # FAISS + embeddings store\r\n│  ├─ utils.py              # file parsing, chunking, helpers\r\n│  ├─ schemas.py            # pydantic request/response models\r\n│  └─ data/                 # persisted index and metadata (created at runtime)\r\n└─ run.sh                  # helper script to run the app locally\r\n```\r\n\r\n---\r\n\r\n## How to run (local)\r\n\r\n1. Create v",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 2
    }
  },
  {
    "chunk_id": "d9627453-4c34-42f4-b437-e8f1e4e82e5b",
    "text": "irtual env and activate it:\r\n\r\n```bash\r\npython -m venv .venv\r\nsource .venv/bin/activate   # or .venv\\Scripts\\activate on Windows\r\npip install -r requirements.txt\r\n```\r\n\r\n2. Set environment variables (OpenAI key used for LLM):\r\n\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"    # or set in your system env\r\n```\r\n\r\n3. Start the API:\r\n\r\n```bash\r\nuvicorn app.main:app --reload --port 8000\r\n```\r\n\r\n4. API endpoints:\r\n\r\n* `POST /upload` — form upload with `file` (multipart)\r\n* `POST /query` — JSON body: `{ \"query\": \"your question\", \"top_k\": 5 }`\r\n\r\n---\r\n\r\n## Requirements\r\n\r\nSee `requirements.txt` — key libraries used:\r\n\r\n* fastapi\r\n* uvicorn\r\n* langchain\r\n* openai\r\n* sentence-transformers\r\n* faiss-cpu\r\n* pypdf\r\n* python-multipart\r\n* pydantic\r\n\r\n---\r\n\r\n## Files (full source)\r\n\r\n### `requirements.txt`\r\n\r\n`",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 3
    }
  },
  {
    "chunk_id": "5d2e4ec8-2add-46ce-abf1-357d626076ef",
    "text": "``text\r\nfastapi\r\nuvicorn[standard]\r\nlangchain\r\nopenai\r\nsentence-transformers\r\nfaiss-cpu\r\npypdf\r\npython-multipart\r\npydantic\r\ntqdm\r\n```\r\n\r\n---\r\n\r\n### `app/schemas.py`\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom typing import List, Optional\r\n\r\nclass UploadResponse(BaseModel):\r\n    success: bool\r\n    message: str\r\n    uploaded_file: Optional[str]\r\n\r\nclass RetrievedChunk(BaseModel):\r\n    chunk_id: str\r\n    text: str\r\n    filename: str\r\n    score: float\r\n\r\nclass QueryRequest(BaseModel):\r\n    query: str\r\n    top_k: int = 5\r\n\r\nclass QueryResponse(BaseModel):\r\n    answer: str\r\n    confidence: float\r\n    retrieved: List[RetrievedChunk]\r\n    safe: bool\r\n    message: Optional[str]\r\n```\r\n\r\n---\r\n\r\n### `app/utils.py`\r\n\r\n```python\r\nimport io\r\nfrom typing import List, Tuple\r\nfrom pypdf import PdfRea",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 4
    }
  },
  {
    "chunk_id": "a90f8c9c-3005-4548-9f7f-844d061ccfb1",
    "text": "der\r\n\r\nALLOWED = {\".pdf\", \".txt\", \".md\"}\r\n\r\n\r\ndef is_allowed(filename: str) -> bool:\r\n    return any(filename.lower().endswith(ext) for ext in ALLOWED)\r\n\r\n\r\ndef extract_text_from_pdf(file_bytes: bytes) -> List[Tuple[int, str]]:\r\n    # returns list of (page_number, text)\r\n    reader = PdfReader(io.BytesIO(file_bytes))\r\n    pages = []\r\n    for i, page in enumerate(reader.pages):\r\n        text = page.extract_text() or \"\"\r\n        pages.append((i + 1, text))\r\n    return pages\r\n\r\n\r\ndef extract_text_from_txt(file_bytes: bytes) -> List[Tuple[int, str]]:\r\n    text = file_bytes.decode(errors=\"ignore\")\r\n    return [(1, text)]\r\n\r\n\r\ndef chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:\r\n    # simple char-based chunker\r\n    chunks = []\r\n    start = 0\r\n    length = len(text)",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 5
    }
  },
  {
    "chunk_id": "3c7e0af5-adbf-42fd-9b0d-7deef907af88",
    "text": "\r\n    while start < length:\r\n        end = min(start + chunk_size, length)\r\n        chunks.append(text[start:end])\r\n        start = max(end - overlap, end)\r\n    return chunks\r\n```\r\n\r\n---\r\n\r\n### `app/vectorstore.py`\r\n\r\n```python\r\nimport os\r\nimport json\r\nimport faiss\r\nimport numpy as np\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom typing import List, Dict, Any\r\n\r\nDATA_DIR = os.path.join(os.path.dirname(__file__), \"data\")\r\nINDEX_PATH = os.path.join(DATA_DIR, \"faiss.index\")\r\nMETA_PATH = os.path.join(DATA_DIR, \"meta.json\")\r\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\r\n\r\nos.makedirs(DATA_DIR, exist_ok=True)\r\n\r\nclass SimpleVectorStore:\r\n    def __init__(self):\r\n        self.model = SentenceTransformer(EMBED_MODEL)\r\n        self.dim = self.model.get_sentence_embedding_di",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 6
    }
  },
  {
    "chunk_id": "9a0129ea-ed68-4c0b-b7eb-5868e698392e",
    "text": "mension()\r\n        if os.path.exists(INDEX_PATH) and os.path.exists(META_PATH):\r\n            self.index = faiss.read_index(INDEX_PATH)\r\n            with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\r\n                self.metadata = json.load(f)\r\n        else:\r\n            self.index = faiss.IndexFlatIP(self.dim)  # cosine via normalized vectors (we'll normalize)\r\n            self.metadata = []\r\n\r\n    def _embed(self, texts: List[str]) -> np.ndarray:\r\n        embs = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\r\n        # normalize for IP-based cosine similarity\r\n        norms = np.linalg.norm(embs, axis=1, keepdims=True)\r\n        norms[norms == 0] = 1e-9\r\n        embs = embs / norms\r\n        return embs.astype('float32')\r\n\r\n    def add_documents(self, docs: List[D",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 7
    }
  },
  {
    "chunk_id": "2bc94d47-95ea-4c60-8392-656e193fa7e6",
    "text": "ict[str, Any]]):\r\n        # docs: list of {\"text\":..., \"metadata\":{...}, \"chunk_id\":...}\r\n        texts = [d[\"text\"] for d in docs]\r\n        embs = self._embed(texts)\r\n        if self.index.ntotal == 0:\r\n            # faiss.IndexFlatIP doesn't need training\r\n            pass\r\n        self.index.add(embs)\r\n        # store metadata aligned with index\r\n        for d in docs:\r\n            self.metadata.append({\"chunk_id\": d[\"chunk_id\"], \"text\": d[\"text\"], \"metadata\": d[\"metadata\"]})\r\n        self._persist()\r\n\r\n    def _persist(self):\r\n        faiss.write_index(self.index, INDEX_PATH)\r\n        with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\r\n            json.dump(self.metadata, f, ensure_ascii=False, indent=2)\r\n\r\n    def retrieve(self, query: str, top_k: int = 5):\r\n        q_emb = self._embed",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 8
    }
  },
  {
    "chunk_id": "2656b0c1-6ace-4e1e-94c6-e3b320c8d5b7",
    "text": "([query])\r\n        if self.index.ntotal == 0:\r\n            return []\r\n        scores, idxs = self.index.search(q_emb, top_k)\r\n        results = []\r\n        for score, idx in zip(scores[0], idxs[0]):\r\n            if idx < 0 or idx >= len(self.metadata):\r\n                continue\r\n            meta = self.metadata[idx]\r\n            results.append({\"chunk_id\": meta[\"chunk_id\"], \"text\": meta[\"text\"], \"metadata\": meta[\"metadata\"], \"score\": float(score)})\r\n        return results\r\n\r\n    def get_all_embeddings_for_ids(self, ids: List[int]) -> np.ndarray:\r\n        # not used often; placeholder\r\n        raise NotImplementedError\r\n\r\n    def num_docs(self):\r\n        return len(self.metadata)\r\n```\r\n\r\n---\r\n\r\n### `app/main.py`\r\n\r\n```python\r\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\r\nfro",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 9
    }
  },
  {
    "chunk_id": "1af020c3-3d5d-43da-8269-96cf88dd727b",
    "text": "m fastapi.responses import JSONResponse\r\nfrom typing import List\r\nimport uuid\r\nimport os\r\nimport traceback\r\nfrom .schemas import UploadResponse, QueryRequest, QueryResponse, RetrievedChunk\r\nfrom .utils import is_allowed, extract_text_from_pdf, extract_text_from_txt, chunk_text\r\nfrom .vectorstore import SimpleVectorStore\r\nfrom langchain import LLMChain, PromptTemplate\r\nfrom langchain.llms import OpenAI\r\nfrom pydantic import BaseModel\r\n\r\napp = FastAPI(title=\"Mini RAG API\")\r\n\r\nvs = SimpleVectorStore()\r\n\r\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB\r\n\r\n\r\n@app.post('/upload', response_model=UploadResponse)\r\nasync def upload(file: UploadFile = File(...)):\r\n    try:\r\n        filename = file.filename\r\n        if not is_allowed(filename):\r\n            raise HTTPException(status_code=400, detail=\"File ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 10
    }
  },
  {
    "chunk_id": "1e698460-0d7e-4413-a715-3732fa0509d4",
    "text": "type not allowed. Allowed: .pdf, .txt, .md\")\r\n        contents = await file.read()\r\n        if len(contents) > MAX_FILE_SIZE:\r\n            raise HTTPException(status_code=400, detail=\"File too large\")\r\n\r\n        docs_to_add = []\r\n        if filename.lower().endswith('.pdf'):\r\n            pages = extract_text_from_pdf(contents)\r\n            for page_no, text in pages:\r\n                if not text.strip():\r\n                    continue\r\n                chunks = chunk_text(text)\r\n                for i, c in enumerate(chunks):\r\n                    docs_to_add.append({\r\n                        \"chunk_id\": str(uuid.uuid4()),\r\n                        \"text\": c,\r\n                        \"metadata\": {\"filename\": filename, \"page\": page_no, \"chunk\": i}\r\n                    })\r\n        else:\r\n        ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 11
    }
  },
  {
    "chunk_id": "0a5b3e33-86ae-4408-b039-6d1d36a69502",
    "text": "    pages = extract_text_from_txt(contents)\r\n            for page_no, text in pages:\r\n                chunks = chunk_text(text)\r\n                for i, c in enumerate(chunks):\r\n                    docs_to_add.append({\r\n                        \"chunk_id\": str(uuid.uuid4()),\r\n                        \"text\": c,\r\n                        \"metadata\": {\"filename\": filename, \"page\": page_no, \"chunk\": i}\r\n                    })\r\n\r\n        if not docs_to_add:\r\n            return UploadResponse(success=False, message=\"No text extracted\", uploaded_file=filename)\r\n\r\n        vs.add_documents(docs_to_add)\r\n        return UploadResponse(success=True, message=f\"Indexed {len(docs_to_add)} chunks\", uploaded_file=filename)\r\n    except HTTPException as e:\r\n        raise e\r\n    except Exception as e:\r\n        #",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 12
    }
  },
  {
    "chunk_id": "8f84dcc3-46b9-4508-baee-aeab327dcac7",
    "text": " minimal safety: don't expose raw trace\r\n        print(\"Error in upload:\", e)\r\n        return UploadResponse(success=False, message=\"Internal server error (see logs)\", uploaded_file=None)\r\n\r\n\r\n@app.post('/query', response_model=QueryResponse)\r\nasync def query_endpoint(req: QueryRequest):\r\n    try:\r\n        # 1. retrieve\r\n        retrieved = vs.retrieve(req.query, top_k= max(5, req.top_k))\r\n\r\n        if not retrieved:\r\n            return QueryResponse(answer=\"\", confidence=0.0, retrieved=[], safe=False, message=\"No documents indexed yet\")\r\n\r\n        # 2. simple rerank by re-computing similarity (already IP scores from FAISS are cosine-like), but we re-score with the embedding model for safety\r\n        # For simplicity we reuse the indexed score, then re-sort by score desc\r\n        retrieved",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 13
    }
  },
  {
    "chunk_id": "c432c0d4-ff71-4ff9-b0d7-596e2f6fc6fd",
    "text": "_sorted = sorted(retrieved, key=lambda x: x['score'], reverse=True)[:req.top_k]\r\n\r\n        # 3. safety: if top score is below threshold, refuse\r\n        top_score = retrieved_sorted[0]['score']\r\n        CONF_THRESHOLD = 0.15  # tuned small because embeddings are normalized in [-1,1], IP in [0,1]\r\n        if top_score < CONF_THRESHOLD:\r\n            return QueryResponse(answer=\"\", confidence=top_score, retrieved=[], safe=False, message=\"Query seems unrelated to uploaded documents. Please upload relevant documents first.\")\r\n\r\n        # 4. build prompt and call LLM via LangChain\r\n        # Keep prompt small and instruct model to only use the given excerpts and to cite chunk ids.\r\n        snippets = \"\\n\\n\".join([f\"[CHUNK {r['chunk_id']}] {r['text']}\" for r in retrieved_sorted])\r\n\r\n        promp",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 14
    }
  },
  {
    "chunk_id": "077837b8-4666-4ea9-8deb-b52bdef50f5e",
    "text": "t_template = PromptTemplate(\r\n            input_variables=[\"snippets\", \"question\"],\r\n            template=(\"You are a helpful assistant. Use only the provided document snippets to answer the question.\\n\"\r\n                      \"If the answer is not contained in the snippets, say 'I don't know based on the provided documents.'\\n\"\r\n                      \"Cite chunks inline like (CHUNK_ID).\\n\\n\"\r\n                      \"DOCUMENTS:\\n{snippets}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\")\r\n        )\r\n\r\n        from langchain_google_genai import ChatGoogleGenerativeAI\r\n        llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\r\n        chain = LLMChain(llm=llm, prompt=prompt_template)\r\n        resp = chain.run({\"snippets\": snippets, \"question\": req.query})\r\n\r\n        # 5. Confidence: use av",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 15
    }
  },
  {
    "chunk_id": "738ede61-e161-49be-bd96-07a7c45778e5",
    "text": "erage of top retrieval scores as a naive confidence\r\n        avg_score = sum([r['score'] for r in retrieved_sorted]) / len(retrieved_sorted)\r\n\r\n        retrieved_out = [RetrievedChunk(chunk_id=r['chunk_id'], text=r['text'][:400], filename=r['metadata'].get('filename',''), score=r['score']) for r in retrieved_sorted]\r\n\r\n        return QueryResponse(answer=resp.strip(), confidence=float(avg_score), retrieved=retrieved_out, safe=True, message=None)\r\n    except Exception as e:\r\n        print(\"Error in query:\", e)\r\n        return QueryResponse(answer=\"\", confidence=0.0, retrieved=[], safe=False, message=\"Internal server error (see logs)\")\r\n```\r\n\r\n---\r\n\r\n## Notes on design decisions\r\n\r\n* **Embeddings**: I used `sentence-transformers/all-MiniLM-L6-v2` for local embeddings (fast, small) and FAISS ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 16
    }
  },
  {
    "chunk_id": "492db996-ee43-41c0-819c-2617b4f495a0",
    "text": "with inner-product on normalized vectors to approximate cosine similarity.\r\n* **Reranking**: A simple reordering by similarity score is used. This keeps the pipeline explainable and fast.\r\n* **LLM**: LangChain's `OpenAI` wrapper is used. The prompt enforces using only snippets; the service additionally checks a similarity threshold to avoid answering unrelated queries.\r\n* **Persistence**: FAISS index + a `meta.json` store metadata. This is simple and sufficient for demos.\r\n* **Safety**: Tries to avoid exposing tracebacks and rejects unrelated queries.\r\n\r\n---\r\n\r\n## What to submit for the assignment\r\n\r\n* Push this repo to GitHub (create a new repo and push files).\r\n* Include a short README (this file already contains instructions).\r\n* Add a short note about functionalities (see top of this R",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 17
    }
  },
  {
    "chunk_id": "84a1382e-34e3-41ea-9663-1dd623653659",
    "text": "EADME).\r\n\r\n---\r\n\r\n## Further improvements (if you'd like to expand beyond 5 hours)\r\n\r\n* Add authentication + user-scoped stores.\r\n* Use a proper DB for metadata and document ownership.\r\n* Use a more advanced reranker (cross-encoder) and/or RAG Fusion (multi-query) as the extra feature.\r\n* Add streaming of LLM tokens to the client (LangChain + server-sent events).\r\n* Add unit tests and CI.\r\n\r\n---\r\n\r\n*End of repository content.*\r\n",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 18
    }
  },
  {
    "chunk_id": "d2275c07-ec63-491d-bd6d-953e474f46fe",
    "text": "# RAG-based API Service (FastAPI + LangChain + FAISS)\r\n\r\nThis repository contains a small Retrieval-Augmented Generation (RAG) API service. It supports document upload, indexing with embeddings + FAISS, and a query endpoint that retrieves relevant chunks, reranks them, runs an LLM to create an answer, and returns citations and a simple confidence indicator.\r\n\r\n> **Note:** This is a compact, educational implementation intended to demonstrate architecture and approach. It is *not* production hardened.\r\n\r\n---\r\n\r\n## Features implemented (mapped to the task)\r\n\r\n* **Document Upload** (`/upload`) — Accepts PDF / .txt / .md files, extracts text, splits into chunks, generates embeddings, and stores them in a FAISS index locally. Stores metadata (filename, chunk_id, page if PDF) in a JSON file.\r\n* *",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "10186809-e476-4ae3-9183-3d19b50a6fe8",
    "text": "*Query Endpoint** (`/query`) — Accepts a user query, retrieves top chunks from FAISS, reranks them by cosine similarity to the query embedding, invokes an LLM (via LangChain/ChatGoogleGenerativeAI) on the top-k chunks using a prompt template, returns: retrieved chunks, final answer, confidence score (based on similarity), and simple citations.\r\n* **Framework feature (LangChain)** — Uses LangChain's `LLMChain` and `PromptTemplate` to implement the `retriever -> reranker -> LLM -> output parser` pipeline (a simplified LCEL-like flow). The reranker is implemented as an explicit reranking step using cosine similarity of embeddings.\r\n* **Extra feature** — **Simple citations**: returned chunks include metadata and chunk IDs; the answer contains citation references.\r\n* **Minimal safety checks**:\r",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "a42c0f1b-052a-4cb7-93c4-b44561c67879",
    "text": "\n\r\n  * Validates filetypes (.pdf, .txt, .md) and file size limits.\r\n  * Refuses to answer queries unrelated to the uploaded documents (checks max similarity vs threshold).\r\n  * Wraps errors and returns safe messages (no raw traceback).\r\n\r\n---\r\n\r\n## Quick repo structure\r\n\r\n```\r\nrag-api-repo/\r\n├─ README.md                # this file\r\n├─ requirements.txt\r\n├─ app/\r\n│  ├─ main.py               # FastAPI app (endpoints)\r\n│  ├─ vectorstore.py        # FAISS + embeddings store\r\n│  ├─ utils.py              # file parsing, chunking, helpers\r\n│  ├─ schemas.py            # pydantic request/response models\r\n│  └─ data/                 # persisted index and metadata (created at runtime)\r\n└─ run.sh                  # helper script to run the app locally\r\n```\r\n\r\n---\r\n\r\n## How to run (local)\r\n\r\n1. Create v",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 2
    }
  },
  {
    "chunk_id": "8b376c8a-3bed-4dc3-86c3-eddb6783d453",
    "text": "irtual env and activate it:\r\n\r\n```bash\r\npython -m venv .venv\r\nsource .venv/bin/activate   # or .venv\\Scripts\\activate on Windows\r\npip install -r requirements.txt\r\n```\r\n\r\n2. Set environment variables (OpenAI key used for LLM):\r\n\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"    # or set in your system env\r\n```\r\n\r\n3. Start the API:\r\n\r\n```bash\r\nuvicorn app.main:app --reload --port 8000\r\n```\r\n\r\n4. API endpoints:\r\n\r\n* `POST /upload` — form upload with `file` (multipart)\r\n* `POST /query` — JSON body: `{ \"query\": \"your question\", \"top_k\": 5 }`\r\n\r\n---\r\n\r\n## Requirements\r\n\r\nSee `requirements.txt` — key libraries used:\r\n\r\n* fastapi\r\n* uvicorn\r\n* langchain\r\n* openai\r\n* sentence-transformers\r\n* faiss-cpu\r\n* pypdf\r\n* python-multipart\r\n* pydantic\r\n\r\n---\r\n\r\n## Files (full source)\r\n\r\n### `requirements.txt`\r\n\r\n`",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 3
    }
  },
  {
    "chunk_id": "1774c623-03b6-44aa-8285-436314e925cb",
    "text": "``text\r\nfastapi\r\nuvicorn[standard]\r\nlangchain\r\nopenai\r\nsentence-transformers\r\nfaiss-cpu\r\npypdf\r\npython-multipart\r\npydantic\r\ntqdm\r\n```\r\n\r\n---\r\n\r\n### `app/schemas.py`\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom typing import List, Optional\r\n\r\nclass UploadResponse(BaseModel):\r\n    success: bool\r\n    message: str\r\n    uploaded_file: Optional[str]\r\n\r\nclass RetrievedChunk(BaseModel):\r\n    chunk_id: str\r\n    text: str\r\n    filename: str\r\n    score: float\r\n\r\nclass QueryRequest(BaseModel):\r\n    query: str\r\n    top_k: int = 5\r\n\r\nclass QueryResponse(BaseModel):\r\n    answer: str\r\n    confidence: float\r\n    retrieved: List[RetrievedChunk]\r\n    safe: bool\r\n    message: Optional[str]\r\n```\r\n\r\n---\r\n\r\n### `app/utils.py`\r\n\r\n```python\r\nimport io\r\nfrom typing import List, Tuple\r\nfrom pypdf import PdfRea",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 4
    }
  },
  {
    "chunk_id": "fd6b655d-0839-4300-b691-ebcbe312c3a3",
    "text": "der\r\n\r\nALLOWED = {\".pdf\", \".txt\", \".md\"}\r\n\r\n\r\ndef is_allowed(filename: str) -> bool:\r\n    return any(filename.lower().endswith(ext) for ext in ALLOWED)\r\n\r\n\r\ndef extract_text_from_pdf(file_bytes: bytes) -> List[Tuple[int, str]]:\r\n    # returns list of (page_number, text)\r\n    reader = PdfReader(io.BytesIO(file_bytes))\r\n    pages = []\r\n    for i, page in enumerate(reader.pages):\r\n        text = page.extract_text() or \"\"\r\n        pages.append((i + 1, text))\r\n    return pages\r\n\r\n\r\ndef extract_text_from_txt(file_bytes: bytes) -> List[Tuple[int, str]]:\r\n    text = file_bytes.decode(errors=\"ignore\")\r\n    return [(1, text)]\r\n\r\n\r\ndef chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:\r\n    # simple char-based chunker\r\n    chunks = []\r\n    start = 0\r\n    length = len(text)",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 5
    }
  },
  {
    "chunk_id": "b996fc8f-b0da-4de3-80d9-04eac2cdf1ee",
    "text": "\r\n    while start < length:\r\n        end = min(start + chunk_size, length)\r\n        chunks.append(text[start:end])\r\n        start = max(end - overlap, end)\r\n    return chunks\r\n```\r\n\r\n---\r\n\r\n### `app/vectorstore.py`\r\n\r\n```python\r\nimport os\r\nimport json\r\nimport faiss\r\nimport numpy as np\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom typing import List, Dict, Any\r\n\r\nDATA_DIR = os.path.join(os.path.dirname(__file__), \"data\")\r\nINDEX_PATH = os.path.join(DATA_DIR, \"faiss.index\")\r\nMETA_PATH = os.path.join(DATA_DIR, \"meta.json\")\r\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\r\n\r\nos.makedirs(DATA_DIR, exist_ok=True)\r\n\r\nclass SimpleVectorStore:\r\n    def __init__(self):\r\n        self.model = SentenceTransformer(EMBED_MODEL)\r\n        self.dim = self.model.get_sentence_embedding_di",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 6
    }
  },
  {
    "chunk_id": "c3d4f81a-0968-4f59-9373-c670886ca015",
    "text": "mension()\r\n        if os.path.exists(INDEX_PATH) and os.path.exists(META_PATH):\r\n            self.index = faiss.read_index(INDEX_PATH)\r\n            with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\r\n                self.metadata = json.load(f)\r\n        else:\r\n            self.index = faiss.IndexFlatIP(self.dim)  # cosine via normalized vectors (we'll normalize)\r\n            self.metadata = []\r\n\r\n    def _embed(self, texts: List[str]) -> np.ndarray:\r\n        embs = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\r\n        # normalize for IP-based cosine similarity\r\n        norms = np.linalg.norm(embs, axis=1, keepdims=True)\r\n        norms[norms == 0] = 1e-9\r\n        embs = embs / norms\r\n        return embs.astype('float32')\r\n\r\n    def add_documents(self, docs: List[D",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 7
    }
  },
  {
    "chunk_id": "786022c0-8e98-4264-b049-720c1952a456",
    "text": "ict[str, Any]]):\r\n        # docs: list of {\"text\":..., \"metadata\":{...}, \"chunk_id\":...}\r\n        texts = [d[\"text\"] for d in docs]\r\n        embs = self._embed(texts)\r\n        if self.index.ntotal == 0:\r\n            # faiss.IndexFlatIP doesn't need training\r\n            pass\r\n        self.index.add(embs)\r\n        # store metadata aligned with index\r\n        for d in docs:\r\n            self.metadata.append({\"chunk_id\": d[\"chunk_id\"], \"text\": d[\"text\"], \"metadata\": d[\"metadata\"]})\r\n        self._persist()\r\n\r\n    def _persist(self):\r\n        faiss.write_index(self.index, INDEX_PATH)\r\n        with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\r\n            json.dump(self.metadata, f, ensure_ascii=False, indent=2)\r\n\r\n    def retrieve(self, query: str, top_k: int = 5):\r\n        q_emb = self._embed",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 8
    }
  },
  {
    "chunk_id": "0bc5e91e-673e-4dd3-8833-555c54edd98d",
    "text": "([query])\r\n        if self.index.ntotal == 0:\r\n            return []\r\n        scores, idxs = self.index.search(q_emb, top_k)\r\n        results = []\r\n        for score, idx in zip(scores[0], idxs[0]):\r\n            if idx < 0 or idx >= len(self.metadata):\r\n                continue\r\n            meta = self.metadata[idx]\r\n            results.append({\"chunk_id\": meta[\"chunk_id\"], \"text\": meta[\"text\"], \"metadata\": meta[\"metadata\"], \"score\": float(score)})\r\n        return results\r\n\r\n    def get_all_embeddings_for_ids(self, ids: List[int]) -> np.ndarray:\r\n        # not used often; placeholder\r\n        raise NotImplementedError\r\n\r\n    def num_docs(self):\r\n        return len(self.metadata)\r\n```\r\n\r\n---\r\n\r\n### `app/main.py`\r\n\r\n```python\r\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\r\nfro",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 9
    }
  },
  {
    "chunk_id": "f894f525-1393-48af-bc04-e9d63fd22034",
    "text": "m fastapi.responses import JSONResponse\r\nfrom typing import List\r\nimport uuid\r\nimport os\r\nimport traceback\r\nfrom .schemas import UploadResponse, QueryRequest, QueryResponse, RetrievedChunk\r\nfrom .utils import is_allowed, extract_text_from_pdf, extract_text_from_txt, chunk_text\r\nfrom .vectorstore import SimpleVectorStore\r\nfrom langchain import LLMChain, PromptTemplate\r\nfrom langchain.llms import OpenAI\r\nfrom pydantic import BaseModel\r\n\r\napp = FastAPI(title=\"Mini RAG API\")\r\n\r\nvs = SimpleVectorStore()\r\n\r\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB\r\n\r\n\r\n@app.post('/upload', response_model=UploadResponse)\r\nasync def upload(file: UploadFile = File(...)):\r\n    try:\r\n        filename = file.filename\r\n        if not is_allowed(filename):\r\n            raise HTTPException(status_code=400, detail=\"File ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 10
    }
  },
  {
    "chunk_id": "cda6a9ed-f85a-4c55-bd28-616a3883296f",
    "text": "type not allowed. Allowed: .pdf, .txt, .md\")\r\n        contents = await file.read()\r\n        if len(contents) > MAX_FILE_SIZE:\r\n            raise HTTPException(status_code=400, detail=\"File too large\")\r\n\r\n        docs_to_add = []\r\n        if filename.lower().endswith('.pdf'):\r\n            pages = extract_text_from_pdf(contents)\r\n            for page_no, text in pages:\r\n                if not text.strip():\r\n                    continue\r\n                chunks = chunk_text(text)\r\n                for i, c in enumerate(chunks):\r\n                    docs_to_add.append({\r\n                        \"chunk_id\": str(uuid.uuid4()),\r\n                        \"text\": c,\r\n                        \"metadata\": {\"filename\": filename, \"page\": page_no, \"chunk\": i}\r\n                    })\r\n        else:\r\n        ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 11
    }
  },
  {
    "chunk_id": "023a97fd-e2d5-4cfc-bf8f-cf51da3f0223",
    "text": "    pages = extract_text_from_txt(contents)\r\n            for page_no, text in pages:\r\n                chunks = chunk_text(text)\r\n                for i, c in enumerate(chunks):\r\n                    docs_to_add.append({\r\n                        \"chunk_id\": str(uuid.uuid4()),\r\n                        \"text\": c,\r\n                        \"metadata\": {\"filename\": filename, \"page\": page_no, \"chunk\": i}\r\n                    })\r\n\r\n        if not docs_to_add:\r\n            return UploadResponse(success=False, message=\"No text extracted\", uploaded_file=filename)\r\n\r\n        vs.add_documents(docs_to_add)\r\n        return UploadResponse(success=True, message=f\"Indexed {len(docs_to_add)} chunks\", uploaded_file=filename)\r\n    except HTTPException as e:\r\n        raise e\r\n    except Exception as e:\r\n        #",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 12
    }
  },
  {
    "chunk_id": "f787081a-12c7-460e-86b4-7773ce0b888e",
    "text": " minimal safety: don't expose raw trace\r\n        print(\"Error in upload:\", e)\r\n        return UploadResponse(success=False, message=\"Internal server error (see logs)\", uploaded_file=None)\r\n\r\n\r\n@app.post('/query', response_model=QueryResponse)\r\nasync def query_endpoint(req: QueryRequest):\r\n    try:\r\n        # 1. retrieve\r\n        retrieved = vs.retrieve(req.query, top_k= max(5, req.top_k))\r\n\r\n        if not retrieved:\r\n            return QueryResponse(answer=\"\", confidence=0.0, retrieved=[], safe=False, message=\"No documents indexed yet\")\r\n\r\n        # 2. simple rerank by re-computing similarity (already IP scores from FAISS are cosine-like), but we re-score with the embedding model for safety\r\n        # For simplicity we reuse the indexed score, then re-sort by score desc\r\n        retrieved",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 13
    }
  },
  {
    "chunk_id": "53891a04-8cee-4b90-8709-eb7fcec37b0a",
    "text": "_sorted = sorted(retrieved, key=lambda x: x['score'], reverse=True)[:req.top_k]\r\n\r\n        # 3. safety: if top score is below threshold, refuse\r\n        top_score = retrieved_sorted[0]['score']\r\n        CONF_THRESHOLD = 0.15  # tuned small because embeddings are normalized in [-1,1], IP in [0,1]\r\n        if top_score < CONF_THRESHOLD:\r\n            return QueryResponse(answer=\"\", confidence=top_score, retrieved=[], safe=False, message=\"Query seems unrelated to uploaded documents. Please upload relevant documents first.\")\r\n\r\n        # 4. build prompt and call LLM via LangChain\r\n        # Keep prompt small and instruct model to only use the given excerpts and to cite chunk ids.\r\n        snippets = \"\\n\\n\".join([f\"[CHUNK {r['chunk_id']}] {r['text']}\" for r in retrieved_sorted])\r\n\r\n        promp",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 14
    }
  },
  {
    "chunk_id": "23ce3afd-4039-4149-b5ad-ea63a5e33469",
    "text": "t_template = PromptTemplate(\r\n            input_variables=[\"snippets\", \"question\"],\r\n            template=(\"You are a helpful assistant. Use only the provided document snippets to answer the question.\\n\"\r\n                      \"If the answer is not contained in the snippets, say 'I don't know based on the provided documents.'\\n\"\r\n                      \"Cite chunks inline like (CHUNK_ID).\\n\\n\"\r\n                      \"DOCUMENTS:\\n{snippets}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\")\r\n        )\r\n\r\n        from langchain_google_genai import ChatGoogleGenerativeAI\r\n        llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\r\n        chain = LLMChain(llm=llm, prompt=prompt_template)\r\n        resp = chain.run({\"snippets\": snippets, \"question\": req.query})\r\n\r\n        # 5. Confidence: use av",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 15
    }
  },
  {
    "chunk_id": "772e1e0c-84ee-49e3-af65-98350d8503ca",
    "text": "erage of top retrieval scores as a naive confidence\r\n        avg_score = sum([r['score'] for r in retrieved_sorted]) / len(retrieved_sorted)\r\n\r\n        retrieved_out = [RetrievedChunk(chunk_id=r['chunk_id'], text=r['text'][:400], filename=r['metadata'].get('filename',''), score=r['score']) for r in retrieved_sorted]\r\n\r\n        return QueryResponse(answer=resp.strip(), confidence=float(avg_score), retrieved=retrieved_out, safe=True, message=None)\r\n    except Exception as e:\r\n        print(\"Error in query:\", e)\r\n        return QueryResponse(answer=\"\", confidence=0.0, retrieved=[], safe=False, message=\"Internal server error (see logs)\")\r\n```\r\n\r\n---\r\n\r\n## Notes on design decisions\r\n\r\n* **Embeddings**: I used `sentence-transformers/all-MiniLM-L6-v2` for local embeddings (fast, small) and FAISS ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 16
    }
  },
  {
    "chunk_id": "228b93f2-9810-4ede-a0ef-61518181ffde",
    "text": "with inner-product on normalized vectors to approximate cosine similarity.\r\n* **Reranking**: A simple reordering by similarity score is used. This keeps the pipeline explainable and fast.\r\n* **LLM**: LangChain's `OpenAI` wrapper is used. The prompt enforces using only snippets; the service additionally checks a similarity threshold to avoid answering unrelated queries.\r\n* **Persistence**: FAISS index + a `meta.json` store metadata. This is simple and sufficient for demos.\r\n* **Safety**: Tries to avoid exposing tracebacks and rejects unrelated queries.\r\n\r\n---\r\n\r\n## What to submit for the assignment\r\n\r\n* Push this repo to GitHub (create a new repo and push files).\r\n* Include a short README (this file already contains instructions).\r\n* Add a short note about functionalities (see top of this R",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 17
    }
  },
  {
    "chunk_id": "9a492bad-1bb2-4bc7-a886-e6508ea4cad7",
    "text": "EADME).\r\n\r\n---\r\n\r\n## Further improvements (if you'd like to expand beyond 5 hours)\r\n\r\n* Add authentication + user-scoped stores.\r\n* Use a proper DB for metadata and document ownership.\r\n* Use a more advanced reranker (cross-encoder) and/or RAG Fusion (multi-query) as the extra feature.\r\n* Add streaming of LLM tokens to the client (LangChain + server-sent events).\r\n* Add unit tests and CI.\r\n\r\n---\r\n\r\n*End of repository content.*\r\n",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 18
    }
  },
  {
    "chunk_id": "7a8f695f-db34-4435-98a3-3811d67a2750",
    "text": "The LangChain Expression Language (LCEL) provides a declarative and composable way to build complex workflows within the LangChain framework. It allows for the chaining of different components, such as prompt templates, language models, retrievers, and output parsers, using a simple and intuitive pipe operator (|). \r\nKey features and concepts of LCEL:\r\nDeclarative Syntax: LCEL focuses on describing what you want to achieve rather than how to achieve it, abstracting away the underlying execution details.\r\nComposability: LCEL enables the combination of small, reusable components (called \"Runnables\") into sophisticated pipelines, promoting modularity and code reusability.\r\nPipe Operator (|): This operator is central to LCEL, chaining components together. The output of the component on the lef",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "2d8e8a57-0bf0-4911-acc2-91d6b90e55d8",
    "text": "t side of the pipe becomes the input for the component on the right.\r\nRunnables: At the core of LCEL are Runnables, which are modular components encapsulating functions or operations. Any two Runnables can be combined using the pipe operator. \r\nLazy Evaluation: Chains defined with LCEL are not executed until a method like .invoke(), .stream(), or .batch() is called, allowing for efficient resource management.\r\nRuntime Flexibility: LCEL pipelines can seamlessly operate in synchronous, asynchronous, streaming, and batch modes.\r\nEnhanced Readability and Maintainability: The visual, left-to-right flow of LCEL chains, similar to shell pipelines, improves code readability and makes it easier to understand and maintain complex workflows.",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "87c08d5d-b5d4-4b3c-8a8e-ff549e714f03",
    "text": "The LangChain Expression Language (LCEL) provides a declarative and composable way to build complex workflows within the LangChain framework. It allows for the chaining of different components, such as prompt templates, language models, retrievers, and output parsers, using a simple and intuitive pipe operator (|). \r\nKey features and concepts of LCEL:\r\nDeclarative Syntax: LCEL focuses on describing what you want to achieve rather than how to achieve it, abstracting away the underlying execution details.\r\nComposability: LCEL enables the combination of small, reusable components (called \"Runnables\") into sophisticated pipelines, promoting modularity and code reusability.\r\nPipe Operator (|): This operator is central to LCEL, chaining components together. The output of the component on the lef",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "492eebd7-8204-40c8-8ea7-e19d23da51f1",
    "text": "t side of the pipe becomes the input for the component on the right.\r\nRunnables: At the core of LCEL are Runnables, which are modular components encapsulating functions or operations. Any two Runnables can be combined using the pipe operator. \r\nLazy Evaluation: Chains defined with LCEL are not executed until a method like .invoke(), .stream(), or .batch() is called, allowing for efficient resource management.\r\nRuntime Flexibility: LCEL pipelines can seamlessly operate in synchronous, asynchronous, streaming, and batch modes.\r\nEnhanced Readability and Maintainability: The visual, left-to-right flow of LCEL chains, similar to shell pipelines, improves code readability and makes it easier to understand and maintain complex workflows.",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7975f2cd-99b1-4880-a2f3-c2c83ff34a8e",
    "text": "The LangChain Expression Language (LCEL) provides a declarative and composable way to build complex workflows within the LangChain framework. It allows for the chaining of different components, such as prompt templates, language models, retrievers, and output parsers, using a simple and intuitive pipe operator (|). \r\nKey features and concepts of LCEL:\r\nDeclarative Syntax: LCEL focuses on describing what you want to achieve rather than how to achieve it, abstracting away the underlying execution details.\r\nComposability: LCEL enables the combination of small, reusable components (called \"Runnables\") into sophisticated pipelines, promoting modularity and code reusability.\r\nPipe Operator (|): This operator is central to LCEL, chaining components together. The output of the component on the lef",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "752f9240-eb5b-47bb-9284-666d63f32a24",
    "text": "t side of the pipe becomes the input for the component on the right.\r\nRunnables: At the core of LCEL are Runnables, which are modular components encapsulating functions or operations. Any two Runnables can be combined using the pipe operator. \r\nLazy Evaluation: Chains defined with LCEL are not executed until a method like .invoke(), .stream(), or .batch() is called, allowing for efficient resource management.\r\nRuntime Flexibility: LCEL pipelines can seamlessly operate in synchronous, asynchronous, streaming, and batch modes.\r\nEnhanced Readability and Maintainability: The visual, left-to-right flow of LCEL chains, similar to shell pipelines, improves code readability and makes it easier to understand and maintain complex workflows.",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "984d8f37-da50-4a52-af7f-12303527d213",
    "text": "The LangChain Expression Language (LCEL) provides a declarative and composable way to build complex workflows within the LangChain framework. It allows for the chaining of different components, such as prompt templates, language models, retrievers, and output parsers, using a simple and intuitive pipe operator (|). \r\nKey features and concepts of LCEL:\r\nDeclarative Syntax: LCEL focuses on describing what you want to achieve rather than how to achieve it, abstracting away the underlying execution details.\r\nComposability: LCEL enables the combination of small, reusable components (called \"Runnables\") into sophisticated pipelines, promoting modularity and code reusability.\r\nPipe Operator (|): This operator is central to LCEL, chaining components together. The output of the component on the lef",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f2d41e62-43aa-4fd0-a403-68f7be98dae3",
    "text": "t side of the pipe becomes the input for the component on the right.\r\nRunnables: At the core of LCEL are Runnables, which are modular components encapsulating functions or operations. Any two Runnables can be combined using the pipe operator. \r\nLazy Evaluation: Chains defined with LCEL are not executed until a method like .invoke(), .stream(), or .batch() is called, allowing for efficient resource management.\r\nRuntime Flexibility: LCEL pipelines can seamlessly operate in synchronous, asynchronous, streaming, and batch modes.\r\nEnhanced Readability and Maintainability: The visual, left-to-right flow of LCEL chains, similar to shell pipelines, improves code readability and makes it easier to understand and maintain complex workflows.    ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "075b3f87-90ea-40ba-9588-ecc3f3b2ad9a",
    "text": "The LangChain Expression Language (LCEL) provides a declarative and composable way to build complex workflows within the LangChain framework. It allows for the chaining of different components, such as prompt templates, language models, retrievers, and output parsers, using a simple and intuitive pipe operator (|). \r\nKey features and concepts of LCEL:\r\nDeclarative Syntax: LCEL focuses on describing what you want to achieve rather than how to achieve it, abstracting away the underlying execution details.\r\nComposability: LCEL enables the combination of small, reusable components (called \"Runnables\") into sophisticated pipelines, promoting modularity and code reusability.\r\nPipe Operator (|): This operator is central to LCEL, chaining components together. The output of the component on the lef",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cefc8da0-128e-4a56-91c6-788fd090846a",
    "text": "t side of the pipe becomes the input for the component on the right.\r\nRunnables: At the core of LCEL are Runnables, which are modular components encapsulating functions or operations. Any two Runnables can be combined using the pipe operator. \r\nLazy Evaluation: Chains defined with LCEL are not executed until a method like .invoke(), .stream(), or .batch() is called, allowing for efficient resource management.\r\nRuntime Flexibility: LCEL pipelines can seamlessly operate in synchronous, asynchronous, streaming, and batch modes.\r\nEnhanced Readability and Maintainability: The visual, left-to-right flow of LCEL chains, similar to shell pipelines, improves code readability and makes it easier to understand and maintain complex workflows.    ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "cc97b50c-6367-403c-a6bd-10dbd356cd49",
    "text": "The LangChain Expression Language (LCEL) provides a declarative and composable way to build complex workflows within the LangChain framework. It allows for the chaining of different components, such as prompt templates, language models, retrievers, and output parsers, using a simple and intuitive pipe operator (|). \r\nKey features and concepts of LCEL:\r\nDeclarative Syntax: LCEL focuses on describing what you want to achieve rather than how to achieve it, abstracting away the underlying execution details.\r\nComposability: LCEL enables the combination of small, reusable components (called \"Runnables\") into sophisticated pipelines, promoting modularity and code reusability.\r\nPipe Operator (|): This operator is central to LCEL, chaining components together. The output of the component on the lef",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cb69e654-a005-4adb-a015-31ae6ea7a3fb",
    "text": "t side of the pipe becomes the input for the component on the right.\r\nRunnables: At the core of LCEL are Runnables, which are modular components encapsulating functions or operations. Any two Runnables can be combined using the pipe operator. \r\nLazy Evaluation: Chains defined with LCEL are not executed until a method like .invoke(), .stream(), or .batch() is called, allowing for efficient resource management.\r\nRuntime Flexibility: LCEL pipelines can seamlessly operate in synchronous, asynchronous, streaming, and batch modes.\r\nEnhanced Readability and Maintainability: The visual, left-to-right flow of LCEL chains, similar to shell pipelines, improves code readability and makes it easier to understand and maintain complex workflows.    ",
    "metadata": {
      "filename": "test_doc.txt",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "a1d4babc-cfa9-4ec0-b62a-3afd7ca1f4e2",
    "text": "AI/ML\nCheatSheet",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d5f50715-7c1b-4508-bd7f-d19e05838199",
    "text": "1. Introduction to AI and ML\nWhat is Artificial Intelligence (AI)?\nTypes of AI: Narrow AI vs. General AI\nWhat is Machine Learning (ML)?\nSupervised vs. Unsupervised vs. Reinforcement Learning\nKey Terminologies (Model, Feature, Target, Algorithm)\nApplications of AI and ML\nDifference Between AI, ML, and Deep Learning (DL)\n2. Mathematics for ML/AI\nLinear Algebra\nVectors, Matrices, and Tensors\nMatrix Operations\nEigenvalues and Eigenvectors\nProbability and Statistics\nMean, Variance, Standard Deviation\nBayes Theorem\nConditional Probability\nProbability Distributions (Normal, Binomial, Poisson)\nCalculus for Optimization\nDerivatives and Gradients\nGradient Descent\n3. Data Preprocessing\nData Cleaning (Missing Values, Outliers)\nData Normalization and Standardization\nEncoding Categorical Variables (One-",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6d7e0b1d-c3d4-49aa-a172-d06e87ed7269",
    "text": "Hot Encoding, Label Encoding)\nFeature Scaling (Min-Max, Z-score)\nFeature Engineering (Polynomial Features, Binning)\nHandling Imbalanced Data (SMOTE, Undersampling, Oversampling)\n4. Supervised Learning Algorithms\nLinear Regression\nSimple vs. Multiple Linear Regression\nGradient Descent and Normal Equation\nRegularization (L1, L2)\nLogistic Regression\nBinary vs. Multiclass Classification\nSigmoid Function and Cost Function\nRegularization\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "8c294d47-0abe-4a2a-aa5a-9dfb10f191f7",
    "text": "4. Supervised Learning Algorithms\nK-Nearest Neighbors (KNN)\nDistance Metrics (Euclidean, Manhattan)\nChoosing K\nAdvantages and Disadvantages\nSupport Vector Machines (SVM)\nHyperplanes and Margins\nLinear and Non-Linear SVM\nKernel Trick\nDecision Trees\nGini Impurity and Entropy\nOverfitting and Pruning\nRandom Forest\nBootstrapping\nBagging\nFeature Importance\nGradient Boosting Machines (GBM)\nXGBoost, LightGBM, CatBoost\nHyperparameter Tuning\nEarly Stopping\nNaive Bayes\nGaussian, Multinomial, Bernoulli Naive Bayes\nAssumptions and Applications\n5. Unsupervised Learning Algorithms\nK-Means Clustering\nAlgorithm Overview\nElbow Method\nK-Means++ Initialization\nHierarchical Clustering\nAgglomerative vs. Divisive Clustering\nDendrogram and Optimal Cut\nPrincipal Component Analysis (PCA)\nDimensionality Reduction\nEi",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "fcf544d2-076f-41f3-bbc2-c66138bb54a7",
    "text": "genvalue Decomposition\nScree Plot and Explained Variance\nDBSCAN\nDensity-Based Clustering\nEpsilon and MinPts Parameters\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 1
    }
  },
  {
    "chunk_id": "234cb812-703b-48f0-9769-11c1847a5de8",
    "text": "6. Reinforcement Learning\nIntroduction to Reinforcement Learning\nKey Concepts (Agent, Environment, State, Action, Reward)\nMarkov Decision Process (MDP)\nQ-Learning and Deep Q-Networks (DQN)\nPolicy Gradients and Actor-Critic Methods\nExploration vs. Exploitation\n7. Neural Networks & Deep Learning\nIntroduction to Neural Networks\nPerceptrons\nActivation Functions (Sigmoid, ReLU, Tanh)\nForward Propagation and Backpropagation\nLoss Functions (MSE, Cross-Entropy)\nDeep Neural Networks (DNN)\nArchitecture and Layers\nTraining Process and Optimizers\nOverfitting and Regularization (Dropout, L2 Regularization)\nConvolutional Neural Networks (CNN)\nConvolutional Layers, Pooling Layers\nFilter/Kernels and Strides\nApplications (Image Classification, Object Detection)\nRecurrent Neural Networks (RNN)\nBasic RNN vs.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "19c6574d-5b10-466d-bc59-7750716be7a7",
    "text": " LSTM vs. GRU\nTime-Series Prediction and NLP Applications\nVanishing and Exploding Gradients Problem\nGenerative Adversarial Networks (GANs)\nGenerator and Discriminator\nTraining Process\nApplications (Image Generation, Data Augmentation)\n8. Natural Language Processing (NLP)\nText Preprocessing\nTokenization, Stopwords, Lemmatization, Stemming\nBag of Words, TF-IDF\nWord Embeddings\nWord2Vec, GloVe, FastText\nSentence Embeddings (BERT, RoBERTa, GPT)\nSequence Models\nRecurrent Neural Networks (RNNs)\nLSTM and GRU\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "33372d5f-6c82-4016-a032-c0a394338090",
    "text": "8. Natural Language Processing (NLP)\nTransformer Architecture\nSelf-Attention Mechanism\nEncoder-Decoder Model\nBERT, GPT, T5 Models\nText Classification\nSentiment Analysis, Named Entity Recognition (NER)\nLanguage Generation\nText Summarization\nMachine Translation\n9. Model Evaluation and Metrics\nClassification Metrics\nAccuracy, Precision, Recall, F1-Score\nConfusion Matrix\nROC Curve, AUC\nRegression Metrics\nMean Absolute Error (MAE), Mean Squared Error (MSE)\nR-Squared and Adjusted R-Squared\nCross-Validation\nK-Fold Cross-Validation\nLeave-One-Out Cross-Validation\nStratified K-Fold\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\n10. Advanced Topics\nTransfer Learning\nPre-trained Models (VGG, ResNet, BERT)\nFine-Tuning and Feature Extraction\nAttention Mechanism\nSelf-Attention, Mul",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7d725162-2e76-45b9-86a4-2d31bf348a57",
    "text": "ti-Head Attention\nApplications in NLP and Vision\nReinforcement Learning in Deep Learning\nActor-Critic, A3C, Proximal Policy Optimization (PPO)\nFederated Learning\nDistributed Learning Frameworks\nPrivacy-Preserving ML\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "f18dc5bc-4b4e-45f1-a4db-d56e1f5f13c1",
    "text": "11. Tools and Libraries for AI/ML\nPython Libraries\nNumPy, Pandas\nScikit-learn\nTensorFlow, Keras, PyTorch\nOpenCV for Computer Vision\nNLTK, SpaCy for NLP\nCloud Platforms\nGoogle Colab\nAWS Sagemaker\nAzure ML\n12. Deployment and Production\nModel Serialization (Pickle, Joblib)\nFlask/Django for Model Deployment\nServing Models with TensorFlow Serving, FastAPI\nMonitoring and Maintaining Models in Production\n13. Practice & Common Beginner Mistakes\nPractice Tasks\nCommon Beginner Mistakes\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5e169811-a574-4712-85c3-da26d4a87c8a",
    "text": "1. INTRODUCTION TO AI AND ML\n1.1 What is Artificial Intelligence (AI)?\nArtificial Intelligence (AI) is the ability of machines or computer programs\nto perform tasks that typically require human intelligence. These tasks can\ninclude understanding language, recognizing patterns, solving problems,\nand making decisions.\nSimple explanation:\nAI is when machines are made smart enough to think and act like\nhumans.\nExamples:\nVoice assistants like Alexa\nImage recognition systems\nChatbots\nSelf-driving cars\n1.2 Types of AI: Narrow AI vs. General AI\nNarrow AI (Weak AI):\nDesigned to perform one specific task\nCannot do anything beyond its programming\nExamples: Email spam filters, facial recognition, recommendation systems\nGeneral AI (Strong AI):\nStill under research and development\nCan learn and perform ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "127bed1c-e2b8-40e6-a82c-dcfe9cb0b0de",
    "text": "any intellectual task a human can do\nWould have reasoning, memory, and decision-making abilities similar to a\nhuman\n1.3 What is Machine Learning (ML)?\nMachine Learning is a subfield of AI that allows machines to learn from\ndata and improve their performance over time without being explicitly\nprogrammed.\nSimple explanation:\nInstead of writing rules for everything, we give the machine data, and it\nfigures out the rules on its own.\nExample:\nA machine learns to identify spam emails by studying thousands of\nexamples.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "f1b5803f-df5f-4af8-9e21-f578267fbbb2",
    "text": "1. INTRODUCTION TO AI AND ML\n1.4 Supervised vs. Unsupervised vs. Reinforcement Learning\nSupervised Learning:\nThe training data includes both input and the correct output (labels)\nThe model learns by comparing its output with the correct output\nExample: Predicting house prices based on features like size, location, etc.\nUnsupervised Learning:\nThe data has no labels\nThe model tries to find patterns or groupings in the data\nExample: Grouping customers based on purchasing behavior\nReinforcement Learning:\nThe model learns through trial and error\nIt receives rewards or penalties based on its actions\nExample: A robot learning to walk or a program learning to play chess\n1.5 Key Terminologies\nModel:\nA program or function that makes predictions or decisions based on data.\nFeature:\nAn input variable ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "fcfefda2-f196-4e63-95de-3839d94a9774",
    "text": "used in making predictions (e.g., age, income,\ntemperature).\nTarget:\nThe value the model is trying to predict (e.g., house price, spam or not).\nAlgorithm:\nA step-by-step method or set of rules used to train the model.\nTraining:\nThe process of teaching the model using a dataset.\nTesting:\nEvaluating the trained model on new, unseen data to measure its\naccuracy.\n1.6 Applications of AI and ML\nRecommendation systems (YouTube, Amazon, Netflix)\nFraud detection in banking\nLanguage translation\nHealthcare diagnosis\nSelf-driving vehicles\nStock market prediction\nChatbots and customer support\nSocial media content moderation",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 1
    }
  },
  {
    "chunk_id": "79cee41c-f91a-46b0-b9ce-9129dd8e040d",
    "text": "Term\nDeep Learning (DL)\nMachine Learning (ML)\nArtificial Intelligence (AI)\nDescription\nA subset of AI where machines learn from data\nThe overall field focused on creating intelligent machines\nA specialized type of ML that uses neural networks inspired by the\nhuman brain\n1. INTRODUCTION TO AI AND ML\nDeep Learning\nMachine Learning\nArtificial Intelligence\n1.7 Difference Between AI, ML, and Deep Learning (DL)\nVisual analogy:\nAI is the broader concept, ML is a part of AI, and DL is a part of ML.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e15844b3-364a-477c-94b3-df210e3912b2",
    "text": "Mathematics is the foundation of AI and Machine Learning. It helps us\nunderstand how algorithms work under the hood and how to fine-tune\nmodels for better performance.\n2.1 Linear Algebra\nLinear Algebra deals with numbers organized in arrays and how these\narrays interact. It is used in almost every ML algorithm.\n2.1.1 Vectors, Matrices, and Tensors\nVector: A 1D array of numbers. Example: [3, 5, 7]\nUsed to represent features like height, weight, age.\nMatrix: A 2D array (rows and columns).\nExample:\nUsed to store datasets or model weights.\nTensor: A generalization of vectors and matrices to more dimensions (3D\nor higher).\nExample: Used in deep learning models like images (3D tensor: width,\nheight, color channels).\n2.1.2 Matrix Operations\nAddition/Subtraction: Add or subtract corresponding elem",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9ac03360-471d-4c96-9fae-3e37ddd4974a",
    "text": "ents of two\nmatrices.\nMultiplication: Used to combine weights and inputs in ML models.\nTranspose: Flip a matrix over its diagonal.\nDot Product: Fundamental in calculating output in neural networks.\n2.1.3 Eigenvalues and Eigenvectors\nEigenvector: A direction that doesn't change during a transformation.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk.\nThese are used in algorithms like Principal Component Analysis (PCA) for\ndimensionality reduction.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "0c71d68a-45e8-442a-ada6-6143017b1235",
    "text": "2.2 Probability and Statistics\nProbability helps machines make decisions under uncertainty, and\nstatistics helps us understand data and model performance.\n2.2.1 Mean, Variance, Standard Deviation\nMean: The average value.\nVariance: How spread out the values are from the mean.\nStandard Deviation: The square root of variance. It measures how much\nthe values vary.\nUsed to understand the distribution and behavior of features in datasets.\n2.2.2 Bayes Theorem\nA mathematical formula to calculate conditional probability:\nUsed in Naive Bayes classifiers for spam detection, document classification,\netc.\n2.2.3 Conditional Probability\nThe probability of one event occurring given that another event has\nalready occurred.\nExample:\nProbability that a user clicks an ad given that they are between 20-30\nyear",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d3081186-5cbb-47a3-9164-a69121c50412",
    "text": "s old.\n2.2.4 Probability Distributions\nNormal Distribution: Bell-shaped curve. Common in real-world data like\nheight, exam scores.\nBinomial Distribution: Used for yes/no type outcomes. Example: Flipping a\ncoin 10 times.\nPoisson Distribution: For events happening over a time period. Example:\nNumber of customer calls per hour.\nThese distributions help in modeling randomness in data.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 1
    }
  },
  {
    "chunk_id": "3fbc5f44-bad2-4c47-843e-b83f75be0452",
    "text": "2.3 Calculus for Optimization\nCalculus helps in training models by optimizing them to reduce errors.\n2.3.1 Derivatives and Gradients\nDerivative: Measures how a function changes as its input changes.\nGradient: A vector of derivatives that tells the slope of a function in multi-\ndimensions.\nUsed to find the direction in which the model should adjust its weights.\n2.3.2 Gradient Descent\nAn optimization algorithm used to minimize the loss (error) function.\nHow it works:\nStart with random values\nCalculate the gradient (slope)\nMove slightly in the opposite direction of the gradient\nRepeat until the loss is minimized\nGradient Descent is the core of many training algorithms in ML and DL.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "13e05bcb-09e0-47d1-8f99-d22494e789d1",
    "text": "Used when data is normally distributed\nUsed when the data is not normally distributed\nStandardization (Z-score Scaling):\nCenters the data around mean = 0 and standard deviation = 1\nFormula:\nBefore feeding data into any machine learning model, it must be cleaned,\ntransformed, and prepared. This step is called data preprocessing, and it is\none of the most important stages in building accurate ML models.\n3.1 Data Cleaning\nReal-world data is often messy. Data cleaning means identifying and fixing\nerrors in the dataset.\n3.1.1 Missing Values\nMissing values can be due to incomplete forms, sensor errors, etc.\nTechniques to handle missing data:\nRemove rows/columns with too many missing values\nFill (impute) missing values using:\nMean/Median/Mode\nForward/Backward fill\nPredictive models (like KNN)\n3.1",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "483b77dc-a26a-4a41-9ff3-1cd686a513dd",
    "text": ".2 Outliers\nOutliers are data points that are very different from others.\nThey can distort results and reduce model performance.\nDetection methods:\nBox plot, Z-score, IQR method\nHandling outliers:\nRemove them\nTransform data (e.g., log scaling)\nCap them (set a maximum/minimum)\n3.2 Data Normalization and Standardization\nHelps scale numeric data so that features contribute equally to the model.\nNormalization (Min-Max Scaling):\nScales all values between 0 and 1\nFormula:\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 1
    }
  },
  {
    "chunk_id": "5c1fde12-9139-42cf-972e-4dcfa89a7d5a",
    "text": "One-Hot Encoding:\nCreates new binary columns for each category.\nExample:\n3.3 Encoding Categorical Variables\nML models work with numbers, not text. Categorical data needs to be\nconverted into numerical form.\nLabel Encoding:\nAssigns each unique category a number.\nExample:\nLabel encoding is good for ordinal data (ranked), while one-hot encoding\nis best for nominal data (non-ranked).\n3.4 Feature Scaling\nEnsures features are on the same scale so the model can learn effectively.\nMin-Max Scaling:\nScales features between 0 and 1.\nGood for algorithms like KNN, neural networks.\nZ-score Scaling (Standardization):\nUseful for models that assume normality, like linear regression or logistic\nregression.\nScaling is crucial for models that use distance or gradient-based optimization.\n3.5 Feature Engineerin",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "0694b20a-b2c2-4a8c-a7ea-757e32946d2f",
    "text": "g\nCreating new features or modifying existing ones to improve model\nperformance.\nPolynomial Features:\nCreate new features by raising existing features to a power.\nExample: From x, create x², x³\nBinning (Discretization):\nConvert continuous data into categories.\nExample: Age → [0–18], [19–35], [36–60], 60+\nFeature engineering can significantly boost the predictive power of a model.\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4e7ebf87-c3e1-4e67-9595-a4722db058f5",
    "text": "3. DATA PREPROCESSING\n3.6 Handling Imbalanced Data\nIn classification, if one class dominates (e.g., 95% non-fraud, 5% fraud),\nmodels may ignore the minority class. This is called class imbalance.\nSMOTE (Synthetic Minority Oversampling Technique):\nCreates synthetic examples of the minority class using nearest neighbors.\nUndersampling:\nRemove some samples from the majority class.\nOversampling:\nDuplicate or generate more samples of the minority class.\nBalancing data improves the ability of the model to correctly predict both\nclasses.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f737cdd4-d21d-487d-aebf-39b5c64e49e3",
    "text": "Works for small datasets.\n4.1.1 Simple vs. Multiple Linear Regression\nSimple Linear Regression: One input (X) to predict one output (Y).\nExample: Predicting salary from years of experience.\nMultiple Linear Regression: Multiple inputs (X1, X2, ..., Xn).\nExample: Predicting price based on area, location, and age.\n4.1.2 Gradient Descent and Normal Equation\nGradient Descent: Iterative method to minimize error (cost function).\nNormal Equation: Direct way to find weights using linear algebra:\nSupervised learning uses labeled data, meaning the model learns from input-\noutput pairs (X → y). The algorithm tries to map inputs (features) to correct\noutputs (targets/labels).\n4.1 Linear Regression\nUsed for predicting continuous values (e.g., predicting house price,\ntemperature).\n4. SUPERVISED LEARNING ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "02bf9262-6f25-4bb6-925f-da71d5c8b942",
    "text": "ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 1
    }
  },
  {
    "chunk_id": "a9a94f23-9f5e-4e6d-94ed-7f1bf6f58b08",
    "text": "4.1.3 Regularization (L1, L2)\nPrevents overfitting by adding a penalty:\nL1 (Lasso): Can reduce coefficients to 0 (feature selection).\nL2 (Ridge): Shrinks coefficients but doesn’t make them 0.\n4.2 Logistic Regression\nUsed for classification problems (e.g., spam vs. not spam).\n4.2.1 Binary vs. Multiclass Classification\nBinary: 2 outcomes (e.g., 0 or 1)\nMulticlass: More than 2 classes (handled using One-vs-Rest or Softmax)\n4.2.2 Sigmoid and Cost Function\nSigmoid Function: Converts outputs to values between 0 and 1.\nCost Function: Log loss used to measure prediction error.\n4.2.3 Regularization\nL1 and L2 regularization help prevent overfitting in logistic regression as\nwell.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9ebdb208-2d62-446d-b50d-ff3ca5bfbd64",
    "text": "4.3.1 Distance Metrics\nEuclidean Distance: Straight line between two points.\nManhattan Distance: Sum of absolute differences.\n4.3.2 Choosing K\nK is the number of neighbors to consider.\nToo low K → sensitive to noise\nToo high K → model becomes less flexible\n4.3.3 Advantages & Disadvantages\nSimple and easy to implement\nSlow for large datasets, sensitive to irrelevant features\n4.4 Support Vector Machines (SVM)\nPowerful classification model for small to medium-sized datasets.\n4.3 K-Nearest Neighbors (KNN)\nA simple classification (or regression) algorithm that uses proximity.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "00afa01c-37fe-4c12-8f1f-e8623d612031",
    "text": "4.5.1 Gini Impurity and Entropy\nMeasures how pure a node is:\nGini Impurity: Probability of misclassification.\nEntropy: Measure of randomness/information.\n4.5.2 Overfitting and Pruning\nOverfitting: Tree memorizes training data.\nPruning: Removes unnecessary branches to reduce overfitting.\n4.4.1 Hyperplanes and Margins\nSVM finds the best hyperplane that separates data with maximum\nmargin.\n4.4.2 Linear vs. Non-Linear SVM\nLinear SVM: Works when data is linearly separable.\nNon-linear SVM: Uses kernel trick for complex datasets.\n4.4.3 Kernel Trick\nTransforms data into higher dimensions to make it separable.\nCommon kernels: RBF (Gaussian), Polynomial, Sigmoid\n4.5 Decision Trees\nTree-like structure used for classification and regression.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "3f4b7d96-6141-4af3-b587-973ae173a9ac",
    "text": "4.7.1 XGBoost, LightGBM, CatBoost\nAdvanced boosting libraries:\nXGBoost: Popular, fast, and accurate\nLightGBM: Faster, uses leaf-wise growth\nCatBoost: Handles categorical features automatically\n4.6.1 Bootstrapping\nRandomly selects subsets of data to train each tree.\n4.6.2 Bagging\nCombines predictions of multiple trees (majority vote or average).\n4.6.3 Feature Importance\nMeasures which features contribute most to model prediction.\n4.7 Gradient Boosting Machines (GBM)\nBoosting is an ensemble method where models are trained sequentially.\n4.6 Random Forest\nAn ensemble of decision trees to improve accuracy and reduce overfitting.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 20,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1b299eee-7e3b-498b-bb55-9a4f205e3bcf",
    "text": "4.8.1 Gaussian, Multinomial, Bernoulli\nGaussian NB: For continuous features (assumes normal distribution)\nMultinomial NB: For text data, counts of words\nBernoulli NB: For binary features (0/1)\n4.8.2 Assumptions and Applications\nAssumes all features are independent (rarely true but still works well)\nCommonly used in spam detection, sentiment analysis, document\nclassif ication\n4.7.2 Hyperparameter Tuning\nAdjust parameters like:\nLearning rate\nNumber of estimators (trees)\nMax depth\nTools: GridSearchCV, RandomSearchCV\n4.7.3 Early Stopping\nStops training if the model stops improving on validation set.\n4.8 Naive Bayes\nProbabilistic classifier based on Bayes' Theorem and strong independence\nassumption.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ed27530c-2bd6-480b-92f3-6622b25b1ab8",
    "text": "Unsupervised learning finds hidden patterns in unlabeled data. Unlike\nsupervised learning, it doesn't rely on labeled outputs (no predefined target).\n5.1 K-Means Clustering\n5.1.1 Algorithm Overview\nK-Means is a clustering algorithm that divides data into K clusters based\non similarity.\nIt works by:\na.Selecting K random centroids.\nb.Assigning each point to the nearest centroid.\nc.Updating the centroid to the mean of its assigned points.\nd.Repeating steps 2–3 until the centroids stop changing.\n5.1.2 Elbow Method\nUsed to choose the optimal number of clusters (K).\nPlot the number of clusters (K) vs. Within-Cluster-Sum-of-Squares (WCSS).\nThe point where the WCSS curve bends (elbow) is the best K.\n5.1.3 K-Means++ Initialization\nImproves basic K-Means by smartly selecting initial centroids, reduc",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "483fa241-7814-475a-8d30-a9cda001c58f",
    "text": "ing the\nchance of poor clustering.\nStarts with one random centroid, then selects the next ones based on\ndistance from the current ones (probabilistically).\n5.2 Hierarchical Clustering\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 1
    }
  },
  {
    "chunk_id": "b32c2f88-e45f-442e-9360-d6dcdbe26e16",
    "text": "5.2.1 Agglomerative vs. Divisive Clustering\nAgglomerative (bottom-up): Start with each point as its own cluster and\nmerge the closest clusters.\nDivisive (top-down): Start with one large cluster and recursively split it.\nAgglomerative is more commonly used.\n5.2.2 Dendrogram and Optimal Cut\nA dendrogram is a tree-like diagram that shows how clusters are formed\nat each step.\nThe height of branches represents the distance between clusters.\nCutting the dendrogram at a certain height gives the desired number of\nclusters.\n5.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique used to simplify datasets\nwhile retaining most of the important information.\n5.3.1 Dimensionality Reduction\nPCA transforms the data into a new coordinate system with fewer\ndimensions (called princ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6e2e35f2-ea9a-4a8f-b709-5293d45e3ce1",
    "text": "ipal components).\nUseful for visualization, speeding up algorithms, and avoiding the curse of\ndimensionality.\n5.3.2 Eigenvalue Decomposition\nPCA is based on eigenvectors and eigenvalues of the covariance matrix of\nthe data.\nThe eigenvectors define the new axes (principal components).\nThe eigenvalues indicate the amount of variance each component\ncaptures.\n5.3.3 Scree Plot and Explained Variance\nScree Plot: A plot of eigenvalues to help decide how many components to\nkeep.\nThe explained variance ratio shows how much of the data’s variance is\ncaptured by each component.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 1
    }
  },
  {
    "chunk_id": "ce85dda9-b676-40b6-9438-ab76366cf6f2",
    "text": "5.4.1 Density-Based Clustering\nUnlike K-Means, DBSCAN doesn't require specifying the number of\nclusters.\nClusters are formed based on dense regions in the data.\n5.4.2 Epsilon and MinPts Parameters\nEpsilon (ε): Radius around a point to search for neighbors.\nMinPts: Minimum number of points required to form a dense region.\nPoints are classified as:\nCore Point: Has MinPts within ε.\nBorder Point: Not a core but within ε of a core.\nNoise: Neither core nor border.\n5.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups closely\npacked points and marks outliers as noise.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 24,
      "chunk": 0
    }
  },
  {
    "chunk_id": "224f57d6-277f-45c5-b62c-0bbd91825fb2",
    "text": "6.2 Key Concepts\nAgent: The learner or decision maker (e.g., a robot, self-driving car).\nEnvironment: Everything the agent interacts with (e.g., a maze, a game).\nState: A snapshot of the current situation (e.g., position in a maze).\nAction: A move or decision made by the agent (e.g., turn left).\nReward: Feedback from the environment (e.g., +1 for reaching goal, -1 for\nhitting a wall).\nThe goal of the agent is to maximize cumulative rewards over time.\n6.3 Markov Decision Process (MDP)\nRL problems are often modeled as Markov Decision Processes (MDPs).\nAn MDP includes: S: Set of states A: Set of actions P: Transition\nprobabilities (P(s' | s, a)) R: Reward function γ (gamma): Discount factor\n(how much future rewards are valued) The \"Markov\" property means the\nnext state only depends on the cur",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 0
    }
  },
  {
    "chunk_id": "178fac24-a760-4059-a5ee-2c8d2166f224",
    "text": "rent state and action, not on previous\nones.\n6.1 Introduction to Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent\nlearns to make decisions by interacting with an environment. It is inspired\nby how humans learn from experience — by trial and error.\nThe agent performs an action\nThe environment responds with a reward\nThe agent uses this feedback to learn better actions over time\nUnlike supervised learning, RL doesn’t rely on labeled data. Instead, it uses\nrewards or penalties to guide learning.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 1
    }
  },
  {
    "chunk_id": "8f20a6b3-29de-4446-991c-8fb761fb8d99",
    "text": "Where:\ns: current state\na: action\nr: reward\ns': next state\nα: learning rate\nγ: discount factor\nDeep Q-Networks (DQN):\nWhen the state/action space is too large, a neural network is used to\napproximate Q-values.\nCombines Q-Learning with Deep Learning.\nUsed in Atari games and robotics.\n6.4 Q-Learning and Deep Q-Networks (DQN)\nQ-Learning:\nA model-free algorithm that learns the value (Q-value) of taking an action\nin a state.\nUses the formula:\n6.5 Policy Gradients and Actor-Critic Methods\nPolicy Gradients:\nInstead of learning value functions, learn the policy directly (probability\ndistribution over actions).\nUse gradient ascent to improve the policy based on the reward received.\nActor-Critic Methods:\nCombine the best of both worlds:\nActor: chooses the action\nCritic: evaluates how good the action",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 0
    }
  },
  {
    "chunk_id": "84f347fb-c37d-47c8-a3fb-95f0a8daac92",
    "text": " was (value function)\nMore stable and efficient than pure policy gradients.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 1
    }
  },
  {
    "chunk_id": "bf4ffa4e-4474-4d7b-9687-cbbbaf7dda5a",
    "text": "6.6 Exploration vs. Exploitation\nExploration: Trying new actions to discover their effects (important early in\ntraining).\nExploitation: Choosing the best-known action for maximum reward.\nRL must balance both:\nToo much exploration = slow learning\nToo much exploitation = stuck in local optimum\nCommon strategy: ε-greedy\nChoose a random action with probability ε\nOtherwise, choose the best-known action\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 27,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f46f1996-847f-41e1-b640-551ef421b32f",
    "text": "7.1 Introduction to Neural Networks\nA neural network is a computer model inspired by the human brain. It\nconsists of neurons (nodes) organized in layers, capable of learning\npatterns from data.\n7.1.1 Perceptrons\nThe perceptron is the simplest type of neural network, with:\nInputs → Weights → Summation → Activation Function → Output\nIt's like a yes/no decision maker (binary classification).\n7.1.2 Activation Functions\nThese introduce non-linearity, allowing the network to learn complex\nfunctions:\nSigmoid: Outputs between 0 and 1. Good for probability-based outputs.\nReLU (Rectified Linear Unit): Most popular. Fast, reduces vanishing\ngradient.\nReLU(x) = max(0, x)\nTanh: Like sigmoid but outputs between -1 and 1.\n7.1.3 Forward Propagation and Backpropagation\nForward Propagation: Input data flows ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8c8105bb-8351-4541-8717-89067d4e2248",
    "text": "through the network to produce an\noutput.\nBackpropagation: Calculates the error and updates weights using\ngradients (from loss function).\nThis is how neural networks learn from data.\n7.1.4 Loss Functions\nThey measure how far off the prediction is from the actual result.\nMSE (Mean Squared Error): Used in regression problems.\nCross-Entropy Loss: Used in classification tasks.\n7.2 Deep Neural Networks (DNN)\nA Deep Neural Network has multiple hidden layers between input and\noutput.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 1
    }
  },
  {
    "chunk_id": "584cf2b9-3b8f-4816-ae69-00e70ebda98a",
    "text": "7.3.1 Convolutional Layers, Pooling Layers\nConvolutional Layers: Apply filters to detect features (edges, corners).\nPooling Layers: Reduce size of feature maps (e.g., Max Pooling).\n7.3.2 Filters/Kernels and Strides\nFilters: Small matrix to slide over input to extract features.\nStrides: Step size of the filter as it moves.\n7.3.3 Applications\nImage Classification\nFace Recognition\nObject Detection\n7.2.1 Architecture and Layers\nInput Layer: Where the data comes in\nHidden Layers: Where computation happens (many neurons per layer)\nOutput Layer: Final predictions\n7.2.2 Training Process and Optimizers\nDuring training, the network:\nMakes predictions\nCalculates the loss\nUpdates weights via optimizers like:\nSGD (Stochastic Gradient Descent)\nAdam (adaptive learning rate)\nRMSProp\n7.2.3 Overfitting and ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c8b975f0-9c80-4651-bff1-82b8a1027a5c",
    "text": "Regularization\nOverfitting happens when the model learns noise instead of patterns.\nRegularization techniques help:\nDropout: Randomly turns off neurons during training.\nL2 Regularization: Penalizes large weights (weight decay).\n7.3 Convolutional Neural Networks (CNN)\nCNNs are specialized for image data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 1
    }
  },
  {
    "chunk_id": "5d675a58-17b3-4b35-875e-90419b493ec8",
    "text": "7.4 Recurrent Neural Networks (RNN)\nRNNs are designed for sequential data (time series, text, etc.).\n7.4.1 Basic RNN vs. LSTM vs. GRU\nBasic RNN: Loops through time steps but suffers from memory issues.\nLSTM (Long Short-Term Memory): Handles long dependencies well.\nGRU (Gated Recurrent Unit): Similar to LSTM but faster.\n7.4.2 Time-Series Prediction and NLP Applications\nPredict stock prices, weather, or language sequences.\nUsed in chatbots, translation, and speech recognition.\n7.4.3 Vanishing and Exploding Gradients\nProblem during training of RNNs where gradients shrink (vanish) or\nexplode.\nLSTM and GRU solve this with gate mechanisms.\n7.5 Generative Adversarial Networks (GANs)\nGANs are powerful models for generating new data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 30,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b96ab796-331e-4dab-8a5e-91010a4ba257",
    "text": "7.5.1 Generator and Discriminator\nGenerator: Creates fake data\nDiscriminator: Tries to distinguish real from fake data\nThey compete with each other (like a forger and a detective).\n7.5.2 Training Process\nGenerator tries to fool the discriminator\nDiscriminator improves to detect fakes\nThey both improve over time — leading to realistic generated data\n7.5.3 Applications\nImage Generation (e.g., fake faces)\nArt and Style Transfer\nData Augmentation for training other ML models\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 31,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8e196f56-0dbd-4319-9e11-ed4d64f9efc7",
    "text": "NLP helps computers understand, interpret, and generate human language.\nIt's widely used in applications like chatbots, translation tools, and voice\nassistants.\n8.1 Text Preprocessing\nBefore using text in machine learning models, we need to clean and\nconvert it into a format the computer understands.\n8.1.1 Tokenization\nBreaking text into smaller parts like words or sentences.\nExample: \"I love AI\" → [\"I\", \"love\", \"AI\"]\n8.1.2 Stopwords\nRemoving common words that do not add much meaning (like “is”, “the”,\n“and”).\n8.1.3 Stemming\nCutting words down to their root form.\nExample: “playing”, “played” → “play”\n8.1.4 Lemmatization\nSimilar to stemming but uses grammar to find the proper base word.\nExample: “better” → “good”\n8.1.5 Bag of Words (BoW)\nConverts text into numbers based on word counts in a ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 0
    }
  },
  {
    "chunk_id": "caac15e7-2f75-49dd-a123-8ad5a55d7050",
    "text": "document.\n8.1.6 TF-IDF\nGives importance to words that appear often in one document but not in\nothers. Helps identify keywords.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d2216cdb-fb5d-4ef9-a7c3-bf6602c44f85",
    "text": "8.2 Word Embeddings\nWord embeddings turn words into vectors (numbers) so that a machine\ncan understand their meaning and context.\n8.2.1 Word2Vec\nA model that learns how words are related based on their surrounding\nwords.\n8.2.2 GloVe\nLearns word meanings by looking at how often words appear together.\n8.2.3 FastText\nSimilar to Word2Vec but also looks at parts of words, which helps with\nunknown words.\n8.2.4 Sentence Embeddings (BERT, RoBERTa, GPT)\nThese models convert full sentences into vectors. They understand context\nmuch better than older models.\n8.3 Sequence Models\nThese models are good for processing data where order matters, like text.\n8.3.1 RNN (Recurrent Neural Networks)\nGood for learning from sequences, such as sentences.\n8.3.2 LSTM (Long Short-Term Memory)\nAn advanced RNN that reme",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 0
    }
  },
  {
    "chunk_id": "210cc8a6-b2b6-4bf7-95d2-ad4709e310f7",
    "text": "mbers long-term information.\n8.3.3 GRU (Gated Recurrent Unit)\nA simpler version of LSTM that works faster and often just as well.\n8.4 Transformer Architecture\nTransformers are a powerful model used in almost all modern NLP\nsystems.\n8.4.1 Self-Attention Mechanism\nThis allows the model to focus on important words in a sentence, no\nmatter where they appear.\n8.4.2 Encoder-Decoder Model\nUsed in tasks like translation where the model reads input (encoder) and\ngenerates output (decoder).\n8.4.3 Examples:\nBERT: Great for understanding text.\nGPT: Great for generating text.\nT5: Can both understand and generate text for many tasks.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 1
    }
  },
  {
    "chunk_id": "b79b73e0-754f-447c-a045-b224fab746b6",
    "text": "8. NATURAL LANGUAGE PROCESSING (NLP)\n8.5 Text Classification\nClassify text into categories.\nExamples:\nSentiment Analysis: Is a review positive or negative?\nNamed Entity Recognition (NER): Find names, places, dates, etc. in text.\n8.6 Language Generation\nGenerate new text from existing input.\n8.6.1 Text Summarization\nShortens a long document while keeping important points.\n8.6.2 Machine Translation\nTranslates text from one language to another (like English to Hindi).",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 34,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f0010dee-6afc-4971-ab42-1531fed5d94c",
    "text": "9. MODEL EVALUATION AND METRICS\nModel evaluation helps us check how well our machine learning models are\nperforming. We use different metrics depending on whether it's a\nclassification or regression problem.\n9.1 Classification Metrics\nUsed when your model predicts categories or classes (e.g., spam or not\nspam).\n9.1.1 Accuracy\nHow often the model is correct.\nFormula: (Correct Predictions) / (Total Predictions)\n9.1.2 Precision\nOut of all predicted positives, how many were actually positive?\nUsed when false positives are costly.\nFormula: TP / (TP + FP)\n9.1.3 Recall (Sensitivity)\nOut of all actual positives, how many were predicted correctly?\nUsed when missing positives is costly.\nFormula: TP / (TP + FN)\n9.1.4 F1-Score\nBalance between precision and recall.\nFormula: 2 * (Precision * Recall) / (",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4326defb-43af-49e0-bfce-e0a773c8198e",
    "text": "Precision + Recall)\n9.1.5 Confusion Matrix\nA table showing True Positives, False Positives, False Negatives, and True\nNegatives.\n9.1.6 ROC Curve (Receiver Operating Characteristic)\nShows the trade-off between True Positive Rate and False Positive Rate.\n9.1.7 AUC (Area Under the Curve)\nMeasures the entire area under the ROC curve.\nHigher AUC = better performance.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 1
    }
  },
  {
    "chunk_id": "0f6fa6a2-d1c8-4489-a5ae-903d38be159e",
    "text": "9. MODEL EVALUATION AND METRICS\n9.2 Regression Metrics\nUsed when the model predicts continuous values (like house price,\ntemperature).\n9.2.1 Mean Absolute Error (MAE)\nAverage of the absolute errors.\nEasy to understand.\n9.2.2 Mean Squared Error (MSE)\nAverage of squared errors.\nPenalizes large errors more than MAE.\n9.2.3 R-Squared (R²)\nExplains how much variance in the output is explained by the model.\nRanges from 0 to 1 (higher is better).\n9.2.4 Adjusted R-Squared\nLike R², but adjusts for the number of predictors (features).\nUseful when comparing models with different numbers of features.\n9.3 Cross-Validation\nUsed to test model performance on different splits of the data.\n9.3.1 K-Fold Cross-Validation\nSplit data into k equal parts. Train on k-1 and test on the remaining part.\nRepeat k times",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 0
    }
  },
  {
    "chunk_id": "427cd2f2-1c8c-4d30-bbc7-986a5e6d62ff",
    "text": ".\n9.3.2 Leave-One-Out Cross-Validation (LOOCV)\nA special case of K-Fold where k = number of data points. Very slow but\nthorough.\n9.3.3 Stratified K-Fold\nSame as K-Fold, but keeps the ratio of classes the same in each fold.\nUseful for imbalanced datasets.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d5477749-cc1d-4f9b-afec-3f33c3cdcf7f",
    "text": "9. MODEL EVALUATION AND METRICS\n9.4 Hyperparameter Tuning\nHyperparameters are settings that control how a model learns (like\nlearning rate, depth of a tree, etc.).\n9.4.1 Grid Search\nTests all combinations of given hyperparameter values.\n9.4.2 Random Search\nRandomly selects combinations. Faster than Grid Search.\n9.4.3 Bayesian Optimization\nUses past results to pick the next best combination. Smart and efficient.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 37,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c6287ffe-32c9-4e6c-9398-49951fd7e38b",
    "text": "10. ADVANCED TOPICS\nThese are modern machine learning methods used in advanced real-world\napplications such as chatbots, recommendation systems, self-driving cars,\nand privacy-focused AI.\n10.1 Transfer Learning\nInstead of training a model from scratch, we use a model that has already\nbeen trained on a large dataset and apply it to a new, similar task.\nPre-trained Models\nThese are models trained on huge datasets.\nExamples:\nVGG, ResNet – for images\nBERT – for text\nFine-Tuning\nSlightly updating the pre-trained model using your own smaller dataset.\nFeature Extraction\nUsing the pre-trained model to extract useful features and then using\nthose features for your own model or task.\nBenefit: Saves training time and works well even with limited data.\n10.2 Attention Mechanism\nThis helps the model dec",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f2805ce1-9a2f-4241-8efe-e6ab1276048c",
    "text": "ide which parts of the input data are most\nimportant.\nSelf-Attention\nEvery part of the input focuses on every other part to understand context\nbetter.\nUsed in NLP (Natural Language Processing) and transformers.\nMulti-Head Attention\nApplies attention multiple times in parallel to capture different\nrelationships within the data.\nApplications\nIn NLP: translation, summarization, question-answering\nIn vision: image recognition with Vision Transformers",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 1
    }
  },
  {
    "chunk_id": "6a46e7d9-a1ce-415a-85d7-8e1d0603b567",
    "text": "10.2 Attention Mechanism\n10.3 Reinforcement Learning in Deep Learning\nCombining deep learning with reinforcement learning for decision-\nmaking tasks.\nActor-Critic\nTwo models work together:\nActor: selects the best action\nCritic: evaluates how good the action was\nA3C (Asynchronous Advantage Actor-Critic)\nUses multiple agents to learn in parallel, which speeds up learning and\nincreases stability.\nPPO (Proximal Policy Optimization)\nAn improved and stable way to train reinforcement learning agents. Used\nin games, robotics, etc.\n10. ADVANCED TOPICS\nMachine\nLearning (ML)\nArtificial\nIntelligence (AI)\nDeep\nLearning (DL)\nReinforcement\nLearning (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 39,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a1e178bf-22ce-45e1-8a27-0f74ec47833e",
    "text": "10.4 Federated Learning\nModel training happens across many devices without collecting data in a\ncentral server. Each device keeps its data private and only sends model\nupdates.\nDistributed Learning Frameworks\nUsed when data is spread across users, hospitals, or devices. Examples\ninclude Google’s keyboard predictions.\nPrivacy-Preserving ML\nSince data never leaves the device, user privacy is protected. This is useful\nin healthcare, banking, and personal mobile applications.\n10. ADVANCED TOPICS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 40,
      "chunk": 0
    }
  },
  {
    "chunk_id": "33597a43-4d68-4def-84f8-294a6a9319dc",
    "text": "These tools help you build, train, and deploy AI/ML models more efficiently.\nThey provide ready-to-use functions so you don’t need to code everything\nfrom scratch.\n11.1 Python Libraries\nPython is the most popular language in AI/ML. These libraries make your\nwork faster and easier:\nNumPy\nUsed for numerical computing.\nSupports arrays, matrices, and linear algebra operations.\nPandas\nUsed for data manipulation and analysis.\nYou can load data, clean it, and analyze it in tabular formats (DataFrames).\nScikit-learn\nA powerful machine learning library.\nIncludes ready-to-use models like Linear Regression, SVM, Random Forest,\nKNN, and more.\nTensorFlow & Keras\nUsed for building deep learning models.\nTensorFlow: low-level control\nKeras: high-level interface, easier to use\nPyTorch\nAn alternative to Ten",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 0
    }
  },
  {
    "chunk_id": "21162e8d-dc5f-4ee6-805d-beac635a6d18",
    "text": "sorFlow, widely used in research and development.\nIt’s flexible, fast, and dynamic (supports on-the-fly computation graphs).\nOpenCV\nUsed for computer vision tasks like image processing, object detection,\nface recognition, etc.\nNLTK & SpaCy\nNatural Language Processing (NLP) libraries.\nNLTK: good for learning, includes many basic NLP tasks\nSpaCy: industrial-strength NLP, faster and more efficient\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9d1a31d6-88ca-4e75-a352-fbf0bbf0ade5",
    "text": "Google Colab\nFree online Jupyter Notebook\nSupports GPU/TPU\nGreat for students and beginners\nAWS SageMaker\nAmazon’s cloud ML platform\nSupports training, tuning, and deploying models at scale\nUsed in enterprise-level applications\nAzure ML\nMicrosoft’s machine learning platform\nIntegrates well with other Microsoft tools (e.g., Excel, Power BI)\nProvides autoML, drag-and-drop pipelines, and more\n11.2 Cloud Platforms\nCloud platforms help you run your models on powerful servers without\nneeding your own hardware.\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 42,
      "chunk": 0
    }
  },
  {
    "chunk_id": "76fe3253-e9bb-443a-b2e9-061baef47eec",
    "text": "12.1 Model Serialization\nWhat it means:\nAfter training your machine learning model, you save (serialize) it to use\nlater without retraining.\nPopular tools:\nPickle – A Python library to serialize and deserialize Python objects.\nJoblib – Similar to Pickle but better for large NumPy arrays.\nExample:\n12.2 Flask/Django for Model Deployment\nThese are web frameworks that let you expose your model as an API\nendpoint, so other apps or users can access it via the internet.\nFlask: Lightweight and easier for quick ML model APIs.\nDjango: Heavier but better for large web applications with built-in admin,\nsecurity, and ORM.\nFlask Example:\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 43,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d4da836e-6f3d-4d61-b0cf-fe92a23b9dd9",
    "text": "12.3 Serving Models with TensorFlow Serving, FastAPI\nTensorFlow Serving:\nUsed to deploy TensorFlow models in production. It supports versioning\nand high-performance serving with REST/gRPC.\nFastAPI:\nA modern, fast (high-performance) framework for building APIs with\nautomatic docs, great for production-grade ML APIs.\nFastAPI Example:\n12.4 Monitoring and Maintaining Models in Production\nOnce your model is live, you need to ensure it continues to perform well.\nWhat to monitor:\nModel accuracy degradation (due to data drift)\nResponse time\nError rates\nSystem metrics (CPU, memory)\nTools:\nPrometheus + Grafana for system and application monitoring\nMLflow or Evidently.ai for tracking model performance over time\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 44,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ffe32f4e-f1e9-40e6-b2f4-c27ef990be24",
    "text": "13.2 Common Beginner Mistakes\nGeneral ML Mistakes\nUsing test data during training\nNot normalizing/scaling features\nIgnoring class imbalance in classification tasks\nForgetting to check for data leakage\nNot splitting the dataset correctly (Train/Validation/Test)\nNeural Network Mistakes\nUsing too many/too few layers without tuning\nChoosing wrong activation/loss functions\nIgnoring overfitting (no dropout or regularization)\nNLP Mistakes\nFeeding raw text without preprocessing\nUsing TF-IDF on small datasets without context\nConfusing stemming with lemmatization\nDeployment Mistakes\nNot checking model performance after deployment\nIgnoring real-time latency\nNo monitoring/logging in place\n13.1 Practice Tasks\nTo strengthen understanding, here are simple practice tasks for each core\nconcept:\n13. PRACTIC",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7ebeef3d-19ef-4420-819d-5e672391b65d",
    "text": "E & COMMON BEGINNER MISTAKES\nNLP\nPCA\nTopic\nGANs\nCNNs\nRNNs\nDecision Trees\nLinear Regression\nLogistic Regression\nK-Means Clustering\nMini Practice Task\nClassify hand-written digits using the MNIST dataset.\nBuild a sentiment analysis model on product reviews.\nPredict the next word in a sentence using a small corpus.\nGenerate new handwritten digits after training on MNIST.\nReduce dimensions in the Iris dataset and visualize clusters.\nClassify if a person will buy a product based on age, income, etc.\nGroup customers by shopping behavior (customer segmentation).\nPredict house prices using a dataset with features like area, rooms,\nand location.\nBuild a binary classifier to detect spam emails.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 1
    }
  },
  {
    "chunk_id": "a206b137-e380-454a-8b3a-42babd3947c7",
    "text": "V2lPeGNmL2dpeEw2WkJnN19RQ1BrSFFCU1F5eXpPVkVmYTVWYWo0R0V1WEt2S1JvOGxkVUFOaUd0emEwPV92MjAw",
    "metadata": {
      "filename": "rag_rag-database.txt",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1c833c7e-e3c4-43e9-a016-6f9c6c20a001",
    "text": "AI/ML\nCheatSheet",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "826138f6-7f7b-443f-80d9-5cc6e6d057ae",
    "text": "1. Introduction to AI and ML\nWhat is Artificial Intelligence (AI)?\nTypes of AI: Narrow AI vs. General AI\nWhat is Machine Learning (ML)?\nSupervised vs. Unsupervised vs. Reinforcement Learning\nKey Terminologies (Model, Feature, Target, Algorithm)\nApplications of AI and ML\nDifference Between AI, ML, and Deep Learning (DL)\n2. Mathematics for ML/AI\nLinear Algebra\nVectors, Matrices, and Tensors\nMatrix Operations\nEigenvalues and Eigenvectors\nProbability and Statistics\nMean, Variance, Standard Deviation\nBayes Theorem\nConditional Probability\nProbability Distributions (Normal, Binomial, Poisson)\nCalculus for Optimization\nDerivatives and Gradients\nGradient Descent\n3. Data Preprocessing\nData Cleaning (Missing Values, Outliers)\nData Normalization and Standardization\nEncoding Categorical Variables (One-",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c6af643b-3a11-4ffc-9545-e05de69fa083",
    "text": "Hot Encoding, Label Encoding)\nFeature Scaling (Min-Max, Z-score)\nFeature Engineering (Polynomial Features, Binning)\nHandling Imbalanced Data (SMOTE, Undersampling, Oversampling)\n4. Supervised Learning Algorithms\nLinear Regression\nSimple vs. Multiple Linear Regression\nGradient Descent and Normal Equation\nRegularization (L1, L2)\nLogistic Regression\nBinary vs. Multiclass Classification\nSigmoid Function and Cost Function\nRegularization\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "cefba6ba-2584-4ee2-941c-4efb7f263720",
    "text": "4. Supervised Learning Algorithms\nK-Nearest Neighbors (KNN)\nDistance Metrics (Euclidean, Manhattan)\nChoosing K\nAdvantages and Disadvantages\nSupport Vector Machines (SVM)\nHyperplanes and Margins\nLinear and Non-Linear SVM\nKernel Trick\nDecision Trees\nGini Impurity and Entropy\nOverfitting and Pruning\nRandom Forest\nBootstrapping\nBagging\nFeature Importance\nGradient Boosting Machines (GBM)\nXGBoost, LightGBM, CatBoost\nHyperparameter Tuning\nEarly Stopping\nNaive Bayes\nGaussian, Multinomial, Bernoulli Naive Bayes\nAssumptions and Applications\n5. Unsupervised Learning Algorithms\nK-Means Clustering\nAlgorithm Overview\nElbow Method\nK-Means++ Initialization\nHierarchical Clustering\nAgglomerative vs. Divisive Clustering\nDendrogram and Optimal Cut\nPrincipal Component Analysis (PCA)\nDimensionality Reduction\nEi",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5b57c14b-e01b-4bc7-9f64-38a3b0bc81e4",
    "text": "genvalue Decomposition\nScree Plot and Explained Variance\nDBSCAN\nDensity-Based Clustering\nEpsilon and MinPts Parameters\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 1
    }
  },
  {
    "chunk_id": "dd72db49-0a8f-4bed-9f1f-c43a97857e4b",
    "text": "6. Reinforcement Learning\nIntroduction to Reinforcement Learning\nKey Concepts (Agent, Environment, State, Action, Reward)\nMarkov Decision Process (MDP)\nQ-Learning and Deep Q-Networks (DQN)\nPolicy Gradients and Actor-Critic Methods\nExploration vs. Exploitation\n7. Neural Networks & Deep Learning\nIntroduction to Neural Networks\nPerceptrons\nActivation Functions (Sigmoid, ReLU, Tanh)\nForward Propagation and Backpropagation\nLoss Functions (MSE, Cross-Entropy)\nDeep Neural Networks (DNN)\nArchitecture and Layers\nTraining Process and Optimizers\nOverfitting and Regularization (Dropout, L2 Regularization)\nConvolutional Neural Networks (CNN)\nConvolutional Layers, Pooling Layers\nFilter/Kernels and Strides\nApplications (Image Classification, Object Detection)\nRecurrent Neural Networks (RNN)\nBasic RNN vs.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "217db7f0-684c-4e0a-81d6-4c1b13e8a3cf",
    "text": " LSTM vs. GRU\nTime-Series Prediction and NLP Applications\nVanishing and Exploding Gradients Problem\nGenerative Adversarial Networks (GANs)\nGenerator and Discriminator\nTraining Process\nApplications (Image Generation, Data Augmentation)\n8. Natural Language Processing (NLP)\nText Preprocessing\nTokenization, Stopwords, Lemmatization, Stemming\nBag of Words, TF-IDF\nWord Embeddings\nWord2Vec, GloVe, FastText\nSentence Embeddings (BERT, RoBERTa, GPT)\nSequence Models\nRecurrent Neural Networks (RNNs)\nLSTM and GRU\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "1fbd1cb0-10d1-442e-9ebc-3e317972ed8a",
    "text": "8. Natural Language Processing (NLP)\nTransformer Architecture\nSelf-Attention Mechanism\nEncoder-Decoder Model\nBERT, GPT, T5 Models\nText Classification\nSentiment Analysis, Named Entity Recognition (NER)\nLanguage Generation\nText Summarization\nMachine Translation\n9. Model Evaluation and Metrics\nClassification Metrics\nAccuracy, Precision, Recall, F1-Score\nConfusion Matrix\nROC Curve, AUC\nRegression Metrics\nMean Absolute Error (MAE), Mean Squared Error (MSE)\nR-Squared and Adjusted R-Squared\nCross-Validation\nK-Fold Cross-Validation\nLeave-One-Out Cross-Validation\nStratified K-Fold\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\n10. Advanced Topics\nTransfer Learning\nPre-trained Models (VGG, ResNet, BERT)\nFine-Tuning and Feature Extraction\nAttention Mechanism\nSelf-Attention, Mul",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "80b3e84d-ed8a-443f-8593-47d0cd62aea6",
    "text": "ti-Head Attention\nApplications in NLP and Vision\nReinforcement Learning in Deep Learning\nActor-Critic, A3C, Proximal Policy Optimization (PPO)\nFederated Learning\nDistributed Learning Frameworks\nPrivacy-Preserving ML\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "3fdd624e-d7b1-47f1-a433-723f3bec060d",
    "text": "11. Tools and Libraries for AI/ML\nPython Libraries\nNumPy, Pandas\nScikit-learn\nTensorFlow, Keras, PyTorch\nOpenCV for Computer Vision\nNLTK, SpaCy for NLP\nCloud Platforms\nGoogle Colab\nAWS Sagemaker\nAzure ML\n12. Deployment and Production\nModel Serialization (Pickle, Joblib)\nFlask/Django for Model Deployment\nServing Models with TensorFlow Serving, FastAPI\nMonitoring and Maintaining Models in Production\n13. Practice & Common Beginner Mistakes\nPractice Tasks\nCommon Beginner Mistakes\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1437e5ff-29fe-4178-9b59-015d84722c97",
    "text": "1. INTRODUCTION TO AI AND ML\n1.1 What is Artificial Intelligence (AI)?\nArtificial Intelligence (AI) is the ability of machines or computer programs\nto perform tasks that typically require human intelligence. These tasks can\ninclude understanding language, recognizing patterns, solving problems,\nand making decisions.\nSimple explanation:\nAI is when machines are made smart enough to think and act like\nhumans.\nExamples:\nVoice assistants like Alexa\nImage recognition systems\nChatbots\nSelf-driving cars\n1.2 Types of AI: Narrow AI vs. General AI\nNarrow AI (Weak AI):\nDesigned to perform one specific task\nCannot do anything beyond its programming\nExamples: Email spam filters, facial recognition, recommendation systems\nGeneral AI (Strong AI):\nStill under research and development\nCan learn and perform ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d4844ee2-8be6-41b0-84af-a8511c6daf7c",
    "text": "any intellectual task a human can do\nWould have reasoning, memory, and decision-making abilities similar to a\nhuman\n1.3 What is Machine Learning (ML)?\nMachine Learning is a subfield of AI that allows machines to learn from\ndata and improve their performance over time without being explicitly\nprogrammed.\nSimple explanation:\nInstead of writing rules for everything, we give the machine data, and it\nfigures out the rules on its own.\nExample:\nA machine learns to identify spam emails by studying thousands of\nexamples.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "13c66771-847f-467e-bad3-03c113ffbaa2",
    "text": "1. INTRODUCTION TO AI AND ML\n1.4 Supervised vs. Unsupervised vs. Reinforcement Learning\nSupervised Learning:\nThe training data includes both input and the correct output (labels)\nThe model learns by comparing its output with the correct output\nExample: Predicting house prices based on features like size, location, etc.\nUnsupervised Learning:\nThe data has no labels\nThe model tries to find patterns or groupings in the data\nExample: Grouping customers based on purchasing behavior\nReinforcement Learning:\nThe model learns through trial and error\nIt receives rewards or penalties based on its actions\nExample: A robot learning to walk or a program learning to play chess\n1.5 Key Terminologies\nModel:\nA program or function that makes predictions or decisions based on data.\nFeature:\nAn input variable ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a5cf6b37-2432-4b56-bcc7-cfc42e6e7936",
    "text": "used in making predictions (e.g., age, income,\ntemperature).\nTarget:\nThe value the model is trying to predict (e.g., house price, spam or not).\nAlgorithm:\nA step-by-step method or set of rules used to train the model.\nTraining:\nThe process of teaching the model using a dataset.\nTesting:\nEvaluating the trained model on new, unseen data to measure its\naccuracy.\n1.6 Applications of AI and ML\nRecommendation systems (YouTube, Amazon, Netflix)\nFraud detection in banking\nLanguage translation\nHealthcare diagnosis\nSelf-driving vehicles\nStock market prediction\nChatbots and customer support\nSocial media content moderation",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 1
    }
  },
  {
    "chunk_id": "91cd5f8c-d85e-4a8b-aa44-988eec071094",
    "text": "Term\nDeep Learning (DL)\nMachine Learning (ML)\nArtificial Intelligence (AI)\nDescription\nA subset of AI where machines learn from data\nThe overall field focused on creating intelligent machines\nA specialized type of ML that uses neural networks inspired by the\nhuman brain\n1. INTRODUCTION TO AI AND ML\nDeep Learning\nMachine Learning\nArtificial Intelligence\n1.7 Difference Between AI, ML, and Deep Learning (DL)\nVisual analogy:\nAI is the broader concept, ML is a part of AI, and DL is a part of ML.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "53e300b1-a9c4-4166-9101-a95cf751d760",
    "text": "Mathematics is the foundation of AI and Machine Learning. It helps us\nunderstand how algorithms work under the hood and how to fine-tune\nmodels for better performance.\n2.1 Linear Algebra\nLinear Algebra deals with numbers organized in arrays and how these\narrays interact. It is used in almost every ML algorithm.\n2.1.1 Vectors, Matrices, and Tensors\nVector: A 1D array of numbers. Example: [3, 5, 7]\nUsed to represent features like height, weight, age.\nMatrix: A 2D array (rows and columns).\nExample:\nUsed to store datasets or model weights.\nTensor: A generalization of vectors and matrices to more dimensions (3D\nor higher).\nExample: Used in deep learning models like images (3D tensor: width,\nheight, color channels).\n2.1.2 Matrix Operations\nAddition/Subtraction: Add or subtract corresponding elem",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ace819c6-7733-4ff8-ae87-83103979b293",
    "text": "ents of two\nmatrices.\nMultiplication: Used to combine weights and inputs in ML models.\nTranspose: Flip a matrix over its diagonal.\nDot Product: Fundamental in calculating output in neural networks.\n2.1.3 Eigenvalues and Eigenvectors\nEigenvector: A direction that doesn't change during a transformation.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk.\nThese are used in algorithms like Principal Component Analysis (PCA) for\ndimensionality reduction.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "28957032-3efc-4e0c-9f00-563f3f87ffee",
    "text": "2.2 Probability and Statistics\nProbability helps machines make decisions under uncertainty, and\nstatistics helps us understand data and model performance.\n2.2.1 Mean, Variance, Standard Deviation\nMean: The average value.\nVariance: How spread out the values are from the mean.\nStandard Deviation: The square root of variance. It measures how much\nthe values vary.\nUsed to understand the distribution and behavior of features in datasets.\n2.2.2 Bayes Theorem\nA mathematical formula to calculate conditional probability:\nUsed in Naive Bayes classifiers for spam detection, document classification,\netc.\n2.2.3 Conditional Probability\nThe probability of one event occurring given that another event has\nalready occurred.\nExample:\nProbability that a user clicks an ad given that they are between 20-30\nyear",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "65a7a6a2-ce85-4bb0-a485-8451cb26ac54",
    "text": "s old.\n2.2.4 Probability Distributions\nNormal Distribution: Bell-shaped curve. Common in real-world data like\nheight, exam scores.\nBinomial Distribution: Used for yes/no type outcomes. Example: Flipping a\ncoin 10 times.\nPoisson Distribution: For events happening over a time period. Example:\nNumber of customer calls per hour.\nThese distributions help in modeling randomness in data.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 1
    }
  },
  {
    "chunk_id": "6712917a-9522-41c2-a87a-ae3cea3bd869",
    "text": "2.3 Calculus for Optimization\nCalculus helps in training models by optimizing them to reduce errors.\n2.3.1 Derivatives and Gradients\nDerivative: Measures how a function changes as its input changes.\nGradient: A vector of derivatives that tells the slope of a function in multi-\ndimensions.\nUsed to find the direction in which the model should adjust its weights.\n2.3.2 Gradient Descent\nAn optimization algorithm used to minimize the loss (error) function.\nHow it works:\nStart with random values\nCalculate the gradient (slope)\nMove slightly in the opposite direction of the gradient\nRepeat until the loss is minimized\nGradient Descent is the core of many training algorithms in ML and DL.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "11314acb-068d-4826-b953-5a66d6e59776",
    "text": "Used when data is normally distributed\nUsed when the data is not normally distributed\nStandardization (Z-score Scaling):\nCenters the data around mean = 0 and standard deviation = 1\nFormula:\nBefore feeding data into any machine learning model, it must be cleaned,\ntransformed, and prepared. This step is called data preprocessing, and it is\none of the most important stages in building accurate ML models.\n3.1 Data Cleaning\nReal-world data is often messy. Data cleaning means identifying and fixing\nerrors in the dataset.\n3.1.1 Missing Values\nMissing values can be due to incomplete forms, sensor errors, etc.\nTechniques to handle missing data:\nRemove rows/columns with too many missing values\nFill (impute) missing values using:\nMean/Median/Mode\nForward/Backward fill\nPredictive models (like KNN)\n3.1",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c0f3cb4c-fa76-4246-9c3b-c9d9084f3972",
    "text": ".2 Outliers\nOutliers are data points that are very different from others.\nThey can distort results and reduce model performance.\nDetection methods:\nBox plot, Z-score, IQR method\nHandling outliers:\nRemove them\nTransform data (e.g., log scaling)\nCap them (set a maximum/minimum)\n3.2 Data Normalization and Standardization\nHelps scale numeric data so that features contribute equally to the model.\nNormalization (Min-Max Scaling):\nScales all values between 0 and 1\nFormula:\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 1
    }
  },
  {
    "chunk_id": "bf8c0420-0603-46b5-a5b8-95ef08ddc7ea",
    "text": "One-Hot Encoding:\nCreates new binary columns for each category.\nExample:\n3.3 Encoding Categorical Variables\nML models work with numbers, not text. Categorical data needs to be\nconverted into numerical form.\nLabel Encoding:\nAssigns each unique category a number.\nExample:\nLabel encoding is good for ordinal data (ranked), while one-hot encoding\nis best for nominal data (non-ranked).\n3.4 Feature Scaling\nEnsures features are on the same scale so the model can learn effectively.\nMin-Max Scaling:\nScales features between 0 and 1.\nGood for algorithms like KNN, neural networks.\nZ-score Scaling (Standardization):\nUseful for models that assume normality, like linear regression or logistic\nregression.\nScaling is crucial for models that use distance or gradient-based optimization.\n3.5 Feature Engineerin",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "eb148961-571e-4729-9087-cc04612090b1",
    "text": "g\nCreating new features or modifying existing ones to improve model\nperformance.\nPolynomial Features:\nCreate new features by raising existing features to a power.\nExample: From x, create x², x³\nBinning (Discretization):\nConvert continuous data into categories.\nExample: Age → [0–18], [19–35], [36–60], 60+\nFeature engineering can significantly boost the predictive power of a model.\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "71162861-160d-48ab-9520-e1fcda20762d",
    "text": "3. DATA PREPROCESSING\n3.6 Handling Imbalanced Data\nIn classification, if one class dominates (e.g., 95% non-fraud, 5% fraud),\nmodels may ignore the minority class. This is called class imbalance.\nSMOTE (Synthetic Minority Oversampling Technique):\nCreates synthetic examples of the minority class using nearest neighbors.\nUndersampling:\nRemove some samples from the majority class.\nOversampling:\nDuplicate or generate more samples of the minority class.\nBalancing data improves the ability of the model to correctly predict both\nclasses.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "fd2bfe5f-e3d8-4f2f-8629-3ec35cd619b0",
    "text": "Works for small datasets.\n4.1.1 Simple vs. Multiple Linear Regression\nSimple Linear Regression: One input (X) to predict one output (Y).\nExample: Predicting salary from years of experience.\nMultiple Linear Regression: Multiple inputs (X1, X2, ..., Xn).\nExample: Predicting price based on area, location, and age.\n4.1.2 Gradient Descent and Normal Equation\nGradient Descent: Iterative method to minimize error (cost function).\nNormal Equation: Direct way to find weights using linear algebra:\nSupervised learning uses labeled data, meaning the model learns from input-\noutput pairs (X → y). The algorithm tries to map inputs (features) to correct\noutputs (targets/labels).\n4.1 Linear Regression\nUsed for predicting continuous values (e.g., predicting house price,\ntemperature).\n4. SUPERVISED LEARNING ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4a6430e2-1d39-4fa7-bec2-8ef7302ee538",
    "text": "ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d6fbbfae-6f96-44ec-b0d8-4aae8070c741",
    "text": "4.1.3 Regularization (L1, L2)\nPrevents overfitting by adding a penalty:\nL1 (Lasso): Can reduce coefficients to 0 (feature selection).\nL2 (Ridge): Shrinks coefficients but doesn’t make them 0.\n4.2 Logistic Regression\nUsed for classification problems (e.g., spam vs. not spam).\n4.2.1 Binary vs. Multiclass Classification\nBinary: 2 outcomes (e.g., 0 or 1)\nMulticlass: More than 2 classes (handled using One-vs-Rest or Softmax)\n4.2.2 Sigmoid and Cost Function\nSigmoid Function: Converts outputs to values between 0 and 1.\nCost Function: Log loss used to measure prediction error.\n4.2.3 Regularization\nL1 and L2 regularization help prevent overfitting in logistic regression as\nwell.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9796d14d-a663-430a-b4dc-994d5d4e7032",
    "text": "4.3.1 Distance Metrics\nEuclidean Distance: Straight line between two points.\nManhattan Distance: Sum of absolute differences.\n4.3.2 Choosing K\nK is the number of neighbors to consider.\nToo low K → sensitive to noise\nToo high K → model becomes less flexible\n4.3.3 Advantages & Disadvantages\nSimple and easy to implement\nSlow for large datasets, sensitive to irrelevant features\n4.4 Support Vector Machines (SVM)\nPowerful classification model for small to medium-sized datasets.\n4.3 K-Nearest Neighbors (KNN)\nA simple classification (or regression) algorithm that uses proximity.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d4e48267-4a3c-4098-9ade-90cd1b5fdc9c",
    "text": "4.5.1 Gini Impurity and Entropy\nMeasures how pure a node is:\nGini Impurity: Probability of misclassification.\nEntropy: Measure of randomness/information.\n4.5.2 Overfitting and Pruning\nOverfitting: Tree memorizes training data.\nPruning: Removes unnecessary branches to reduce overfitting.\n4.4.1 Hyperplanes and Margins\nSVM finds the best hyperplane that separates data with maximum\nmargin.\n4.4.2 Linear vs. Non-Linear SVM\nLinear SVM: Works when data is linearly separable.\nNon-linear SVM: Uses kernel trick for complex datasets.\n4.4.3 Kernel Trick\nTransforms data into higher dimensions to make it separable.\nCommon kernels: RBF (Gaussian), Polynomial, Sigmoid\n4.5 Decision Trees\nTree-like structure used for classification and regression.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c31eaba0-511c-4052-afc2-bb09d83ed25c",
    "text": "4.7.1 XGBoost, LightGBM, CatBoost\nAdvanced boosting libraries:\nXGBoost: Popular, fast, and accurate\nLightGBM: Faster, uses leaf-wise growth\nCatBoost: Handles categorical features automatically\n4.6.1 Bootstrapping\nRandomly selects subsets of data to train each tree.\n4.6.2 Bagging\nCombines predictions of multiple trees (majority vote or average).\n4.6.3 Feature Importance\nMeasures which features contribute most to model prediction.\n4.7 Gradient Boosting Machines (GBM)\nBoosting is an ensemble method where models are trained sequentially.\n4.6 Random Forest\nAn ensemble of decision trees to improve accuracy and reduce overfitting.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 20,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4035aa7b-8601-4666-81a5-6c8e28eec09f",
    "text": "4.8.1 Gaussian, Multinomial, Bernoulli\nGaussian NB: For continuous features (assumes normal distribution)\nMultinomial NB: For text data, counts of words\nBernoulli NB: For binary features (0/1)\n4.8.2 Assumptions and Applications\nAssumes all features are independent (rarely true but still works well)\nCommonly used in spam detection, sentiment analysis, document\nclassif ication\n4.7.2 Hyperparameter Tuning\nAdjust parameters like:\nLearning rate\nNumber of estimators (trees)\nMax depth\nTools: GridSearchCV, RandomSearchCV\n4.7.3 Early Stopping\nStops training if the model stops improving on validation set.\n4.8 Naive Bayes\nProbabilistic classifier based on Bayes' Theorem and strong independence\nassumption.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "93739cc0-550c-4f01-8812-a0d688eede62",
    "text": "Unsupervised learning finds hidden patterns in unlabeled data. Unlike\nsupervised learning, it doesn't rely on labeled outputs (no predefined target).\n5.1 K-Means Clustering\n5.1.1 Algorithm Overview\nK-Means is a clustering algorithm that divides data into K clusters based\non similarity.\nIt works by:\na.Selecting K random centroids.\nb.Assigning each point to the nearest centroid.\nc.Updating the centroid to the mean of its assigned points.\nd.Repeating steps 2–3 until the centroids stop changing.\n5.1.2 Elbow Method\nUsed to choose the optimal number of clusters (K).\nPlot the number of clusters (K) vs. Within-Cluster-Sum-of-Squares (WCSS).\nThe point where the WCSS curve bends (elbow) is the best K.\n5.1.3 K-Means++ Initialization\nImproves basic K-Means by smartly selecting initial centroids, reduc",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e0f2e81a-f910-4291-9acb-fe79c3bb4f15",
    "text": "ing the\nchance of poor clustering.\nStarts with one random centroid, then selects the next ones based on\ndistance from the current ones (probabilistically).\n5.2 Hierarchical Clustering\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 1
    }
  },
  {
    "chunk_id": "87014fb5-73e5-4030-966e-665778eb994a",
    "text": "5.2.1 Agglomerative vs. Divisive Clustering\nAgglomerative (bottom-up): Start with each point as its own cluster and\nmerge the closest clusters.\nDivisive (top-down): Start with one large cluster and recursively split it.\nAgglomerative is more commonly used.\n5.2.2 Dendrogram and Optimal Cut\nA dendrogram is a tree-like diagram that shows how clusters are formed\nat each step.\nThe height of branches represents the distance between clusters.\nCutting the dendrogram at a certain height gives the desired number of\nclusters.\n5.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique used to simplify datasets\nwhile retaining most of the important information.\n5.3.1 Dimensionality Reduction\nPCA transforms the data into a new coordinate system with fewer\ndimensions (called princ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 0
    }
  },
  {
    "chunk_id": "2863ab8a-d9cc-4cfd-b124-9f6ac04223ae",
    "text": "ipal components).\nUseful for visualization, speeding up algorithms, and avoiding the curse of\ndimensionality.\n5.3.2 Eigenvalue Decomposition\nPCA is based on eigenvectors and eigenvalues of the covariance matrix of\nthe data.\nThe eigenvectors define the new axes (principal components).\nThe eigenvalues indicate the amount of variance each component\ncaptures.\n5.3.3 Scree Plot and Explained Variance\nScree Plot: A plot of eigenvalues to help decide how many components to\nkeep.\nThe explained variance ratio shows how much of the data’s variance is\ncaptured by each component.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 1
    }
  },
  {
    "chunk_id": "c338ad86-2f3e-4ee4-911b-f853e017d8a9",
    "text": "5.4.1 Density-Based Clustering\nUnlike K-Means, DBSCAN doesn't require specifying the number of\nclusters.\nClusters are formed based on dense regions in the data.\n5.4.2 Epsilon and MinPts Parameters\nEpsilon (ε): Radius around a point to search for neighbors.\nMinPts: Minimum number of points required to form a dense region.\nPoints are classified as:\nCore Point: Has MinPts within ε.\nBorder Point: Not a core but within ε of a core.\nNoise: Neither core nor border.\n5.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups closely\npacked points and marks outliers as noise.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 24,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8ea1408b-be8c-4d97-84ba-af8be1f203ad",
    "text": "6.2 Key Concepts\nAgent: The learner or decision maker (e.g., a robot, self-driving car).\nEnvironment: Everything the agent interacts with (e.g., a maze, a game).\nState: A snapshot of the current situation (e.g., position in a maze).\nAction: A move or decision made by the agent (e.g., turn left).\nReward: Feedback from the environment (e.g., +1 for reaching goal, -1 for\nhitting a wall).\nThe goal of the agent is to maximize cumulative rewards over time.\n6.3 Markov Decision Process (MDP)\nRL problems are often modeled as Markov Decision Processes (MDPs).\nAn MDP includes: S: Set of states A: Set of actions P: Transition\nprobabilities (P(s' | s, a)) R: Reward function γ (gamma): Discount factor\n(how much future rewards are valued) The \"Markov\" property means the\nnext state only depends on the cur",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7223a366-688c-495f-9d3f-d87d6d761b43",
    "text": "rent state and action, not on previous\nones.\n6.1 Introduction to Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent\nlearns to make decisions by interacting with an environment. It is inspired\nby how humans learn from experience — by trial and error.\nThe agent performs an action\nThe environment responds with a reward\nThe agent uses this feedback to learn better actions over time\nUnlike supervised learning, RL doesn’t rely on labeled data. Instead, it uses\nrewards or penalties to guide learning.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4bf9762a-c515-40ef-90a5-ade54aaaded1",
    "text": "Where:\ns: current state\na: action\nr: reward\ns': next state\nα: learning rate\nγ: discount factor\nDeep Q-Networks (DQN):\nWhen the state/action space is too large, a neural network is used to\napproximate Q-values.\nCombines Q-Learning with Deep Learning.\nUsed in Atari games and robotics.\n6.4 Q-Learning and Deep Q-Networks (DQN)\nQ-Learning:\nA model-free algorithm that learns the value (Q-value) of taking an action\nin a state.\nUses the formula:\n6.5 Policy Gradients and Actor-Critic Methods\nPolicy Gradients:\nInstead of learning value functions, learn the policy directly (probability\ndistribution over actions).\nUse gradient ascent to improve the policy based on the reward received.\nActor-Critic Methods:\nCombine the best of both worlds:\nActor: chooses the action\nCritic: evaluates how good the action",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 0
    }
  },
  {
    "chunk_id": "492da12d-b633-4f3d-8c1d-8ef7d1fd8c79",
    "text": " was (value function)\nMore stable and efficient than pure policy gradients.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 1
    }
  },
  {
    "chunk_id": "c0ad8ab4-d313-4d44-a7ae-b5d772422e36",
    "text": "6.6 Exploration vs. Exploitation\nExploration: Trying new actions to discover their effects (important early in\ntraining).\nExploitation: Choosing the best-known action for maximum reward.\nRL must balance both:\nToo much exploration = slow learning\nToo much exploitation = stuck in local optimum\nCommon strategy: ε-greedy\nChoose a random action with probability ε\nOtherwise, choose the best-known action\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 27,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6e46e4a4-e8c8-458e-a4d5-590c666f901b",
    "text": "7.1 Introduction to Neural Networks\nA neural network is a computer model inspired by the human brain. It\nconsists of neurons (nodes) organized in layers, capable of learning\npatterns from data.\n7.1.1 Perceptrons\nThe perceptron is the simplest type of neural network, with:\nInputs → Weights → Summation → Activation Function → Output\nIt's like a yes/no decision maker (binary classification).\n7.1.2 Activation Functions\nThese introduce non-linearity, allowing the network to learn complex\nfunctions:\nSigmoid: Outputs between 0 and 1. Good for probability-based outputs.\nReLU (Rectified Linear Unit): Most popular. Fast, reduces vanishing\ngradient.\nReLU(x) = max(0, x)\nTanh: Like sigmoid but outputs between -1 and 1.\n7.1.3 Forward Propagation and Backpropagation\nForward Propagation: Input data flows ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 0
    }
  },
  {
    "chunk_id": "935f9f95-507b-4400-92fc-e73250235784",
    "text": "through the network to produce an\noutput.\nBackpropagation: Calculates the error and updates weights using\ngradients (from loss function).\nThis is how neural networks learn from data.\n7.1.4 Loss Functions\nThey measure how far off the prediction is from the actual result.\nMSE (Mean Squared Error): Used in regression problems.\nCross-Entropy Loss: Used in classification tasks.\n7.2 Deep Neural Networks (DNN)\nA Deep Neural Network has multiple hidden layers between input and\noutput.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 1
    }
  },
  {
    "chunk_id": "67ce4e2e-5546-43f2-86f2-d5ec3cf2d5ce",
    "text": "7.3.1 Convolutional Layers, Pooling Layers\nConvolutional Layers: Apply filters to detect features (edges, corners).\nPooling Layers: Reduce size of feature maps (e.g., Max Pooling).\n7.3.2 Filters/Kernels and Strides\nFilters: Small matrix to slide over input to extract features.\nStrides: Step size of the filter as it moves.\n7.3.3 Applications\nImage Classification\nFace Recognition\nObject Detection\n7.2.1 Architecture and Layers\nInput Layer: Where the data comes in\nHidden Layers: Where computation happens (many neurons per layer)\nOutput Layer: Final predictions\n7.2.2 Training Process and Optimizers\nDuring training, the network:\nMakes predictions\nCalculates the loss\nUpdates weights via optimizers like:\nSGD (Stochastic Gradient Descent)\nAdam (adaptive learning rate)\nRMSProp\n7.2.3 Overfitting and ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e09580a9-03d3-4eef-8a9e-1b94b4a2bf74",
    "text": "Regularization\nOverfitting happens when the model learns noise instead of patterns.\nRegularization techniques help:\nDropout: Randomly turns off neurons during training.\nL2 Regularization: Penalizes large weights (weight decay).\n7.3 Convolutional Neural Networks (CNN)\nCNNs are specialized for image data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 1
    }
  },
  {
    "chunk_id": "8d072f59-2726-4ae4-a477-7766e59d1c68",
    "text": "7.4 Recurrent Neural Networks (RNN)\nRNNs are designed for sequential data (time series, text, etc.).\n7.4.1 Basic RNN vs. LSTM vs. GRU\nBasic RNN: Loops through time steps but suffers from memory issues.\nLSTM (Long Short-Term Memory): Handles long dependencies well.\nGRU (Gated Recurrent Unit): Similar to LSTM but faster.\n7.4.2 Time-Series Prediction and NLP Applications\nPredict stock prices, weather, or language sequences.\nUsed in chatbots, translation, and speech recognition.\n7.4.3 Vanishing and Exploding Gradients\nProblem during training of RNNs where gradients shrink (vanish) or\nexplode.\nLSTM and GRU solve this with gate mechanisms.\n7.5 Generative Adversarial Networks (GANs)\nGANs are powerful models for generating new data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 30,
      "chunk": 0
    }
  },
  {
    "chunk_id": "83f29b74-4c97-4679-b015-a37ec3b1527e",
    "text": "7.5.1 Generator and Discriminator\nGenerator: Creates fake data\nDiscriminator: Tries to distinguish real from fake data\nThey compete with each other (like a forger and a detective).\n7.5.2 Training Process\nGenerator tries to fool the discriminator\nDiscriminator improves to detect fakes\nThey both improve over time — leading to realistic generated data\n7.5.3 Applications\nImage Generation (e.g., fake faces)\nArt and Style Transfer\nData Augmentation for training other ML models\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 31,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8b237777-6488-47d2-9257-ae7662196327",
    "text": "NLP helps computers understand, interpret, and generate human language.\nIt's widely used in applications like chatbots, translation tools, and voice\nassistants.\n8.1 Text Preprocessing\nBefore using text in machine learning models, we need to clean and\nconvert it into a format the computer understands.\n8.1.1 Tokenization\nBreaking text into smaller parts like words or sentences.\nExample: \"I love AI\" → [\"I\", \"love\", \"AI\"]\n8.1.2 Stopwords\nRemoving common words that do not add much meaning (like “is”, “the”,\n“and”).\n8.1.3 Stemming\nCutting words down to their root form.\nExample: “playing”, “played” → “play”\n8.1.4 Lemmatization\nSimilar to stemming but uses grammar to find the proper base word.\nExample: “better” → “good”\n8.1.5 Bag of Words (BoW)\nConverts text into numbers based on word counts in a ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7dee0c3f-bb3c-4aa7-aeb1-5f2a417759cb",
    "text": "document.\n8.1.6 TF-IDF\nGives importance to words that appear often in one document but not in\nothers. Helps identify keywords.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d3d826a3-c52f-48d9-bf58-2f5e1108bdb2",
    "text": "8.2 Word Embeddings\nWord embeddings turn words into vectors (numbers) so that a machine\ncan understand their meaning and context.\n8.2.1 Word2Vec\nA model that learns how words are related based on their surrounding\nwords.\n8.2.2 GloVe\nLearns word meanings by looking at how often words appear together.\n8.2.3 FastText\nSimilar to Word2Vec but also looks at parts of words, which helps with\nunknown words.\n8.2.4 Sentence Embeddings (BERT, RoBERTa, GPT)\nThese models convert full sentences into vectors. They understand context\nmuch better than older models.\n8.3 Sequence Models\nThese models are good for processing data where order matters, like text.\n8.3.1 RNN (Recurrent Neural Networks)\nGood for learning from sequences, such as sentences.\n8.3.2 LSTM (Long Short-Term Memory)\nAn advanced RNN that reme",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a5a3c71d-851a-4c51-b732-2191824a4397",
    "text": "mbers long-term information.\n8.3.3 GRU (Gated Recurrent Unit)\nA simpler version of LSTM that works faster and often just as well.\n8.4 Transformer Architecture\nTransformers are a powerful model used in almost all modern NLP\nsystems.\n8.4.1 Self-Attention Mechanism\nThis allows the model to focus on important words in a sentence, no\nmatter where they appear.\n8.4.2 Encoder-Decoder Model\nUsed in tasks like translation where the model reads input (encoder) and\ngenerates output (decoder).\n8.4.3 Examples:\nBERT: Great for understanding text.\nGPT: Great for generating text.\nT5: Can both understand and generate text for many tasks.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 1
    }
  },
  {
    "chunk_id": "0047ab65-816b-420b-81cc-57e593796d52",
    "text": "8. NATURAL LANGUAGE PROCESSING (NLP)\n8.5 Text Classification\nClassify text into categories.\nExamples:\nSentiment Analysis: Is a review positive or negative?\nNamed Entity Recognition (NER): Find names, places, dates, etc. in text.\n8.6 Language Generation\nGenerate new text from existing input.\n8.6.1 Text Summarization\nShortens a long document while keeping important points.\n8.6.2 Machine Translation\nTranslates text from one language to another (like English to Hindi).",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 34,
      "chunk": 0
    }
  },
  {
    "chunk_id": "19eb3669-5143-47ce-9f83-938e9d7990cf",
    "text": "9. MODEL EVALUATION AND METRICS\nModel evaluation helps us check how well our machine learning models are\nperforming. We use different metrics depending on whether it's a\nclassification or regression problem.\n9.1 Classification Metrics\nUsed when your model predicts categories or classes (e.g., spam or not\nspam).\n9.1.1 Accuracy\nHow often the model is correct.\nFormula: (Correct Predictions) / (Total Predictions)\n9.1.2 Precision\nOut of all predicted positives, how many were actually positive?\nUsed when false positives are costly.\nFormula: TP / (TP + FP)\n9.1.3 Recall (Sensitivity)\nOut of all actual positives, how many were predicted correctly?\nUsed when missing positives is costly.\nFormula: TP / (TP + FN)\n9.1.4 F1-Score\nBalance between precision and recall.\nFormula: 2 * (Precision * Recall) / (",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 0
    }
  },
  {
    "chunk_id": "358fc357-f3ce-4997-8bec-10c684899493",
    "text": "Precision + Recall)\n9.1.5 Confusion Matrix\nA table showing True Positives, False Positives, False Negatives, and True\nNegatives.\n9.1.6 ROC Curve (Receiver Operating Characteristic)\nShows the trade-off between True Positive Rate and False Positive Rate.\n9.1.7 AUC (Area Under the Curve)\nMeasures the entire area under the ROC curve.\nHigher AUC = better performance.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 1
    }
  },
  {
    "chunk_id": "99b8dfa7-8e79-4827-8a63-36084f05a551",
    "text": "9. MODEL EVALUATION AND METRICS\n9.2 Regression Metrics\nUsed when the model predicts continuous values (like house price,\ntemperature).\n9.2.1 Mean Absolute Error (MAE)\nAverage of the absolute errors.\nEasy to understand.\n9.2.2 Mean Squared Error (MSE)\nAverage of squared errors.\nPenalizes large errors more than MAE.\n9.2.3 R-Squared (R²)\nExplains how much variance in the output is explained by the model.\nRanges from 0 to 1 (higher is better).\n9.2.4 Adjusted R-Squared\nLike R², but adjusts for the number of predictors (features).\nUseful when comparing models with different numbers of features.\n9.3 Cross-Validation\nUsed to test model performance on different splits of the data.\n9.3.1 K-Fold Cross-Validation\nSplit data into k equal parts. Train on k-1 and test on the remaining part.\nRepeat k times",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 0
    }
  },
  {
    "chunk_id": "75d38866-1328-4277-b480-e4e6e66ce60e",
    "text": ".\n9.3.2 Leave-One-Out Cross-Validation (LOOCV)\nA special case of K-Fold where k = number of data points. Very slow but\nthorough.\n9.3.3 Stratified K-Fold\nSame as K-Fold, but keeps the ratio of classes the same in each fold.\nUseful for imbalanced datasets.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 1
    }
  },
  {
    "chunk_id": "2a68792f-aeb0-49fd-81d9-ec67e83f93cc",
    "text": "9. MODEL EVALUATION AND METRICS\n9.4 Hyperparameter Tuning\nHyperparameters are settings that control how a model learns (like\nlearning rate, depth of a tree, etc.).\n9.4.1 Grid Search\nTests all combinations of given hyperparameter values.\n9.4.2 Random Search\nRandomly selects combinations. Faster than Grid Search.\n9.4.3 Bayesian Optimization\nUses past results to pick the next best combination. Smart and efficient.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 37,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5fd91152-b859-47e2-a5f5-eb4e21e9110a",
    "text": "10. ADVANCED TOPICS\nThese are modern machine learning methods used in advanced real-world\napplications such as chatbots, recommendation systems, self-driving cars,\nand privacy-focused AI.\n10.1 Transfer Learning\nInstead of training a model from scratch, we use a model that has already\nbeen trained on a large dataset and apply it to a new, similar task.\nPre-trained Models\nThese are models trained on huge datasets.\nExamples:\nVGG, ResNet – for images\nBERT – for text\nFine-Tuning\nSlightly updating the pre-trained model using your own smaller dataset.\nFeature Extraction\nUsing the pre-trained model to extract useful features and then using\nthose features for your own model or task.\nBenefit: Saves training time and works well even with limited data.\n10.2 Attention Mechanism\nThis helps the model dec",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 0
    }
  },
  {
    "chunk_id": "2dec653d-6c21-40ca-9dbc-e70553e22c59",
    "text": "ide which parts of the input data are most\nimportant.\nSelf-Attention\nEvery part of the input focuses on every other part to understand context\nbetter.\nUsed in NLP (Natural Language Processing) and transformers.\nMulti-Head Attention\nApplies attention multiple times in parallel to capture different\nrelationships within the data.\nApplications\nIn NLP: translation, summarization, question-answering\nIn vision: image recognition with Vision Transformers",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 1
    }
  },
  {
    "chunk_id": "994f46b0-ce6e-426d-ac23-a3a6a40f1623",
    "text": "10.2 Attention Mechanism\n10.3 Reinforcement Learning in Deep Learning\nCombining deep learning with reinforcement learning for decision-\nmaking tasks.\nActor-Critic\nTwo models work together:\nActor: selects the best action\nCritic: evaluates how good the action was\nA3C (Asynchronous Advantage Actor-Critic)\nUses multiple agents to learn in parallel, which speeds up learning and\nincreases stability.\nPPO (Proximal Policy Optimization)\nAn improved and stable way to train reinforcement learning agents. Used\nin games, robotics, etc.\n10. ADVANCED TOPICS\nMachine\nLearning (ML)\nArtificial\nIntelligence (AI)\nDeep\nLearning (DL)\nReinforcement\nLearning (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 39,
      "chunk": 0
    }
  },
  {
    "chunk_id": "965ced60-c90b-4aaf-81f9-bf4ce9568b50",
    "text": "10.4 Federated Learning\nModel training happens across many devices without collecting data in a\ncentral server. Each device keeps its data private and only sends model\nupdates.\nDistributed Learning Frameworks\nUsed when data is spread across users, hospitals, or devices. Examples\ninclude Google’s keyboard predictions.\nPrivacy-Preserving ML\nSince data never leaves the device, user privacy is protected. This is useful\nin healthcare, banking, and personal mobile applications.\n10. ADVANCED TOPICS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 40,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b821d302-b7b1-4256-ae6b-a49017354595",
    "text": "These tools help you build, train, and deploy AI/ML models more efficiently.\nThey provide ready-to-use functions so you don’t need to code everything\nfrom scratch.\n11.1 Python Libraries\nPython is the most popular language in AI/ML. These libraries make your\nwork faster and easier:\nNumPy\nUsed for numerical computing.\nSupports arrays, matrices, and linear algebra operations.\nPandas\nUsed for data manipulation and analysis.\nYou can load data, clean it, and analyze it in tabular formats (DataFrames).\nScikit-learn\nA powerful machine learning library.\nIncludes ready-to-use models like Linear Regression, SVM, Random Forest,\nKNN, and more.\nTensorFlow & Keras\nUsed for building deep learning models.\nTensorFlow: low-level control\nKeras: high-level interface, easier to use\nPyTorch\nAn alternative to Ten",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 0
    }
  },
  {
    "chunk_id": "fb2afc51-c66b-40da-82af-95b8d023f7bb",
    "text": "sorFlow, widely used in research and development.\nIt’s flexible, fast, and dynamic (supports on-the-fly computation graphs).\nOpenCV\nUsed for computer vision tasks like image processing, object detection,\nface recognition, etc.\nNLTK & SpaCy\nNatural Language Processing (NLP) libraries.\nNLTK: good for learning, includes many basic NLP tasks\nSpaCy: industrial-strength NLP, faster and more efficient\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4c10b5b7-5cc5-4cc2-8c94-0bf43c4088c6",
    "text": "Google Colab\nFree online Jupyter Notebook\nSupports GPU/TPU\nGreat for students and beginners\nAWS SageMaker\nAmazon’s cloud ML platform\nSupports training, tuning, and deploying models at scale\nUsed in enterprise-level applications\nAzure ML\nMicrosoft’s machine learning platform\nIntegrates well with other Microsoft tools (e.g., Excel, Power BI)\nProvides autoML, drag-and-drop pipelines, and more\n11.2 Cloud Platforms\nCloud platforms help you run your models on powerful servers without\nneeding your own hardware.\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 42,
      "chunk": 0
    }
  },
  {
    "chunk_id": "154217a3-5219-4d58-9994-13aa9437b70f",
    "text": "12.1 Model Serialization\nWhat it means:\nAfter training your machine learning model, you save (serialize) it to use\nlater without retraining.\nPopular tools:\nPickle – A Python library to serialize and deserialize Python objects.\nJoblib – Similar to Pickle but better for large NumPy arrays.\nExample:\n12.2 Flask/Django for Model Deployment\nThese are web frameworks that let you expose your model as an API\nendpoint, so other apps or users can access it via the internet.\nFlask: Lightweight and easier for quick ML model APIs.\nDjango: Heavier but better for large web applications with built-in admin,\nsecurity, and ORM.\nFlask Example:\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 43,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a6a718ee-6604-4a6d-b366-672a6e0e047c",
    "text": "12.3 Serving Models with TensorFlow Serving, FastAPI\nTensorFlow Serving:\nUsed to deploy TensorFlow models in production. It supports versioning\nand high-performance serving with REST/gRPC.\nFastAPI:\nA modern, fast (high-performance) framework for building APIs with\nautomatic docs, great for production-grade ML APIs.\nFastAPI Example:\n12.4 Monitoring and Maintaining Models in Production\nOnce your model is live, you need to ensure it continues to perform well.\nWhat to monitor:\nModel accuracy degradation (due to data drift)\nResponse time\nError rates\nSystem metrics (CPU, memory)\nTools:\nPrometheus + Grafana for system and application monitoring\nMLflow or Evidently.ai for tracking model performance over time\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 44,
      "chunk": 0
    }
  },
  {
    "chunk_id": "45971416-ddb2-4cca-a65d-3e2c00865853",
    "text": "13.2 Common Beginner Mistakes\nGeneral ML Mistakes\nUsing test data during training\nNot normalizing/scaling features\nIgnoring class imbalance in classification tasks\nForgetting to check for data leakage\nNot splitting the dataset correctly (Train/Validation/Test)\nNeural Network Mistakes\nUsing too many/too few layers without tuning\nChoosing wrong activation/loss functions\nIgnoring overfitting (no dropout or regularization)\nNLP Mistakes\nFeeding raw text without preprocessing\nUsing TF-IDF on small datasets without context\nConfusing stemming with lemmatization\nDeployment Mistakes\nNot checking model performance after deployment\nIgnoring real-time latency\nNo monitoring/logging in place\n13.1 Practice Tasks\nTo strengthen understanding, here are simple practice tasks for each core\nconcept:\n13. PRACTIC",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7d7ae913-e76d-413a-856e-7722b2fc2890",
    "text": "E & COMMON BEGINNER MISTAKES\nNLP\nPCA\nTopic\nGANs\nCNNs\nRNNs\nDecision Trees\nLinear Regression\nLogistic Regression\nK-Means Clustering\nMini Practice Task\nClassify hand-written digits using the MNIST dataset.\nBuild a sentiment analysis model on product reviews.\nPredict the next word in a sentence using a small corpus.\nGenerate new handwritten digits after training on MNIST.\nReduce dimensions in the Iris dataset and visualize clusters.\nClassify if a person will buy a product based on age, income, etc.\nGroup customers by shopping behavior (customer segmentation).\nPredict house prices using a dataset with features like area, rooms,\nand location.\nBuild a binary classifier to detect spam emails.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 1
    }
  },
  {
    "chunk_id": "1375cf3b-c442-420f-85fb-cc2f3c437235",
    "text": "AI/ML\nCheatSheet",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "fa792447-d192-4ef4-90a0-abc5d204fd98",
    "text": "1. Introduction to AI and ML\nWhat is Artificial Intelligence (AI)?\nTypes of AI: Narrow AI vs. General AI\nWhat is Machine Learning (ML)?\nSupervised vs. Unsupervised vs. Reinforcement Learning\nKey Terminologies (Model, Feature, Target, Algorithm)\nApplications of AI and ML\nDifference Between AI, ML, and Deep Learning (DL)\n2. Mathematics for ML/AI\nLinear Algebra\nVectors, Matrices, and Tensors\nMatrix Operations\nEigenvalues and Eigenvectors\nProbability and Statistics\nMean, Variance, Standard Deviation\nBayes Theorem\nConditional Probability\nProbability Distributions (Normal, Binomial, Poisson)\nCalculus for Optimization\nDerivatives and Gradients\nGradient Descent\n3. Data Preprocessing\nData Cleaning (Missing Values, Outliers)\nData Normalization and Standardization\nEncoding Categorical Variables (One-",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b1b6c6dd-6acc-4da4-83c9-27783589c9c8",
    "text": "Hot Encoding, Label Encoding)\nFeature Scaling (Min-Max, Z-score)\nFeature Engineering (Polynomial Features, Binning)\nHandling Imbalanced Data (SMOTE, Undersampling, Oversampling)\n4. Supervised Learning Algorithms\nLinear Regression\nSimple vs. Multiple Linear Regression\nGradient Descent and Normal Equation\nRegularization (L1, L2)\nLogistic Regression\nBinary vs. Multiclass Classification\nSigmoid Function and Cost Function\nRegularization\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "b2680b55-396c-4b08-89f9-4f26575148ae",
    "text": "4. Supervised Learning Algorithms\nK-Nearest Neighbors (KNN)\nDistance Metrics (Euclidean, Manhattan)\nChoosing K\nAdvantages and Disadvantages\nSupport Vector Machines (SVM)\nHyperplanes and Margins\nLinear and Non-Linear SVM\nKernel Trick\nDecision Trees\nGini Impurity and Entropy\nOverfitting and Pruning\nRandom Forest\nBootstrapping\nBagging\nFeature Importance\nGradient Boosting Machines (GBM)\nXGBoost, LightGBM, CatBoost\nHyperparameter Tuning\nEarly Stopping\nNaive Bayes\nGaussian, Multinomial, Bernoulli Naive Bayes\nAssumptions and Applications\n5. Unsupervised Learning Algorithms\nK-Means Clustering\nAlgorithm Overview\nElbow Method\nK-Means++ Initialization\nHierarchical Clustering\nAgglomerative vs. Divisive Clustering\nDendrogram and Optimal Cut\nPrincipal Component Analysis (PCA)\nDimensionality Reduction\nEi",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "dea8d25e-991d-4b20-9450-6ed40f86555d",
    "text": "genvalue Decomposition\nScree Plot and Explained Variance\nDBSCAN\nDensity-Based Clustering\nEpsilon and MinPts Parameters\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4f773640-5a58-423f-85f3-cda4016d923f",
    "text": "6. Reinforcement Learning\nIntroduction to Reinforcement Learning\nKey Concepts (Agent, Environment, State, Action, Reward)\nMarkov Decision Process (MDP)\nQ-Learning and Deep Q-Networks (DQN)\nPolicy Gradients and Actor-Critic Methods\nExploration vs. Exploitation\n7. Neural Networks & Deep Learning\nIntroduction to Neural Networks\nPerceptrons\nActivation Functions (Sigmoid, ReLU, Tanh)\nForward Propagation and Backpropagation\nLoss Functions (MSE, Cross-Entropy)\nDeep Neural Networks (DNN)\nArchitecture and Layers\nTraining Process and Optimizers\nOverfitting and Regularization (Dropout, L2 Regularization)\nConvolutional Neural Networks (CNN)\nConvolutional Layers, Pooling Layers\nFilter/Kernels and Strides\nApplications (Image Classification, Object Detection)\nRecurrent Neural Networks (RNN)\nBasic RNN vs.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ca36bd58-80e7-41c6-ab5c-bcfc57e8e5ba",
    "text": " LSTM vs. GRU\nTime-Series Prediction and NLP Applications\nVanishing and Exploding Gradients Problem\nGenerative Adversarial Networks (GANs)\nGenerator and Discriminator\nTraining Process\nApplications (Image Generation, Data Augmentation)\n8. Natural Language Processing (NLP)\nText Preprocessing\nTokenization, Stopwords, Lemmatization, Stemming\nBag of Words, TF-IDF\nWord Embeddings\nWord2Vec, GloVe, FastText\nSentence Embeddings (BERT, RoBERTa, GPT)\nSequence Models\nRecurrent Neural Networks (RNNs)\nLSTM and GRU\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4d072fd9-164e-4886-9052-6c732e640b06",
    "text": "8. Natural Language Processing (NLP)\nTransformer Architecture\nSelf-Attention Mechanism\nEncoder-Decoder Model\nBERT, GPT, T5 Models\nText Classification\nSentiment Analysis, Named Entity Recognition (NER)\nLanguage Generation\nText Summarization\nMachine Translation\n9. Model Evaluation and Metrics\nClassification Metrics\nAccuracy, Precision, Recall, F1-Score\nConfusion Matrix\nROC Curve, AUC\nRegression Metrics\nMean Absolute Error (MAE), Mean Squared Error (MSE)\nR-Squared and Adjusted R-Squared\nCross-Validation\nK-Fold Cross-Validation\nLeave-One-Out Cross-Validation\nStratified K-Fold\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\n10. Advanced Topics\nTransfer Learning\nPre-trained Models (VGG, ResNet, BERT)\nFine-Tuning and Feature Extraction\nAttention Mechanism\nSelf-Attention, Mul",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d30b0936-13ab-4963-9d2d-220807de147b",
    "text": "ti-Head Attention\nApplications in NLP and Vision\nReinforcement Learning in Deep Learning\nActor-Critic, A3C, Proximal Policy Optimization (PPO)\nFederated Learning\nDistributed Learning Frameworks\nPrivacy-Preserving ML\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e17d5722-1481-42e3-bffd-10f3cbffaae9",
    "text": "11. Tools and Libraries for AI/ML\nPython Libraries\nNumPy, Pandas\nScikit-learn\nTensorFlow, Keras, PyTorch\nOpenCV for Computer Vision\nNLTK, SpaCy for NLP\nCloud Platforms\nGoogle Colab\nAWS Sagemaker\nAzure ML\n12. Deployment and Production\nModel Serialization (Pickle, Joblib)\nFlask/Django for Model Deployment\nServing Models with TensorFlow Serving, FastAPI\nMonitoring and Maintaining Models in Production\n13. Practice & Common Beginner Mistakes\nPractice Tasks\nCommon Beginner Mistakes\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d7cd1f91-d300-4ca5-bce0-53f1bcf29ff2",
    "text": "1. INTRODUCTION TO AI AND ML\n1.1 What is Artificial Intelligence (AI)?\nArtificial Intelligence (AI) is the ability of machines or computer programs\nto perform tasks that typically require human intelligence. These tasks can\ninclude understanding language, recognizing patterns, solving problems,\nand making decisions.\nSimple explanation:\nAI is when machines are made smart enough to think and act like\nhumans.\nExamples:\nVoice assistants like Alexa\nImage recognition systems\nChatbots\nSelf-driving cars\n1.2 Types of AI: Narrow AI vs. General AI\nNarrow AI (Weak AI):\nDesigned to perform one specific task\nCannot do anything beyond its programming\nExamples: Email spam filters, facial recognition, recommendation systems\nGeneral AI (Strong AI):\nStill under research and development\nCan learn and perform ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d143dba9-efd1-48d0-914a-4446159af184",
    "text": "any intellectual task a human can do\nWould have reasoning, memory, and decision-making abilities similar to a\nhuman\n1.3 What is Machine Learning (ML)?\nMachine Learning is a subfield of AI that allows machines to learn from\ndata and improve their performance over time without being explicitly\nprogrammed.\nSimple explanation:\nInstead of writing rules for everything, we give the machine data, and it\nfigures out the rules on its own.\nExample:\nA machine learns to identify spam emails by studying thousands of\nexamples.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7d715ec3-94f4-4c7d-b03b-6f9821ea05a1",
    "text": "1. INTRODUCTION TO AI AND ML\n1.4 Supervised vs. Unsupervised vs. Reinforcement Learning\nSupervised Learning:\nThe training data includes both input and the correct output (labels)\nThe model learns by comparing its output with the correct output\nExample: Predicting house prices based on features like size, location, etc.\nUnsupervised Learning:\nThe data has no labels\nThe model tries to find patterns or groupings in the data\nExample: Grouping customers based on purchasing behavior\nReinforcement Learning:\nThe model learns through trial and error\nIt receives rewards or penalties based on its actions\nExample: A robot learning to walk or a program learning to play chess\n1.5 Key Terminologies\nModel:\nA program or function that makes predictions or decisions based on data.\nFeature:\nAn input variable ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d5062af9-add0-4ee4-80c3-398053201ffe",
    "text": "used in making predictions (e.g., age, income,\ntemperature).\nTarget:\nThe value the model is trying to predict (e.g., house price, spam or not).\nAlgorithm:\nA step-by-step method or set of rules used to train the model.\nTraining:\nThe process of teaching the model using a dataset.\nTesting:\nEvaluating the trained model on new, unseen data to measure its\naccuracy.\n1.6 Applications of AI and ML\nRecommendation systems (YouTube, Amazon, Netflix)\nFraud detection in banking\nLanguage translation\nHealthcare diagnosis\nSelf-driving vehicles\nStock market prediction\nChatbots and customer support\nSocial media content moderation",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 1
    }
  },
  {
    "chunk_id": "05837196-aacb-4e87-b3d9-c107587961bf",
    "text": "Term\nDeep Learning (DL)\nMachine Learning (ML)\nArtificial Intelligence (AI)\nDescription\nA subset of AI where machines learn from data\nThe overall field focused on creating intelligent machines\nA specialized type of ML that uses neural networks inspired by the\nhuman brain\n1. INTRODUCTION TO AI AND ML\nDeep Learning\nMachine Learning\nArtificial Intelligence\n1.7 Difference Between AI, ML, and Deep Learning (DL)\nVisual analogy:\nAI is the broader concept, ML is a part of AI, and DL is a part of ML.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e436cf76-5966-4fac-ba35-6fd250bdbfd6",
    "text": "Mathematics is the foundation of AI and Machine Learning. It helps us\nunderstand how algorithms work under the hood and how to fine-tune\nmodels for better performance.\n2.1 Linear Algebra\nLinear Algebra deals with numbers organized in arrays and how these\narrays interact. It is used in almost every ML algorithm.\n2.1.1 Vectors, Matrices, and Tensors\nVector: A 1D array of numbers. Example: [3, 5, 7]\nUsed to represent features like height, weight, age.\nMatrix: A 2D array (rows and columns).\nExample:\nUsed to store datasets or model weights.\nTensor: A generalization of vectors and matrices to more dimensions (3D\nor higher).\nExample: Used in deep learning models like images (3D tensor: width,\nheight, color channels).\n2.1.2 Matrix Operations\nAddition/Subtraction: Add or subtract corresponding elem",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "001d6d58-5f8d-4d08-a4cf-6a7b3273a98b",
    "text": "ents of two\nmatrices.\nMultiplication: Used to combine weights and inputs in ML models.\nTranspose: Flip a matrix over its diagonal.\nDot Product: Fundamental in calculating output in neural networks.\n2.1.3 Eigenvalues and Eigenvectors\nEigenvector: A direction that doesn't change during a transformation.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk.\nThese are used in algorithms like Principal Component Analysis (PCA) for\ndimensionality reduction.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d929efd8-41cd-4dfb-a84f-f7cc73bf399f",
    "text": "2.2 Probability and Statistics\nProbability helps machines make decisions under uncertainty, and\nstatistics helps us understand data and model performance.\n2.2.1 Mean, Variance, Standard Deviation\nMean: The average value.\nVariance: How spread out the values are from the mean.\nStandard Deviation: The square root of variance. It measures how much\nthe values vary.\nUsed to understand the distribution and behavior of features in datasets.\n2.2.2 Bayes Theorem\nA mathematical formula to calculate conditional probability:\nUsed in Naive Bayes classifiers for spam detection, document classification,\netc.\n2.2.3 Conditional Probability\nThe probability of one event occurring given that another event has\nalready occurred.\nExample:\nProbability that a user clicks an ad given that they are between 20-30\nyear",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ee7a6804-bfa5-4b11-a111-07667a5304e7",
    "text": "s old.\n2.2.4 Probability Distributions\nNormal Distribution: Bell-shaped curve. Common in real-world data like\nheight, exam scores.\nBinomial Distribution: Used for yes/no type outcomes. Example: Flipping a\ncoin 10 times.\nPoisson Distribution: For events happening over a time period. Example:\nNumber of customer calls per hour.\nThese distributions help in modeling randomness in data.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 1
    }
  },
  {
    "chunk_id": "464d7639-7912-4e0a-828f-fb505f300e08",
    "text": "2.3 Calculus for Optimization\nCalculus helps in training models by optimizing them to reduce errors.\n2.3.1 Derivatives and Gradients\nDerivative: Measures how a function changes as its input changes.\nGradient: A vector of derivatives that tells the slope of a function in multi-\ndimensions.\nUsed to find the direction in which the model should adjust its weights.\n2.3.2 Gradient Descent\nAn optimization algorithm used to minimize the loss (error) function.\nHow it works:\nStart with random values\nCalculate the gradient (slope)\nMove slightly in the opposite direction of the gradient\nRepeat until the loss is minimized\nGradient Descent is the core of many training algorithms in ML and DL.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e3bfe6d8-5602-4c9c-b530-bc4b9c047c78",
    "text": "Used when data is normally distributed\nUsed when the data is not normally distributed\nStandardization (Z-score Scaling):\nCenters the data around mean = 0 and standard deviation = 1\nFormula:\nBefore feeding data into any machine learning model, it must be cleaned,\ntransformed, and prepared. This step is called data preprocessing, and it is\none of the most important stages in building accurate ML models.\n3.1 Data Cleaning\nReal-world data is often messy. Data cleaning means identifying and fixing\nerrors in the dataset.\n3.1.1 Missing Values\nMissing values can be due to incomplete forms, sensor errors, etc.\nTechniques to handle missing data:\nRemove rows/columns with too many missing values\nFill (impute) missing values using:\nMean/Median/Mode\nForward/Backward fill\nPredictive models (like KNN)\n3.1",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7df8b840-ab16-48bf-bc66-d164c3a17819",
    "text": ".2 Outliers\nOutliers are data points that are very different from others.\nThey can distort results and reduce model performance.\nDetection methods:\nBox plot, Z-score, IQR method\nHandling outliers:\nRemove them\nTransform data (e.g., log scaling)\nCap them (set a maximum/minimum)\n3.2 Data Normalization and Standardization\nHelps scale numeric data so that features contribute equally to the model.\nNormalization (Min-Max Scaling):\nScales all values between 0 and 1\nFormula:\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 1
    }
  },
  {
    "chunk_id": "8bba5cd1-5b5d-45f1-8039-a9ca29cd3a13",
    "text": "One-Hot Encoding:\nCreates new binary columns for each category.\nExample:\n3.3 Encoding Categorical Variables\nML models work with numbers, not text. Categorical data needs to be\nconverted into numerical form.\nLabel Encoding:\nAssigns each unique category a number.\nExample:\nLabel encoding is good for ordinal data (ranked), while one-hot encoding\nis best for nominal data (non-ranked).\n3.4 Feature Scaling\nEnsures features are on the same scale so the model can learn effectively.\nMin-Max Scaling:\nScales features between 0 and 1.\nGood for algorithms like KNN, neural networks.\nZ-score Scaling (Standardization):\nUseful for models that assume normality, like linear regression or logistic\nregression.\nScaling is crucial for models that use distance or gradient-based optimization.\n3.5 Feature Engineerin",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "297af891-224d-494e-846c-47e7eb83cb04",
    "text": "g\nCreating new features or modifying existing ones to improve model\nperformance.\nPolynomial Features:\nCreate new features by raising existing features to a power.\nExample: From x, create x², x³\nBinning (Discretization):\nConvert continuous data into categories.\nExample: Age → [0–18], [19–35], [36–60], 60+\nFeature engineering can significantly boost the predictive power of a model.\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9a6aa79d-08c8-4fa9-9fb2-89ac7502685b",
    "text": "3. DATA PREPROCESSING\n3.6 Handling Imbalanced Data\nIn classification, if one class dominates (e.g., 95% non-fraud, 5% fraud),\nmodels may ignore the minority class. This is called class imbalance.\nSMOTE (Synthetic Minority Oversampling Technique):\nCreates synthetic examples of the minority class using nearest neighbors.\nUndersampling:\nRemove some samples from the majority class.\nOversampling:\nDuplicate or generate more samples of the minority class.\nBalancing data improves the ability of the model to correctly predict both\nclasses.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d980906a-9180-4e49-adcb-db3848f294d9",
    "text": "Works for small datasets.\n4.1.1 Simple vs. Multiple Linear Regression\nSimple Linear Regression: One input (X) to predict one output (Y).\nExample: Predicting salary from years of experience.\nMultiple Linear Regression: Multiple inputs (X1, X2, ..., Xn).\nExample: Predicting price based on area, location, and age.\n4.1.2 Gradient Descent and Normal Equation\nGradient Descent: Iterative method to minimize error (cost function).\nNormal Equation: Direct way to find weights using linear algebra:\nSupervised learning uses labeled data, meaning the model learns from input-\noutput pairs (X → y). The algorithm tries to map inputs (features) to correct\noutputs (targets/labels).\n4.1 Linear Regression\nUsed for predicting continuous values (e.g., predicting house price,\ntemperature).\n4. SUPERVISED LEARNING ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b1e9f258-341c-4d91-a421-71fe2cea2028",
    "text": "ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 1
    }
  },
  {
    "chunk_id": "cae5531a-e38f-4469-b57f-6d1887e9ca10",
    "text": "4.1.3 Regularization (L1, L2)\nPrevents overfitting by adding a penalty:\nL1 (Lasso): Can reduce coefficients to 0 (feature selection).\nL2 (Ridge): Shrinks coefficients but doesn’t make them 0.\n4.2 Logistic Regression\nUsed for classification problems (e.g., spam vs. not spam).\n4.2.1 Binary vs. Multiclass Classification\nBinary: 2 outcomes (e.g., 0 or 1)\nMulticlass: More than 2 classes (handled using One-vs-Rest or Softmax)\n4.2.2 Sigmoid and Cost Function\nSigmoid Function: Converts outputs to values between 0 and 1.\nCost Function: Log loss used to measure prediction error.\n4.2.3 Regularization\nL1 and L2 regularization help prevent overfitting in logistic regression as\nwell.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "58f8d4b5-4b20-48eb-8904-da1b95dbcf65",
    "text": "4.3.1 Distance Metrics\nEuclidean Distance: Straight line between two points.\nManhattan Distance: Sum of absolute differences.\n4.3.2 Choosing K\nK is the number of neighbors to consider.\nToo low K → sensitive to noise\nToo high K → model becomes less flexible\n4.3.3 Advantages & Disadvantages\nSimple and easy to implement\nSlow for large datasets, sensitive to irrelevant features\n4.4 Support Vector Machines (SVM)\nPowerful classification model for small to medium-sized datasets.\n4.3 K-Nearest Neighbors (KNN)\nA simple classification (or regression) algorithm that uses proximity.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4f2a5b0f-36c8-480c-a0ff-723881736622",
    "text": "4.5.1 Gini Impurity and Entropy\nMeasures how pure a node is:\nGini Impurity: Probability of misclassification.\nEntropy: Measure of randomness/information.\n4.5.2 Overfitting and Pruning\nOverfitting: Tree memorizes training data.\nPruning: Removes unnecessary branches to reduce overfitting.\n4.4.1 Hyperplanes and Margins\nSVM finds the best hyperplane that separates data with maximum\nmargin.\n4.4.2 Linear vs. Non-Linear SVM\nLinear SVM: Works when data is linearly separable.\nNon-linear SVM: Uses kernel trick for complex datasets.\n4.4.3 Kernel Trick\nTransforms data into higher dimensions to make it separable.\nCommon kernels: RBF (Gaussian), Polynomial, Sigmoid\n4.5 Decision Trees\nTree-like structure used for classification and regression.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9c9d3951-5208-42f7-b5c3-92b85ccfaa3a",
    "text": "4.7.1 XGBoost, LightGBM, CatBoost\nAdvanced boosting libraries:\nXGBoost: Popular, fast, and accurate\nLightGBM: Faster, uses leaf-wise growth\nCatBoost: Handles categorical features automatically\n4.6.1 Bootstrapping\nRandomly selects subsets of data to train each tree.\n4.6.2 Bagging\nCombines predictions of multiple trees (majority vote or average).\n4.6.3 Feature Importance\nMeasures which features contribute most to model prediction.\n4.7 Gradient Boosting Machines (GBM)\nBoosting is an ensemble method where models are trained sequentially.\n4.6 Random Forest\nAn ensemble of decision trees to improve accuracy and reduce overfitting.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 20,
      "chunk": 0
    }
  },
  {
    "chunk_id": "56c25afe-b369-4b2b-b84f-6d43d7af9e58",
    "text": "4.8.1 Gaussian, Multinomial, Bernoulli\nGaussian NB: For continuous features (assumes normal distribution)\nMultinomial NB: For text data, counts of words\nBernoulli NB: For binary features (0/1)\n4.8.2 Assumptions and Applications\nAssumes all features are independent (rarely true but still works well)\nCommonly used in spam detection, sentiment analysis, document\nclassif ication\n4.7.2 Hyperparameter Tuning\nAdjust parameters like:\nLearning rate\nNumber of estimators (trees)\nMax depth\nTools: GridSearchCV, RandomSearchCV\n4.7.3 Early Stopping\nStops training if the model stops improving on validation set.\n4.8 Naive Bayes\nProbabilistic classifier based on Bayes' Theorem and strong independence\nassumption.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "49ac116c-c6b6-4024-9baa-8103fc63eb1a",
    "text": "Unsupervised learning finds hidden patterns in unlabeled data. Unlike\nsupervised learning, it doesn't rely on labeled outputs (no predefined target).\n5.1 K-Means Clustering\n5.1.1 Algorithm Overview\nK-Means is a clustering algorithm that divides data into K clusters based\non similarity.\nIt works by:\na.Selecting K random centroids.\nb.Assigning each point to the nearest centroid.\nc.Updating the centroid to the mean of its assigned points.\nd.Repeating steps 2–3 until the centroids stop changing.\n5.1.2 Elbow Method\nUsed to choose the optimal number of clusters (K).\nPlot the number of clusters (K) vs. Within-Cluster-Sum-of-Squares (WCSS).\nThe point where the WCSS curve bends (elbow) is the best K.\n5.1.3 K-Means++ Initialization\nImproves basic K-Means by smartly selecting initial centroids, reduc",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ac0349a7-0125-43ec-ad37-990aa0b363e1",
    "text": "ing the\nchance of poor clustering.\nStarts with one random centroid, then selects the next ones based on\ndistance from the current ones (probabilistically).\n5.2 Hierarchical Clustering\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 1
    }
  },
  {
    "chunk_id": "f327f367-26a2-48a2-b861-7dd5a092e273",
    "text": "5.2.1 Agglomerative vs. Divisive Clustering\nAgglomerative (bottom-up): Start with each point as its own cluster and\nmerge the closest clusters.\nDivisive (top-down): Start with one large cluster and recursively split it.\nAgglomerative is more commonly used.\n5.2.2 Dendrogram and Optimal Cut\nA dendrogram is a tree-like diagram that shows how clusters are formed\nat each step.\nThe height of branches represents the distance between clusters.\nCutting the dendrogram at a certain height gives the desired number of\nclusters.\n5.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique used to simplify datasets\nwhile retaining most of the important information.\n5.3.1 Dimensionality Reduction\nPCA transforms the data into a new coordinate system with fewer\ndimensions (called princ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4e55c119-35df-49d6-ab91-4ded304b5dad",
    "text": "ipal components).\nUseful for visualization, speeding up algorithms, and avoiding the curse of\ndimensionality.\n5.3.2 Eigenvalue Decomposition\nPCA is based on eigenvectors and eigenvalues of the covariance matrix of\nthe data.\nThe eigenvectors define the new axes (principal components).\nThe eigenvalues indicate the amount of variance each component\ncaptures.\n5.3.3 Scree Plot and Explained Variance\nScree Plot: A plot of eigenvalues to help decide how many components to\nkeep.\nThe explained variance ratio shows how much of the data’s variance is\ncaptured by each component.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 1
    }
  },
  {
    "chunk_id": "044ab419-76d2-4731-b5c9-395e2a5c1a48",
    "text": "5.4.1 Density-Based Clustering\nUnlike K-Means, DBSCAN doesn't require specifying the number of\nclusters.\nClusters are formed based on dense regions in the data.\n5.4.2 Epsilon and MinPts Parameters\nEpsilon (ε): Radius around a point to search for neighbors.\nMinPts: Minimum number of points required to form a dense region.\nPoints are classified as:\nCore Point: Has MinPts within ε.\nBorder Point: Not a core but within ε of a core.\nNoise: Neither core nor border.\n5.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups closely\npacked points and marks outliers as noise.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 24,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1befe291-e122-4f6c-bf80-e696b467214d",
    "text": "6.2 Key Concepts\nAgent: The learner or decision maker (e.g., a robot, self-driving car).\nEnvironment: Everything the agent interacts with (e.g., a maze, a game).\nState: A snapshot of the current situation (e.g., position in a maze).\nAction: A move or decision made by the agent (e.g., turn left).\nReward: Feedback from the environment (e.g., +1 for reaching goal, -1 for\nhitting a wall).\nThe goal of the agent is to maximize cumulative rewards over time.\n6.3 Markov Decision Process (MDP)\nRL problems are often modeled as Markov Decision Processes (MDPs).\nAn MDP includes: S: Set of states A: Set of actions P: Transition\nprobabilities (P(s' | s, a)) R: Reward function γ (gamma): Discount factor\n(how much future rewards are valued) The \"Markov\" property means the\nnext state only depends on the cur",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 0
    }
  },
  {
    "chunk_id": "bf640ce6-5134-4ea9-86d6-62ab6b2a4114",
    "text": "rent state and action, not on previous\nones.\n6.1 Introduction to Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent\nlearns to make decisions by interacting with an environment. It is inspired\nby how humans learn from experience — by trial and error.\nThe agent performs an action\nThe environment responds with a reward\nThe agent uses this feedback to learn better actions over time\nUnlike supervised learning, RL doesn’t rely on labeled data. Instead, it uses\nrewards or penalties to guide learning.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 1
    }
  },
  {
    "chunk_id": "cb58d11b-baab-452a-b129-2cec9e0ac968",
    "text": "Where:\ns: current state\na: action\nr: reward\ns': next state\nα: learning rate\nγ: discount factor\nDeep Q-Networks (DQN):\nWhen the state/action space is too large, a neural network is used to\napproximate Q-values.\nCombines Q-Learning with Deep Learning.\nUsed in Atari games and robotics.\n6.4 Q-Learning and Deep Q-Networks (DQN)\nQ-Learning:\nA model-free algorithm that learns the value (Q-value) of taking an action\nin a state.\nUses the formula:\n6.5 Policy Gradients and Actor-Critic Methods\nPolicy Gradients:\nInstead of learning value functions, learn the policy directly (probability\ndistribution over actions).\nUse gradient ascent to improve the policy based on the reward received.\nActor-Critic Methods:\nCombine the best of both worlds:\nActor: chooses the action\nCritic: evaluates how good the action",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 0
    }
  },
  {
    "chunk_id": "378fde7f-6f7b-49c3-bcb0-69b051299658",
    "text": " was (value function)\nMore stable and efficient than pure policy gradients.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 1
    }
  },
  {
    "chunk_id": "dc07f4f0-d4e7-44f3-acfc-9e0c53e64d6a",
    "text": "6.6 Exploration vs. Exploitation\nExploration: Trying new actions to discover their effects (important early in\ntraining).\nExploitation: Choosing the best-known action for maximum reward.\nRL must balance both:\nToo much exploration = slow learning\nToo much exploitation = stuck in local optimum\nCommon strategy: ε-greedy\nChoose a random action with probability ε\nOtherwise, choose the best-known action\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 27,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c94f5283-635b-4d7b-88c8-df340e67f7a8",
    "text": "7.1 Introduction to Neural Networks\nA neural network is a computer model inspired by the human brain. It\nconsists of neurons (nodes) organized in layers, capable of learning\npatterns from data.\n7.1.1 Perceptrons\nThe perceptron is the simplest type of neural network, with:\nInputs → Weights → Summation → Activation Function → Output\nIt's like a yes/no decision maker (binary classification).\n7.1.2 Activation Functions\nThese introduce non-linearity, allowing the network to learn complex\nfunctions:\nSigmoid: Outputs between 0 and 1. Good for probability-based outputs.\nReLU (Rectified Linear Unit): Most popular. Fast, reduces vanishing\ngradient.\nReLU(x) = max(0, x)\nTanh: Like sigmoid but outputs between -1 and 1.\n7.1.3 Forward Propagation and Backpropagation\nForward Propagation: Input data flows ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5036cd7e-055a-4f09-907e-f568e16c855f",
    "text": "through the network to produce an\noutput.\nBackpropagation: Calculates the error and updates weights using\ngradients (from loss function).\nThis is how neural networks learn from data.\n7.1.4 Loss Functions\nThey measure how far off the prediction is from the actual result.\nMSE (Mean Squared Error): Used in regression problems.\nCross-Entropy Loss: Used in classification tasks.\n7.2 Deep Neural Networks (DNN)\nA Deep Neural Network has multiple hidden layers between input and\noutput.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9eb870fd-e558-41d0-a452-3b2e9beab393",
    "text": "7.3.1 Convolutional Layers, Pooling Layers\nConvolutional Layers: Apply filters to detect features (edges, corners).\nPooling Layers: Reduce size of feature maps (e.g., Max Pooling).\n7.3.2 Filters/Kernels and Strides\nFilters: Small matrix to slide over input to extract features.\nStrides: Step size of the filter as it moves.\n7.3.3 Applications\nImage Classification\nFace Recognition\nObject Detection\n7.2.1 Architecture and Layers\nInput Layer: Where the data comes in\nHidden Layers: Where computation happens (many neurons per layer)\nOutput Layer: Final predictions\n7.2.2 Training Process and Optimizers\nDuring training, the network:\nMakes predictions\nCalculates the loss\nUpdates weights via optimizers like:\nSGD (Stochastic Gradient Descent)\nAdam (adaptive learning rate)\nRMSProp\n7.2.3 Overfitting and ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ddbe3c3a-2e4c-4719-bac7-30ae6e414ad1",
    "text": "Regularization\nOverfitting happens when the model learns noise instead of patterns.\nRegularization techniques help:\nDropout: Randomly turns off neurons during training.\nL2 Regularization: Penalizes large weights (weight decay).\n7.3 Convolutional Neural Networks (CNN)\nCNNs are specialized for image data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 1
    }
  },
  {
    "chunk_id": "5f5db1a5-3f94-4786-b887-b77449ec16bb",
    "text": "7.4 Recurrent Neural Networks (RNN)\nRNNs are designed for sequential data (time series, text, etc.).\n7.4.1 Basic RNN vs. LSTM vs. GRU\nBasic RNN: Loops through time steps but suffers from memory issues.\nLSTM (Long Short-Term Memory): Handles long dependencies well.\nGRU (Gated Recurrent Unit): Similar to LSTM but faster.\n7.4.2 Time-Series Prediction and NLP Applications\nPredict stock prices, weather, or language sequences.\nUsed in chatbots, translation, and speech recognition.\n7.4.3 Vanishing and Exploding Gradients\nProblem during training of RNNs where gradients shrink (vanish) or\nexplode.\nLSTM and GRU solve this with gate mechanisms.\n7.5 Generative Adversarial Networks (GANs)\nGANs are powerful models for generating new data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 30,
      "chunk": 0
    }
  },
  {
    "chunk_id": "286e7ae8-2f8b-4b43-945b-fb503580cd96",
    "text": "7.5.1 Generator and Discriminator\nGenerator: Creates fake data\nDiscriminator: Tries to distinguish real from fake data\nThey compete with each other (like a forger and a detective).\n7.5.2 Training Process\nGenerator tries to fool the discriminator\nDiscriminator improves to detect fakes\nThey both improve over time — leading to realistic generated data\n7.5.3 Applications\nImage Generation (e.g., fake faces)\nArt and Style Transfer\nData Augmentation for training other ML models\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 31,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f4dce19e-54b7-4962-b1c4-4a6eb515c89d",
    "text": "NLP helps computers understand, interpret, and generate human language.\nIt's widely used in applications like chatbots, translation tools, and voice\nassistants.\n8.1 Text Preprocessing\nBefore using text in machine learning models, we need to clean and\nconvert it into a format the computer understands.\n8.1.1 Tokenization\nBreaking text into smaller parts like words or sentences.\nExample: \"I love AI\" → [\"I\", \"love\", \"AI\"]\n8.1.2 Stopwords\nRemoving common words that do not add much meaning (like “is”, “the”,\n“and”).\n8.1.3 Stemming\nCutting words down to their root form.\nExample: “playing”, “played” → “play”\n8.1.4 Lemmatization\nSimilar to stemming but uses grammar to find the proper base word.\nExample: “better” → “good”\n8.1.5 Bag of Words (BoW)\nConverts text into numbers based on word counts in a ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4c446307-ef36-4514-839b-90df7b937764",
    "text": "document.\n8.1.6 TF-IDF\nGives importance to words that appear often in one document but not in\nothers. Helps identify keywords.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 1
    }
  },
  {
    "chunk_id": "b6055a09-efa3-485f-ba66-8eff5335cedc",
    "text": "8.2 Word Embeddings\nWord embeddings turn words into vectors (numbers) so that a machine\ncan understand their meaning and context.\n8.2.1 Word2Vec\nA model that learns how words are related based on their surrounding\nwords.\n8.2.2 GloVe\nLearns word meanings by looking at how often words appear together.\n8.2.3 FastText\nSimilar to Word2Vec but also looks at parts of words, which helps with\nunknown words.\n8.2.4 Sentence Embeddings (BERT, RoBERTa, GPT)\nThese models convert full sentences into vectors. They understand context\nmuch better than older models.\n8.3 Sequence Models\nThese models are good for processing data where order matters, like text.\n8.3.1 RNN (Recurrent Neural Networks)\nGood for learning from sequences, such as sentences.\n8.3.2 LSTM (Long Short-Term Memory)\nAn advanced RNN that reme",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f96731f3-5ce4-44fc-83a1-6640a9879a69",
    "text": "mbers long-term information.\n8.3.3 GRU (Gated Recurrent Unit)\nA simpler version of LSTM that works faster and often just as well.\n8.4 Transformer Architecture\nTransformers are a powerful model used in almost all modern NLP\nsystems.\n8.4.1 Self-Attention Mechanism\nThis allows the model to focus on important words in a sentence, no\nmatter where they appear.\n8.4.2 Encoder-Decoder Model\nUsed in tasks like translation where the model reads input (encoder) and\ngenerates output (decoder).\n8.4.3 Examples:\nBERT: Great for understanding text.\nGPT: Great for generating text.\nT5: Can both understand and generate text for many tasks.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 1
    }
  },
  {
    "chunk_id": "07c74a39-c144-45f9-bc0e-429b6ef96149",
    "text": "8. NATURAL LANGUAGE PROCESSING (NLP)\n8.5 Text Classification\nClassify text into categories.\nExamples:\nSentiment Analysis: Is a review positive or negative?\nNamed Entity Recognition (NER): Find names, places, dates, etc. in text.\n8.6 Language Generation\nGenerate new text from existing input.\n8.6.1 Text Summarization\nShortens a long document while keeping important points.\n8.6.2 Machine Translation\nTranslates text from one language to another (like English to Hindi).",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 34,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9b21dcdd-5f34-40bf-a570-c1bd9141a8d6",
    "text": "9. MODEL EVALUATION AND METRICS\nModel evaluation helps us check how well our machine learning models are\nperforming. We use different metrics depending on whether it's a\nclassification or regression problem.\n9.1 Classification Metrics\nUsed when your model predicts categories or classes (e.g., spam or not\nspam).\n9.1.1 Accuracy\nHow often the model is correct.\nFormula: (Correct Predictions) / (Total Predictions)\n9.1.2 Precision\nOut of all predicted positives, how many were actually positive?\nUsed when false positives are costly.\nFormula: TP / (TP + FP)\n9.1.3 Recall (Sensitivity)\nOut of all actual positives, how many were predicted correctly?\nUsed when missing positives is costly.\nFormula: TP / (TP + FN)\n9.1.4 F1-Score\nBalance between precision and recall.\nFormula: 2 * (Precision * Recall) / (",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 0
    }
  },
  {
    "chunk_id": "faed0d3e-3e24-44fb-ac66-c25dd2c734f1",
    "text": "Precision + Recall)\n9.1.5 Confusion Matrix\nA table showing True Positives, False Positives, False Negatives, and True\nNegatives.\n9.1.6 ROC Curve (Receiver Operating Characteristic)\nShows the trade-off between True Positive Rate and False Positive Rate.\n9.1.7 AUC (Area Under the Curve)\nMeasures the entire area under the ROC curve.\nHigher AUC = better performance.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 1
    }
  },
  {
    "chunk_id": "934574c7-e702-4513-8bb1-8c20f5d55b9e",
    "text": "9. MODEL EVALUATION AND METRICS\n9.2 Regression Metrics\nUsed when the model predicts continuous values (like house price,\ntemperature).\n9.2.1 Mean Absolute Error (MAE)\nAverage of the absolute errors.\nEasy to understand.\n9.2.2 Mean Squared Error (MSE)\nAverage of squared errors.\nPenalizes large errors more than MAE.\n9.2.3 R-Squared (R²)\nExplains how much variance in the output is explained by the model.\nRanges from 0 to 1 (higher is better).\n9.2.4 Adjusted R-Squared\nLike R², but adjusts for the number of predictors (features).\nUseful when comparing models with different numbers of features.\n9.3 Cross-Validation\nUsed to test model performance on different splits of the data.\n9.3.1 K-Fold Cross-Validation\nSplit data into k equal parts. Train on k-1 and test on the remaining part.\nRepeat k times",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8b92a081-1930-41b2-b814-0b1b780a7b42",
    "text": ".\n9.3.2 Leave-One-Out Cross-Validation (LOOCV)\nA special case of K-Fold where k = number of data points. Very slow but\nthorough.\n9.3.3 Stratified K-Fold\nSame as K-Fold, but keeps the ratio of classes the same in each fold.\nUseful for imbalanced datasets.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 1
    }
  },
  {
    "chunk_id": "bfb54f51-1ff7-4aa7-a97b-6783415ee9f1",
    "text": "9. MODEL EVALUATION AND METRICS\n9.4 Hyperparameter Tuning\nHyperparameters are settings that control how a model learns (like\nlearning rate, depth of a tree, etc.).\n9.4.1 Grid Search\nTests all combinations of given hyperparameter values.\n9.4.2 Random Search\nRandomly selects combinations. Faster than Grid Search.\n9.4.3 Bayesian Optimization\nUses past results to pick the next best combination. Smart and efficient.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 37,
      "chunk": 0
    }
  },
  {
    "chunk_id": "acd77220-0bc9-4c9e-a6ab-b1bb28519a45",
    "text": "10. ADVANCED TOPICS\nThese are modern machine learning methods used in advanced real-world\napplications such as chatbots, recommendation systems, self-driving cars,\nand privacy-focused AI.\n10.1 Transfer Learning\nInstead of training a model from scratch, we use a model that has already\nbeen trained on a large dataset and apply it to a new, similar task.\nPre-trained Models\nThese are models trained on huge datasets.\nExamples:\nVGG, ResNet – for images\nBERT – for text\nFine-Tuning\nSlightly updating the pre-trained model using your own smaller dataset.\nFeature Extraction\nUsing the pre-trained model to extract useful features and then using\nthose features for your own model or task.\nBenefit: Saves training time and works well even with limited data.\n10.2 Attention Mechanism\nThis helps the model dec",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7c21f29f-957c-4b54-b8f2-0d3cb8f8ed08",
    "text": "ide which parts of the input data are most\nimportant.\nSelf-Attention\nEvery part of the input focuses on every other part to understand context\nbetter.\nUsed in NLP (Natural Language Processing) and transformers.\nMulti-Head Attention\nApplies attention multiple times in parallel to capture different\nrelationships within the data.\nApplications\nIn NLP: translation, summarization, question-answering\nIn vision: image recognition with Vision Transformers",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 1
    }
  },
  {
    "chunk_id": "81e645db-dac1-4983-8032-3a8494375918",
    "text": "10.2 Attention Mechanism\n10.3 Reinforcement Learning in Deep Learning\nCombining deep learning with reinforcement learning for decision-\nmaking tasks.\nActor-Critic\nTwo models work together:\nActor: selects the best action\nCritic: evaluates how good the action was\nA3C (Asynchronous Advantage Actor-Critic)\nUses multiple agents to learn in parallel, which speeds up learning and\nincreases stability.\nPPO (Proximal Policy Optimization)\nAn improved and stable way to train reinforcement learning agents. Used\nin games, robotics, etc.\n10. ADVANCED TOPICS\nMachine\nLearning (ML)\nArtificial\nIntelligence (AI)\nDeep\nLearning (DL)\nReinforcement\nLearning (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 39,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7c1c9d39-72a0-4ab6-afd7-49f82d729c00",
    "text": "10.4 Federated Learning\nModel training happens across many devices without collecting data in a\ncentral server. Each device keeps its data private and only sends model\nupdates.\nDistributed Learning Frameworks\nUsed when data is spread across users, hospitals, or devices. Examples\ninclude Google’s keyboard predictions.\nPrivacy-Preserving ML\nSince data never leaves the device, user privacy is protected. This is useful\nin healthcare, banking, and personal mobile applications.\n10. ADVANCED TOPICS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 40,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d177dcfd-c0c1-40f4-8acc-8172868766af",
    "text": "These tools help you build, train, and deploy AI/ML models more efficiently.\nThey provide ready-to-use functions so you don’t need to code everything\nfrom scratch.\n11.1 Python Libraries\nPython is the most popular language in AI/ML. These libraries make your\nwork faster and easier:\nNumPy\nUsed for numerical computing.\nSupports arrays, matrices, and linear algebra operations.\nPandas\nUsed for data manipulation and analysis.\nYou can load data, clean it, and analyze it in tabular formats (DataFrames).\nScikit-learn\nA powerful machine learning library.\nIncludes ready-to-use models like Linear Regression, SVM, Random Forest,\nKNN, and more.\nTensorFlow & Keras\nUsed for building deep learning models.\nTensorFlow: low-level control\nKeras: high-level interface, easier to use\nPyTorch\nAn alternative to Ten",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 0
    }
  },
  {
    "chunk_id": "456d868a-ef76-444c-ac88-8f8025db94f8",
    "text": "sorFlow, widely used in research and development.\nIt’s flexible, fast, and dynamic (supports on-the-fly computation graphs).\nOpenCV\nUsed for computer vision tasks like image processing, object detection,\nface recognition, etc.\nNLTK & SpaCy\nNatural Language Processing (NLP) libraries.\nNLTK: good for learning, includes many basic NLP tasks\nSpaCy: industrial-strength NLP, faster and more efficient\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d5ac0700-9026-4cb4-bfe2-217f04110d1c",
    "text": "Google Colab\nFree online Jupyter Notebook\nSupports GPU/TPU\nGreat for students and beginners\nAWS SageMaker\nAmazon’s cloud ML platform\nSupports training, tuning, and deploying models at scale\nUsed in enterprise-level applications\nAzure ML\nMicrosoft’s machine learning platform\nIntegrates well with other Microsoft tools (e.g., Excel, Power BI)\nProvides autoML, drag-and-drop pipelines, and more\n11.2 Cloud Platforms\nCloud platforms help you run your models on powerful servers without\nneeding your own hardware.\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 42,
      "chunk": 0
    }
  },
  {
    "chunk_id": "69856169-0eb7-488b-b6af-6fb3533efd64",
    "text": "12.1 Model Serialization\nWhat it means:\nAfter training your machine learning model, you save (serialize) it to use\nlater without retraining.\nPopular tools:\nPickle – A Python library to serialize and deserialize Python objects.\nJoblib – Similar to Pickle but better for large NumPy arrays.\nExample:\n12.2 Flask/Django for Model Deployment\nThese are web frameworks that let you expose your model as an API\nendpoint, so other apps or users can access it via the internet.\nFlask: Lightweight and easier for quick ML model APIs.\nDjango: Heavier but better for large web applications with built-in admin,\nsecurity, and ORM.\nFlask Example:\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 43,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a89e4f1b-0caa-4523-b22a-40dd5e55f667",
    "text": "12.3 Serving Models with TensorFlow Serving, FastAPI\nTensorFlow Serving:\nUsed to deploy TensorFlow models in production. It supports versioning\nand high-performance serving with REST/gRPC.\nFastAPI:\nA modern, fast (high-performance) framework for building APIs with\nautomatic docs, great for production-grade ML APIs.\nFastAPI Example:\n12.4 Monitoring and Maintaining Models in Production\nOnce your model is live, you need to ensure it continues to perform well.\nWhat to monitor:\nModel accuracy degradation (due to data drift)\nResponse time\nError rates\nSystem metrics (CPU, memory)\nTools:\nPrometheus + Grafana for system and application monitoring\nMLflow or Evidently.ai for tracking model performance over time\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 44,
      "chunk": 0
    }
  },
  {
    "chunk_id": "00bff9ed-9475-48e0-9ab7-c211909397eb",
    "text": "13.2 Common Beginner Mistakes\nGeneral ML Mistakes\nUsing test data during training\nNot normalizing/scaling features\nIgnoring class imbalance in classification tasks\nForgetting to check for data leakage\nNot splitting the dataset correctly (Train/Validation/Test)\nNeural Network Mistakes\nUsing too many/too few layers without tuning\nChoosing wrong activation/loss functions\nIgnoring overfitting (no dropout or regularization)\nNLP Mistakes\nFeeding raw text without preprocessing\nUsing TF-IDF on small datasets without context\nConfusing stemming with lemmatization\nDeployment Mistakes\nNot checking model performance after deployment\nIgnoring real-time latency\nNo monitoring/logging in place\n13.1 Practice Tasks\nTo strengthen understanding, here are simple practice tasks for each core\nconcept:\n13. PRACTIC",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a1cf1dd6-fcbf-4de9-a40a-9e1a49f69ee4",
    "text": "E & COMMON BEGINNER MISTAKES\nNLP\nPCA\nTopic\nGANs\nCNNs\nRNNs\nDecision Trees\nLinear Regression\nLogistic Regression\nK-Means Clustering\nMini Practice Task\nClassify hand-written digits using the MNIST dataset.\nBuild a sentiment analysis model on product reviews.\nPredict the next word in a sentence using a small corpus.\nGenerate new handwritten digits after training on MNIST.\nReduce dimensions in the Iris dataset and visualize clusters.\nClassify if a person will buy a product based on age, income, etc.\nGroup customers by shopping behavior (customer segmentation).\nPredict house prices using a dataset with features like area, rooms,\nand location.\nBuild a binary classifier to detect spam emails.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d853e50d-102b-4b0d-9c23-ce2c263b1d86",
    "text": "AI/ML\nCheatSheet",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "055d1a71-b709-4169-b098-57b5186dee10",
    "text": "1. Introduction to AI and ML\nWhat is Artificial Intelligence (AI)?\nTypes of AI: Narrow AI vs. General AI\nWhat is Machine Learning (ML)?\nSupervised vs. Unsupervised vs. Reinforcement Learning\nKey Terminologies (Model, Feature, Target, Algorithm)\nApplications of AI and ML\nDifference Between AI, ML, and Deep Learning (DL)\n2. Mathematics for ML/AI\nLinear Algebra\nVectors, Matrices, and Tensors\nMatrix Operations\nEigenvalues and Eigenvectors\nProbability and Statistics\nMean, Variance, Standard Deviation\nBayes Theorem\nConditional Probability\nProbability Distributions (Normal, Binomial, Poisson)\nCalculus for Optimization\nDerivatives and Gradients\nGradient Descent\n3. Data Preprocessing\nData Cleaning (Missing Values, Outliers)\nData Normalization and Standardization\nEncoding Categorical Variables (One-",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "32aaf6d8-2f12-41fc-b111-6a2831a0890a",
    "text": "Hot Encoding, Label Encoding)\nFeature Scaling (Min-Max, Z-score)\nFeature Engineering (Polynomial Features, Binning)\nHandling Imbalanced Data (SMOTE, Undersampling, Oversampling)\n4. Supervised Learning Algorithms\nLinear Regression\nSimple vs. Multiple Linear Regression\nGradient Descent and Normal Equation\nRegularization (L1, L2)\nLogistic Regression\nBinary vs. Multiclass Classification\nSigmoid Function and Cost Function\nRegularization\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d309bad8-6921-4537-834b-611cd67f0010",
    "text": "4. Supervised Learning Algorithms\nK-Nearest Neighbors (KNN)\nDistance Metrics (Euclidean, Manhattan)\nChoosing K\nAdvantages and Disadvantages\nSupport Vector Machines (SVM)\nHyperplanes and Margins\nLinear and Non-Linear SVM\nKernel Trick\nDecision Trees\nGini Impurity and Entropy\nOverfitting and Pruning\nRandom Forest\nBootstrapping\nBagging\nFeature Importance\nGradient Boosting Machines (GBM)\nXGBoost, LightGBM, CatBoost\nHyperparameter Tuning\nEarly Stopping\nNaive Bayes\nGaussian, Multinomial, Bernoulli Naive Bayes\nAssumptions and Applications\n5. Unsupervised Learning Algorithms\nK-Means Clustering\nAlgorithm Overview\nElbow Method\nK-Means++ Initialization\nHierarchical Clustering\nAgglomerative vs. Divisive Clustering\nDendrogram and Optimal Cut\nPrincipal Component Analysis (PCA)\nDimensionality Reduction\nEi",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "69a85a73-e8e0-4b2e-bdec-87972de28d69",
    "text": "genvalue Decomposition\nScree Plot and Explained Variance\nDBSCAN\nDensity-Based Clustering\nEpsilon and MinPts Parameters\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9fc8e3ad-a866-422d-88b2-cdcdc124f28b",
    "text": "6. Reinforcement Learning\nIntroduction to Reinforcement Learning\nKey Concepts (Agent, Environment, State, Action, Reward)\nMarkov Decision Process (MDP)\nQ-Learning and Deep Q-Networks (DQN)\nPolicy Gradients and Actor-Critic Methods\nExploration vs. Exploitation\n7. Neural Networks & Deep Learning\nIntroduction to Neural Networks\nPerceptrons\nActivation Functions (Sigmoid, ReLU, Tanh)\nForward Propagation and Backpropagation\nLoss Functions (MSE, Cross-Entropy)\nDeep Neural Networks (DNN)\nArchitecture and Layers\nTraining Process and Optimizers\nOverfitting and Regularization (Dropout, L2 Regularization)\nConvolutional Neural Networks (CNN)\nConvolutional Layers, Pooling Layers\nFilter/Kernels and Strides\nApplications (Image Classification, Object Detection)\nRecurrent Neural Networks (RNN)\nBasic RNN vs.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7c4a92bb-d149-415f-8f8b-f773bfd127ac",
    "text": " LSTM vs. GRU\nTime-Series Prediction and NLP Applications\nVanishing and Exploding Gradients Problem\nGenerative Adversarial Networks (GANs)\nGenerator and Discriminator\nTraining Process\nApplications (Image Generation, Data Augmentation)\n8. Natural Language Processing (NLP)\nText Preprocessing\nTokenization, Stopwords, Lemmatization, Stemming\nBag of Words, TF-IDF\nWord Embeddings\nWord2Vec, GloVe, FastText\nSentence Embeddings (BERT, RoBERTa, GPT)\nSequence Models\nRecurrent Neural Networks (RNNs)\nLSTM and GRU\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e788942d-e155-4c7c-8dfe-1e7734b36f9c",
    "text": "8. Natural Language Processing (NLP)\nTransformer Architecture\nSelf-Attention Mechanism\nEncoder-Decoder Model\nBERT, GPT, T5 Models\nText Classification\nSentiment Analysis, Named Entity Recognition (NER)\nLanguage Generation\nText Summarization\nMachine Translation\n9. Model Evaluation and Metrics\nClassification Metrics\nAccuracy, Precision, Recall, F1-Score\nConfusion Matrix\nROC Curve, AUC\nRegression Metrics\nMean Absolute Error (MAE), Mean Squared Error (MSE)\nR-Squared and Adjusted R-Squared\nCross-Validation\nK-Fold Cross-Validation\nLeave-One-Out Cross-Validation\nStratified K-Fold\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\n10. Advanced Topics\nTransfer Learning\nPre-trained Models (VGG, ResNet, BERT)\nFine-Tuning and Feature Extraction\nAttention Mechanism\nSelf-Attention, Mul",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "30d1bfaf-6c25-4984-8e71-3873cc2851c0",
    "text": "ti-Head Attention\nApplications in NLP and Vision\nReinforcement Learning in Deep Learning\nActor-Critic, A3C, Proximal Policy Optimization (PPO)\nFederated Learning\nDistributed Learning Frameworks\nPrivacy-Preserving ML\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4caf034d-28b3-43e6-8f57-496069f8dae1",
    "text": "11. Tools and Libraries for AI/ML\nPython Libraries\nNumPy, Pandas\nScikit-learn\nTensorFlow, Keras, PyTorch\nOpenCV for Computer Vision\nNLTK, SpaCy for NLP\nCloud Platforms\nGoogle Colab\nAWS Sagemaker\nAzure ML\n12. Deployment and Production\nModel Serialization (Pickle, Joblib)\nFlask/Django for Model Deployment\nServing Models with TensorFlow Serving, FastAPI\nMonitoring and Maintaining Models in Production\n13. Practice & Common Beginner Mistakes\nPractice Tasks\nCommon Beginner Mistakes\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "073e7b04-6af0-4c44-ae68-0944504dc019",
    "text": "1. INTRODUCTION TO AI AND ML\n1.1 What is Artificial Intelligence (AI)?\nArtificial Intelligence (AI) is the ability of machines or computer programs\nto perform tasks that typically require human intelligence. These tasks can\ninclude understanding language, recognizing patterns, solving problems,\nand making decisions.\nSimple explanation:\nAI is when machines are made smart enough to think and act like\nhumans.\nExamples:\nVoice assistants like Alexa\nImage recognition systems\nChatbots\nSelf-driving cars\n1.2 Types of AI: Narrow AI vs. General AI\nNarrow AI (Weak AI):\nDesigned to perform one specific task\nCannot do anything beyond its programming\nExamples: Email spam filters, facial recognition, recommendation systems\nGeneral AI (Strong AI):\nStill under research and development\nCan learn and perform ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e0c4696b-2b89-4612-914c-602114361a3d",
    "text": "any intellectual task a human can do\nWould have reasoning, memory, and decision-making abilities similar to a\nhuman\n1.3 What is Machine Learning (ML)?\nMachine Learning is a subfield of AI that allows machines to learn from\ndata and improve their performance over time without being explicitly\nprogrammed.\nSimple explanation:\nInstead of writing rules for everything, we give the machine data, and it\nfigures out the rules on its own.\nExample:\nA machine learns to identify spam emails by studying thousands of\nexamples.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "085ae8a4-95a6-46e3-afef-613c4cba6254",
    "text": "1. INTRODUCTION TO AI AND ML\n1.4 Supervised vs. Unsupervised vs. Reinforcement Learning\nSupervised Learning:\nThe training data includes both input and the correct output (labels)\nThe model learns by comparing its output with the correct output\nExample: Predicting house prices based on features like size, location, etc.\nUnsupervised Learning:\nThe data has no labels\nThe model tries to find patterns or groupings in the data\nExample: Grouping customers based on purchasing behavior\nReinforcement Learning:\nThe model learns through trial and error\nIt receives rewards or penalties based on its actions\nExample: A robot learning to walk or a program learning to play chess\n1.5 Key Terminologies\nModel:\nA program or function that makes predictions or decisions based on data.\nFeature:\nAn input variable ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "dfa762a0-8522-441c-99a9-60d7481d91d5",
    "text": "used in making predictions (e.g., age, income,\ntemperature).\nTarget:\nThe value the model is trying to predict (e.g., house price, spam or not).\nAlgorithm:\nA step-by-step method or set of rules used to train the model.\nTraining:\nThe process of teaching the model using a dataset.\nTesting:\nEvaluating the trained model on new, unseen data to measure its\naccuracy.\n1.6 Applications of AI and ML\nRecommendation systems (YouTube, Amazon, Netflix)\nFraud detection in banking\nLanguage translation\nHealthcare diagnosis\nSelf-driving vehicles\nStock market prediction\nChatbots and customer support\nSocial media content moderation",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 1
    }
  },
  {
    "chunk_id": "60899083-ee28-4613-b2ff-fadc2de09d45",
    "text": "Term\nDeep Learning (DL)\nMachine Learning (ML)\nArtificial Intelligence (AI)\nDescription\nA subset of AI where machines learn from data\nThe overall field focused on creating intelligent machines\nA specialized type of ML that uses neural networks inspired by the\nhuman brain\n1. INTRODUCTION TO AI AND ML\nDeep Learning\nMachine Learning\nArtificial Intelligence\n1.7 Difference Between AI, ML, and Deep Learning (DL)\nVisual analogy:\nAI is the broader concept, ML is a part of AI, and DL is a part of ML.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cd02b227-3bc2-4c18-8cdd-2607bffbf81d",
    "text": "Mathematics is the foundation of AI and Machine Learning. It helps us\nunderstand how algorithms work under the hood and how to fine-tune\nmodels for better performance.\n2.1 Linear Algebra\nLinear Algebra deals with numbers organized in arrays and how these\narrays interact. It is used in almost every ML algorithm.\n2.1.1 Vectors, Matrices, and Tensors\nVector: A 1D array of numbers. Example: [3, 5, 7]\nUsed to represent features like height, weight, age.\nMatrix: A 2D array (rows and columns).\nExample:\nUsed to store datasets or model weights.\nTensor: A generalization of vectors and matrices to more dimensions (3D\nor higher).\nExample: Used in deep learning models like images (3D tensor: width,\nheight, color channels).\n2.1.2 Matrix Operations\nAddition/Subtraction: Add or subtract corresponding elem",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cf519bf9-c89e-4520-93a3-c6f6144cce8b",
    "text": "ents of two\nmatrices.\nMultiplication: Used to combine weights and inputs in ML models.\nTranspose: Flip a matrix over its diagonal.\nDot Product: Fundamental in calculating output in neural networks.\n2.1.3 Eigenvalues and Eigenvectors\nEigenvector: A direction that doesn't change during a transformation.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk.\nThese are used in algorithms like Principal Component Analysis (PCA) for\ndimensionality reduction.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "30254a3a-489e-4333-80d2-a20184011b14",
    "text": "2.2 Probability and Statistics\nProbability helps machines make decisions under uncertainty, and\nstatistics helps us understand data and model performance.\n2.2.1 Mean, Variance, Standard Deviation\nMean: The average value.\nVariance: How spread out the values are from the mean.\nStandard Deviation: The square root of variance. It measures how much\nthe values vary.\nUsed to understand the distribution and behavior of features in datasets.\n2.2.2 Bayes Theorem\nA mathematical formula to calculate conditional probability:\nUsed in Naive Bayes classifiers for spam detection, document classification,\netc.\n2.2.3 Conditional Probability\nThe probability of one event occurring given that another event has\nalready occurred.\nExample:\nProbability that a user clicks an ad given that they are between 20-30\nyear",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e022bfe2-5638-4194-b9db-cbe9baecca96",
    "text": "s old.\n2.2.4 Probability Distributions\nNormal Distribution: Bell-shaped curve. Common in real-world data like\nheight, exam scores.\nBinomial Distribution: Used for yes/no type outcomes. Example: Flipping a\ncoin 10 times.\nPoisson Distribution: For events happening over a time period. Example:\nNumber of customer calls per hour.\nThese distributions help in modeling randomness in data.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 1
    }
  },
  {
    "chunk_id": "21ff8e67-a0eb-44e1-acca-d452b6292b78",
    "text": "2.3 Calculus for Optimization\nCalculus helps in training models by optimizing them to reduce errors.\n2.3.1 Derivatives and Gradients\nDerivative: Measures how a function changes as its input changes.\nGradient: A vector of derivatives that tells the slope of a function in multi-\ndimensions.\nUsed to find the direction in which the model should adjust its weights.\n2.3.2 Gradient Descent\nAn optimization algorithm used to minimize the loss (error) function.\nHow it works:\nStart with random values\nCalculate the gradient (slope)\nMove slightly in the opposite direction of the gradient\nRepeat until the loss is minimized\nGradient Descent is the core of many training algorithms in ML and DL.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "026de26d-caf9-419d-80fd-59997ef96fcf",
    "text": "Used when data is normally distributed\nUsed when the data is not normally distributed\nStandardization (Z-score Scaling):\nCenters the data around mean = 0 and standard deviation = 1\nFormula:\nBefore feeding data into any machine learning model, it must be cleaned,\ntransformed, and prepared. This step is called data preprocessing, and it is\none of the most important stages in building accurate ML models.\n3.1 Data Cleaning\nReal-world data is often messy. Data cleaning means identifying and fixing\nerrors in the dataset.\n3.1.1 Missing Values\nMissing values can be due to incomplete forms, sensor errors, etc.\nTechniques to handle missing data:\nRemove rows/columns with too many missing values\nFill (impute) missing values using:\nMean/Median/Mode\nForward/Backward fill\nPredictive models (like KNN)\n3.1",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "bbf1036c-2423-415a-9db3-e4e773d1a78c",
    "text": ".2 Outliers\nOutliers are data points that are very different from others.\nThey can distort results and reduce model performance.\nDetection methods:\nBox plot, Z-score, IQR method\nHandling outliers:\nRemove them\nTransform data (e.g., log scaling)\nCap them (set a maximum/minimum)\n3.2 Data Normalization and Standardization\nHelps scale numeric data so that features contribute equally to the model.\nNormalization (Min-Max Scaling):\nScales all values between 0 and 1\nFormula:\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 1
    }
  },
  {
    "chunk_id": "a92deb1b-532a-4729-8841-0f1157c09029",
    "text": "One-Hot Encoding:\nCreates new binary columns for each category.\nExample:\n3.3 Encoding Categorical Variables\nML models work with numbers, not text. Categorical data needs to be\nconverted into numerical form.\nLabel Encoding:\nAssigns each unique category a number.\nExample:\nLabel encoding is good for ordinal data (ranked), while one-hot encoding\nis best for nominal data (non-ranked).\n3.4 Feature Scaling\nEnsures features are on the same scale so the model can learn effectively.\nMin-Max Scaling:\nScales features between 0 and 1.\nGood for algorithms like KNN, neural networks.\nZ-score Scaling (Standardization):\nUseful for models that assume normality, like linear regression or logistic\nregression.\nScaling is crucial for models that use distance or gradient-based optimization.\n3.5 Feature Engineerin",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "65dc10b1-00cf-41c5-8a6f-c5c680701e8e",
    "text": "g\nCreating new features or modifying existing ones to improve model\nperformance.\nPolynomial Features:\nCreate new features by raising existing features to a power.\nExample: From x, create x², x³\nBinning (Discretization):\nConvert continuous data into categories.\nExample: Age → [0–18], [19–35], [36–60], 60+\nFeature engineering can significantly boost the predictive power of a model.\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "98dfb520-085a-4d6a-86a0-5e775dd4433d",
    "text": "3. DATA PREPROCESSING\n3.6 Handling Imbalanced Data\nIn classification, if one class dominates (e.g., 95% non-fraud, 5% fraud),\nmodels may ignore the minority class. This is called class imbalance.\nSMOTE (Synthetic Minority Oversampling Technique):\nCreates synthetic examples of the minority class using nearest neighbors.\nUndersampling:\nRemove some samples from the majority class.\nOversampling:\nDuplicate or generate more samples of the minority class.\nBalancing data improves the ability of the model to correctly predict both\nclasses.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c3856272-31e8-477e-9c65-8956b27fc649",
    "text": "Works for small datasets.\n4.1.1 Simple vs. Multiple Linear Regression\nSimple Linear Regression: One input (X) to predict one output (Y).\nExample: Predicting salary from years of experience.\nMultiple Linear Regression: Multiple inputs (X1, X2, ..., Xn).\nExample: Predicting price based on area, location, and age.\n4.1.2 Gradient Descent and Normal Equation\nGradient Descent: Iterative method to minimize error (cost function).\nNormal Equation: Direct way to find weights using linear algebra:\nSupervised learning uses labeled data, meaning the model learns from input-\noutput pairs (X → y). The algorithm tries to map inputs (features) to correct\noutputs (targets/labels).\n4.1 Linear Regression\nUsed for predicting continuous values (e.g., predicting house price,\ntemperature).\n4. SUPERVISED LEARNING ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "837dce2f-cf56-4a2c-bb0f-6b5a8534542c",
    "text": "ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e821573d-fcb6-40bf-b1b6-ac1b71ab0efb",
    "text": "4.1.3 Regularization (L1, L2)\nPrevents overfitting by adding a penalty:\nL1 (Lasso): Can reduce coefficients to 0 (feature selection).\nL2 (Ridge): Shrinks coefficients but doesn’t make them 0.\n4.2 Logistic Regression\nUsed for classification problems (e.g., spam vs. not spam).\n4.2.1 Binary vs. Multiclass Classification\nBinary: 2 outcomes (e.g., 0 or 1)\nMulticlass: More than 2 classes (handled using One-vs-Rest or Softmax)\n4.2.2 Sigmoid and Cost Function\nSigmoid Function: Converts outputs to values between 0 and 1.\nCost Function: Log loss used to measure prediction error.\n4.2.3 Regularization\nL1 and L2 regularization help prevent overfitting in logistic regression as\nwell.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "49fec905-6c15-433f-91af-f22bfd728d1a",
    "text": "4.3.1 Distance Metrics\nEuclidean Distance: Straight line between two points.\nManhattan Distance: Sum of absolute differences.\n4.3.2 Choosing K\nK is the number of neighbors to consider.\nToo low K → sensitive to noise\nToo high K → model becomes less flexible\n4.3.3 Advantages & Disadvantages\nSimple and easy to implement\nSlow for large datasets, sensitive to irrelevant features\n4.4 Support Vector Machines (SVM)\nPowerful classification model for small to medium-sized datasets.\n4.3 K-Nearest Neighbors (KNN)\nA simple classification (or regression) algorithm that uses proximity.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "df10be0c-4665-4794-abd7-874794b62406",
    "text": "4.5.1 Gini Impurity and Entropy\nMeasures how pure a node is:\nGini Impurity: Probability of misclassification.\nEntropy: Measure of randomness/information.\n4.5.2 Overfitting and Pruning\nOverfitting: Tree memorizes training data.\nPruning: Removes unnecessary branches to reduce overfitting.\n4.4.1 Hyperplanes and Margins\nSVM finds the best hyperplane that separates data with maximum\nmargin.\n4.4.2 Linear vs. Non-Linear SVM\nLinear SVM: Works when data is linearly separable.\nNon-linear SVM: Uses kernel trick for complex datasets.\n4.4.3 Kernel Trick\nTransforms data into higher dimensions to make it separable.\nCommon kernels: RBF (Gaussian), Polynomial, Sigmoid\n4.5 Decision Trees\nTree-like structure used for classification and regression.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "714184b1-5986-4055-be7a-bf38fff19754",
    "text": "4.7.1 XGBoost, LightGBM, CatBoost\nAdvanced boosting libraries:\nXGBoost: Popular, fast, and accurate\nLightGBM: Faster, uses leaf-wise growth\nCatBoost: Handles categorical features automatically\n4.6.1 Bootstrapping\nRandomly selects subsets of data to train each tree.\n4.6.2 Bagging\nCombines predictions of multiple trees (majority vote or average).\n4.6.3 Feature Importance\nMeasures which features contribute most to model prediction.\n4.7 Gradient Boosting Machines (GBM)\nBoosting is an ensemble method where models are trained sequentially.\n4.6 Random Forest\nAn ensemble of decision trees to improve accuracy and reduce overfitting.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 20,
      "chunk": 0
    }
  },
  {
    "chunk_id": "2f0bbfde-d662-4769-b462-2243d4e028ae",
    "text": "4.8.1 Gaussian, Multinomial, Bernoulli\nGaussian NB: For continuous features (assumes normal distribution)\nMultinomial NB: For text data, counts of words\nBernoulli NB: For binary features (0/1)\n4.8.2 Assumptions and Applications\nAssumes all features are independent (rarely true but still works well)\nCommonly used in spam detection, sentiment analysis, document\nclassif ication\n4.7.2 Hyperparameter Tuning\nAdjust parameters like:\nLearning rate\nNumber of estimators (trees)\nMax depth\nTools: GridSearchCV, RandomSearchCV\n4.7.3 Early Stopping\nStops training if the model stops improving on validation set.\n4.8 Naive Bayes\nProbabilistic classifier based on Bayes' Theorem and strong independence\nassumption.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "19ec4c85-7c59-4890-b44f-ef8d958328df",
    "text": "Unsupervised learning finds hidden patterns in unlabeled data. Unlike\nsupervised learning, it doesn't rely on labeled outputs (no predefined target).\n5.1 K-Means Clustering\n5.1.1 Algorithm Overview\nK-Means is a clustering algorithm that divides data into K clusters based\non similarity.\nIt works by:\na.Selecting K random centroids.\nb.Assigning each point to the nearest centroid.\nc.Updating the centroid to the mean of its assigned points.\nd.Repeating steps 2–3 until the centroids stop changing.\n5.1.2 Elbow Method\nUsed to choose the optimal number of clusters (K).\nPlot the number of clusters (K) vs. Within-Cluster-Sum-of-Squares (WCSS).\nThe point where the WCSS curve bends (elbow) is the best K.\n5.1.3 K-Means++ Initialization\nImproves basic K-Means by smartly selecting initial centroids, reduc",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "83f0c5b5-4605-4a31-a20c-98fbdf57722b",
    "text": "ing the\nchance of poor clustering.\nStarts with one random centroid, then selects the next ones based on\ndistance from the current ones (probabilistically).\n5.2 Hierarchical Clustering\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9070f690-bcdb-428c-9a4d-784f7bb0bc5c",
    "text": "5.2.1 Agglomerative vs. Divisive Clustering\nAgglomerative (bottom-up): Start with each point as its own cluster and\nmerge the closest clusters.\nDivisive (top-down): Start with one large cluster and recursively split it.\nAgglomerative is more commonly used.\n5.2.2 Dendrogram and Optimal Cut\nA dendrogram is a tree-like diagram that shows how clusters are formed\nat each step.\nThe height of branches represents the distance between clusters.\nCutting the dendrogram at a certain height gives the desired number of\nclusters.\n5.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique used to simplify datasets\nwhile retaining most of the important information.\n5.3.1 Dimensionality Reduction\nPCA transforms the data into a new coordinate system with fewer\ndimensions (called princ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 0
    }
  },
  {
    "chunk_id": "56d3f9fb-edf2-4a5b-b912-7449b3d43ddc",
    "text": "ipal components).\nUseful for visualization, speeding up algorithms, and avoiding the curse of\ndimensionality.\n5.3.2 Eigenvalue Decomposition\nPCA is based on eigenvectors and eigenvalues of the covariance matrix of\nthe data.\nThe eigenvectors define the new axes (principal components).\nThe eigenvalues indicate the amount of variance each component\ncaptures.\n5.3.3 Scree Plot and Explained Variance\nScree Plot: A plot of eigenvalues to help decide how many components to\nkeep.\nThe explained variance ratio shows how much of the data’s variance is\ncaptured by each component.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 1
    }
  },
  {
    "chunk_id": "0291eb69-c071-4c79-aa5b-7dd64741e569",
    "text": "5.4.1 Density-Based Clustering\nUnlike K-Means, DBSCAN doesn't require specifying the number of\nclusters.\nClusters are formed based on dense regions in the data.\n5.4.2 Epsilon and MinPts Parameters\nEpsilon (ε): Radius around a point to search for neighbors.\nMinPts: Minimum number of points required to form a dense region.\nPoints are classified as:\nCore Point: Has MinPts within ε.\nBorder Point: Not a core but within ε of a core.\nNoise: Neither core nor border.\n5.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups closely\npacked points and marks outliers as noise.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 24,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9fb9119a-804e-443b-a199-1b5dbba5fc77",
    "text": "6.2 Key Concepts\nAgent: The learner or decision maker (e.g., a robot, self-driving car).\nEnvironment: Everything the agent interacts with (e.g., a maze, a game).\nState: A snapshot of the current situation (e.g., position in a maze).\nAction: A move or decision made by the agent (e.g., turn left).\nReward: Feedback from the environment (e.g., +1 for reaching goal, -1 for\nhitting a wall).\nThe goal of the agent is to maximize cumulative rewards over time.\n6.3 Markov Decision Process (MDP)\nRL problems are often modeled as Markov Decision Processes (MDPs).\nAn MDP includes: S: Set of states A: Set of actions P: Transition\nprobabilities (P(s' | s, a)) R: Reward function γ (gamma): Discount factor\n(how much future rewards are valued) The \"Markov\" property means the\nnext state only depends on the cur",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ec43b2c6-65d1-4d35-b10b-cbc7321d5691",
    "text": "rent state and action, not on previous\nones.\n6.1 Introduction to Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent\nlearns to make decisions by interacting with an environment. It is inspired\nby how humans learn from experience — by trial and error.\nThe agent performs an action\nThe environment responds with a reward\nThe agent uses this feedback to learn better actions over time\nUnlike supervised learning, RL doesn’t rely on labeled data. Instead, it uses\nrewards or penalties to guide learning.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 1
    }
  },
  {
    "chunk_id": "787efe07-0d66-44ec-9e78-d3e618ef776e",
    "text": "Where:\ns: current state\na: action\nr: reward\ns': next state\nα: learning rate\nγ: discount factor\nDeep Q-Networks (DQN):\nWhen the state/action space is too large, a neural network is used to\napproximate Q-values.\nCombines Q-Learning with Deep Learning.\nUsed in Atari games and robotics.\n6.4 Q-Learning and Deep Q-Networks (DQN)\nQ-Learning:\nA model-free algorithm that learns the value (Q-value) of taking an action\nin a state.\nUses the formula:\n6.5 Policy Gradients and Actor-Critic Methods\nPolicy Gradients:\nInstead of learning value functions, learn the policy directly (probability\ndistribution over actions).\nUse gradient ascent to improve the policy based on the reward received.\nActor-Critic Methods:\nCombine the best of both worlds:\nActor: chooses the action\nCritic: evaluates how good the action",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 0
    }
  },
  {
    "chunk_id": "fef7a09f-21bf-4a81-86f0-3047ee0f6478",
    "text": " was (value function)\nMore stable and efficient than pure policy gradients.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e45ab879-be4a-4ab7-a5b5-3b3145bc840a",
    "text": "6.6 Exploration vs. Exploitation\nExploration: Trying new actions to discover their effects (important early in\ntraining).\nExploitation: Choosing the best-known action for maximum reward.\nRL must balance both:\nToo much exploration = slow learning\nToo much exploitation = stuck in local optimum\nCommon strategy: ε-greedy\nChoose a random action with probability ε\nOtherwise, choose the best-known action\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 27,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5f989d3a-8de7-47dd-96d0-810c0f8b2a67",
    "text": "7.1 Introduction to Neural Networks\nA neural network is a computer model inspired by the human brain. It\nconsists of neurons (nodes) organized in layers, capable of learning\npatterns from data.\n7.1.1 Perceptrons\nThe perceptron is the simplest type of neural network, with:\nInputs → Weights → Summation → Activation Function → Output\nIt's like a yes/no decision maker (binary classification).\n7.1.2 Activation Functions\nThese introduce non-linearity, allowing the network to learn complex\nfunctions:\nSigmoid: Outputs between 0 and 1. Good for probability-based outputs.\nReLU (Rectified Linear Unit): Most popular. Fast, reduces vanishing\ngradient.\nReLU(x) = max(0, x)\nTanh: Like sigmoid but outputs between -1 and 1.\n7.1.3 Forward Propagation and Backpropagation\nForward Propagation: Input data flows ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 0
    }
  },
  {
    "chunk_id": "0832f28d-84a4-40ae-b4e1-30bc3ce00131",
    "text": "through the network to produce an\noutput.\nBackpropagation: Calculates the error and updates weights using\ngradients (from loss function).\nThis is how neural networks learn from data.\n7.1.4 Loss Functions\nThey measure how far off the prediction is from the actual result.\nMSE (Mean Squared Error): Used in regression problems.\nCross-Entropy Loss: Used in classification tasks.\n7.2 Deep Neural Networks (DNN)\nA Deep Neural Network has multiple hidden layers between input and\noutput.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7adb4a0a-e90e-431d-b6a1-5646784d1b70",
    "text": "7.3.1 Convolutional Layers, Pooling Layers\nConvolutional Layers: Apply filters to detect features (edges, corners).\nPooling Layers: Reduce size of feature maps (e.g., Max Pooling).\n7.3.2 Filters/Kernels and Strides\nFilters: Small matrix to slide over input to extract features.\nStrides: Step size of the filter as it moves.\n7.3.3 Applications\nImage Classification\nFace Recognition\nObject Detection\n7.2.1 Architecture and Layers\nInput Layer: Where the data comes in\nHidden Layers: Where computation happens (many neurons per layer)\nOutput Layer: Final predictions\n7.2.2 Training Process and Optimizers\nDuring training, the network:\nMakes predictions\nCalculates the loss\nUpdates weights via optimizers like:\nSGD (Stochastic Gradient Descent)\nAdam (adaptive learning rate)\nRMSProp\n7.2.3 Overfitting and ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 0
    }
  },
  {
    "chunk_id": "333572c8-536b-47f8-9f3e-767e259e2fb2",
    "text": "Regularization\nOverfitting happens when the model learns noise instead of patterns.\nRegularization techniques help:\nDropout: Randomly turns off neurons during training.\nL2 Regularization: Penalizes large weights (weight decay).\n7.3 Convolutional Neural Networks (CNN)\nCNNs are specialized for image data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 1
    }
  },
  {
    "chunk_id": "f886f2ea-5be7-4e2b-8623-4964281ba1eb",
    "text": "7.4 Recurrent Neural Networks (RNN)\nRNNs are designed for sequential data (time series, text, etc.).\n7.4.1 Basic RNN vs. LSTM vs. GRU\nBasic RNN: Loops through time steps but suffers from memory issues.\nLSTM (Long Short-Term Memory): Handles long dependencies well.\nGRU (Gated Recurrent Unit): Similar to LSTM but faster.\n7.4.2 Time-Series Prediction and NLP Applications\nPredict stock prices, weather, or language sequences.\nUsed in chatbots, translation, and speech recognition.\n7.4.3 Vanishing and Exploding Gradients\nProblem during training of RNNs where gradients shrink (vanish) or\nexplode.\nLSTM and GRU solve this with gate mechanisms.\n7.5 Generative Adversarial Networks (GANs)\nGANs are powerful models for generating new data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 30,
      "chunk": 0
    }
  },
  {
    "chunk_id": "899f4125-77ab-4bf2-99e2-67045f5e14ff",
    "text": "7.5.1 Generator and Discriminator\nGenerator: Creates fake data\nDiscriminator: Tries to distinguish real from fake data\nThey compete with each other (like a forger and a detective).\n7.5.2 Training Process\nGenerator tries to fool the discriminator\nDiscriminator improves to detect fakes\nThey both improve over time — leading to realistic generated data\n7.5.3 Applications\nImage Generation (e.g., fake faces)\nArt and Style Transfer\nData Augmentation for training other ML models\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 31,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ea69e27c-c11f-44e1-9045-cda4497aec54",
    "text": "NLP helps computers understand, interpret, and generate human language.\nIt's widely used in applications like chatbots, translation tools, and voice\nassistants.\n8.1 Text Preprocessing\nBefore using text in machine learning models, we need to clean and\nconvert it into a format the computer understands.\n8.1.1 Tokenization\nBreaking text into smaller parts like words or sentences.\nExample: \"I love AI\" → [\"I\", \"love\", \"AI\"]\n8.1.2 Stopwords\nRemoving common words that do not add much meaning (like “is”, “the”,\n“and”).\n8.1.3 Stemming\nCutting words down to their root form.\nExample: “playing”, “played” → “play”\n8.1.4 Lemmatization\nSimilar to stemming but uses grammar to find the proper base word.\nExample: “better” → “good”\n8.1.5 Bag of Words (BoW)\nConverts text into numbers based on word counts in a ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b0095082-8932-4267-a1b0-910f4e905d13",
    "text": "document.\n8.1.6 TF-IDF\nGives importance to words that appear often in one document but not in\nothers. Helps identify keywords.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d6fc1245-36fc-4de8-8918-3fdc0497ce1d",
    "text": "8.2 Word Embeddings\nWord embeddings turn words into vectors (numbers) so that a machine\ncan understand their meaning and context.\n8.2.1 Word2Vec\nA model that learns how words are related based on their surrounding\nwords.\n8.2.2 GloVe\nLearns word meanings by looking at how often words appear together.\n8.2.3 FastText\nSimilar to Word2Vec but also looks at parts of words, which helps with\nunknown words.\n8.2.4 Sentence Embeddings (BERT, RoBERTa, GPT)\nThese models convert full sentences into vectors. They understand context\nmuch better than older models.\n8.3 Sequence Models\nThese models are good for processing data where order matters, like text.\n8.3.1 RNN (Recurrent Neural Networks)\nGood for learning from sequences, such as sentences.\n8.3.2 LSTM (Long Short-Term Memory)\nAn advanced RNN that reme",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ef05a192-0275-427d-a6eb-dee44e06a0b4",
    "text": "mbers long-term information.\n8.3.3 GRU (Gated Recurrent Unit)\nA simpler version of LSTM that works faster and often just as well.\n8.4 Transformer Architecture\nTransformers are a powerful model used in almost all modern NLP\nsystems.\n8.4.1 Self-Attention Mechanism\nThis allows the model to focus on important words in a sentence, no\nmatter where they appear.\n8.4.2 Encoder-Decoder Model\nUsed in tasks like translation where the model reads input (encoder) and\ngenerates output (decoder).\n8.4.3 Examples:\nBERT: Great for understanding text.\nGPT: Great for generating text.\nT5: Can both understand and generate text for many tasks.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 1
    }
  },
  {
    "chunk_id": "a1ad9e10-981a-4b97-af44-c0e9ecaa40a8",
    "text": "8. NATURAL LANGUAGE PROCESSING (NLP)\n8.5 Text Classification\nClassify text into categories.\nExamples:\nSentiment Analysis: Is a review positive or negative?\nNamed Entity Recognition (NER): Find names, places, dates, etc. in text.\n8.6 Language Generation\nGenerate new text from existing input.\n8.6.1 Text Summarization\nShortens a long document while keeping important points.\n8.6.2 Machine Translation\nTranslates text from one language to another (like English to Hindi).",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 34,
      "chunk": 0
    }
  },
  {
    "chunk_id": "83d7acc5-4d58-490a-9a33-e40238e45205",
    "text": "9. MODEL EVALUATION AND METRICS\nModel evaluation helps us check how well our machine learning models are\nperforming. We use different metrics depending on whether it's a\nclassification or regression problem.\n9.1 Classification Metrics\nUsed when your model predicts categories or classes (e.g., spam or not\nspam).\n9.1.1 Accuracy\nHow often the model is correct.\nFormula: (Correct Predictions) / (Total Predictions)\n9.1.2 Precision\nOut of all predicted positives, how many were actually positive?\nUsed when false positives are costly.\nFormula: TP / (TP + FP)\n9.1.3 Recall (Sensitivity)\nOut of all actual positives, how many were predicted correctly?\nUsed when missing positives is costly.\nFormula: TP / (TP + FN)\n9.1.4 F1-Score\nBalance between precision and recall.\nFormula: 2 * (Precision * Recall) / (",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 0
    }
  },
  {
    "chunk_id": "23c3eb64-a698-4aad-8d92-0120bbc0a9f8",
    "text": "Precision + Recall)\n9.1.5 Confusion Matrix\nA table showing True Positives, False Positives, False Negatives, and True\nNegatives.\n9.1.6 ROC Curve (Receiver Operating Characteristic)\nShows the trade-off between True Positive Rate and False Positive Rate.\n9.1.7 AUC (Area Under the Curve)\nMeasures the entire area under the ROC curve.\nHigher AUC = better performance.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 1
    }
  },
  {
    "chunk_id": "5adfc126-9f53-4ca8-95de-bd70cf0cad84",
    "text": "9. MODEL EVALUATION AND METRICS\n9.2 Regression Metrics\nUsed when the model predicts continuous values (like house price,\ntemperature).\n9.2.1 Mean Absolute Error (MAE)\nAverage of the absolute errors.\nEasy to understand.\n9.2.2 Mean Squared Error (MSE)\nAverage of squared errors.\nPenalizes large errors more than MAE.\n9.2.3 R-Squared (R²)\nExplains how much variance in the output is explained by the model.\nRanges from 0 to 1 (higher is better).\n9.2.4 Adjusted R-Squared\nLike R², but adjusts for the number of predictors (features).\nUseful when comparing models with different numbers of features.\n9.3 Cross-Validation\nUsed to test model performance on different splits of the data.\n9.3.1 K-Fold Cross-Validation\nSplit data into k equal parts. Train on k-1 and test on the remaining part.\nRepeat k times",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 0
    }
  },
  {
    "chunk_id": "98ea48d4-b45e-46c6-90a9-37de625be253",
    "text": ".\n9.3.2 Leave-One-Out Cross-Validation (LOOCV)\nA special case of K-Fold where k = number of data points. Very slow but\nthorough.\n9.3.3 Stratified K-Fold\nSame as K-Fold, but keeps the ratio of classes the same in each fold.\nUseful for imbalanced datasets.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e23a2c66-9982-41fb-ae18-a8b7ef0b3c62",
    "text": "9. MODEL EVALUATION AND METRICS\n9.4 Hyperparameter Tuning\nHyperparameters are settings that control how a model learns (like\nlearning rate, depth of a tree, etc.).\n9.4.1 Grid Search\nTests all combinations of given hyperparameter values.\n9.4.2 Random Search\nRandomly selects combinations. Faster than Grid Search.\n9.4.3 Bayesian Optimization\nUses past results to pick the next best combination. Smart and efficient.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 37,
      "chunk": 0
    }
  },
  {
    "chunk_id": "2d1dc474-640e-4936-a034-92165035ee91",
    "text": "10. ADVANCED TOPICS\nThese are modern machine learning methods used in advanced real-world\napplications such as chatbots, recommendation systems, self-driving cars,\nand privacy-focused AI.\n10.1 Transfer Learning\nInstead of training a model from scratch, we use a model that has already\nbeen trained on a large dataset and apply it to a new, similar task.\nPre-trained Models\nThese are models trained on huge datasets.\nExamples:\nVGG, ResNet – for images\nBERT – for text\nFine-Tuning\nSlightly updating the pre-trained model using your own smaller dataset.\nFeature Extraction\nUsing the pre-trained model to extract useful features and then using\nthose features for your own model or task.\nBenefit: Saves training time and works well even with limited data.\n10.2 Attention Mechanism\nThis helps the model dec",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 0
    }
  },
  {
    "chunk_id": "58878530-9cf0-4f7a-a75b-b3dabc30bcf5",
    "text": "ide which parts of the input data are most\nimportant.\nSelf-Attention\nEvery part of the input focuses on every other part to understand context\nbetter.\nUsed in NLP (Natural Language Processing) and transformers.\nMulti-Head Attention\nApplies attention multiple times in parallel to capture different\nrelationships within the data.\nApplications\nIn NLP: translation, summarization, question-answering\nIn vision: image recognition with Vision Transformers",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7fd8701b-7536-4d1f-a2af-b2618cc033a7",
    "text": "10.2 Attention Mechanism\n10.3 Reinforcement Learning in Deep Learning\nCombining deep learning with reinforcement learning for decision-\nmaking tasks.\nActor-Critic\nTwo models work together:\nActor: selects the best action\nCritic: evaluates how good the action was\nA3C (Asynchronous Advantage Actor-Critic)\nUses multiple agents to learn in parallel, which speeds up learning and\nincreases stability.\nPPO (Proximal Policy Optimization)\nAn improved and stable way to train reinforcement learning agents. Used\nin games, robotics, etc.\n10. ADVANCED TOPICS\nMachine\nLearning (ML)\nArtificial\nIntelligence (AI)\nDeep\nLearning (DL)\nReinforcement\nLearning (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 39,
      "chunk": 0
    }
  },
  {
    "chunk_id": "3e9ee491-88b8-4db9-b77f-94267c4de8d2",
    "text": "10.4 Federated Learning\nModel training happens across many devices without collecting data in a\ncentral server. Each device keeps its data private and only sends model\nupdates.\nDistributed Learning Frameworks\nUsed when data is spread across users, hospitals, or devices. Examples\ninclude Google’s keyboard predictions.\nPrivacy-Preserving ML\nSince data never leaves the device, user privacy is protected. This is useful\nin healthcare, banking, and personal mobile applications.\n10. ADVANCED TOPICS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 40,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1cdc3ee7-ea02-4025-89c9-81773b33a1f1",
    "text": "These tools help you build, train, and deploy AI/ML models more efficiently.\nThey provide ready-to-use functions so you don’t need to code everything\nfrom scratch.\n11.1 Python Libraries\nPython is the most popular language in AI/ML. These libraries make your\nwork faster and easier:\nNumPy\nUsed for numerical computing.\nSupports arrays, matrices, and linear algebra operations.\nPandas\nUsed for data manipulation and analysis.\nYou can load data, clean it, and analyze it in tabular formats (DataFrames).\nScikit-learn\nA powerful machine learning library.\nIncludes ready-to-use models like Linear Regression, SVM, Random Forest,\nKNN, and more.\nTensorFlow & Keras\nUsed for building deep learning models.\nTensorFlow: low-level control\nKeras: high-level interface, easier to use\nPyTorch\nAn alternative to Ten",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4449dc55-6bd2-495d-b0b3-b78b14900f1c",
    "text": "sorFlow, widely used in research and development.\nIt’s flexible, fast, and dynamic (supports on-the-fly computation graphs).\nOpenCV\nUsed for computer vision tasks like image processing, object detection,\nface recognition, etc.\nNLTK & SpaCy\nNatural Language Processing (NLP) libraries.\nNLTK: good for learning, includes many basic NLP tasks\nSpaCy: industrial-strength NLP, faster and more efficient\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 1
    }
  },
  {
    "chunk_id": "2ebdafd1-a2c9-409e-bb07-5225674a682d",
    "text": "Google Colab\nFree online Jupyter Notebook\nSupports GPU/TPU\nGreat for students and beginners\nAWS SageMaker\nAmazon’s cloud ML platform\nSupports training, tuning, and deploying models at scale\nUsed in enterprise-level applications\nAzure ML\nMicrosoft’s machine learning platform\nIntegrates well with other Microsoft tools (e.g., Excel, Power BI)\nProvides autoML, drag-and-drop pipelines, and more\n11.2 Cloud Platforms\nCloud platforms help you run your models on powerful servers without\nneeding your own hardware.\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 42,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6cd530b4-c109-4b14-a0f5-05ad079cbaf2",
    "text": "12.1 Model Serialization\nWhat it means:\nAfter training your machine learning model, you save (serialize) it to use\nlater without retraining.\nPopular tools:\nPickle – A Python library to serialize and deserialize Python objects.\nJoblib – Similar to Pickle but better for large NumPy arrays.\nExample:\n12.2 Flask/Django for Model Deployment\nThese are web frameworks that let you expose your model as an API\nendpoint, so other apps or users can access it via the internet.\nFlask: Lightweight and easier for quick ML model APIs.\nDjango: Heavier but better for large web applications with built-in admin,\nsecurity, and ORM.\nFlask Example:\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 43,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ce9fde6b-a8f5-47dc-af29-cd513062800d",
    "text": "12.3 Serving Models with TensorFlow Serving, FastAPI\nTensorFlow Serving:\nUsed to deploy TensorFlow models in production. It supports versioning\nand high-performance serving with REST/gRPC.\nFastAPI:\nA modern, fast (high-performance) framework for building APIs with\nautomatic docs, great for production-grade ML APIs.\nFastAPI Example:\n12.4 Monitoring and Maintaining Models in Production\nOnce your model is live, you need to ensure it continues to perform well.\nWhat to monitor:\nModel accuracy degradation (due to data drift)\nResponse time\nError rates\nSystem metrics (CPU, memory)\nTools:\nPrometheus + Grafana for system and application monitoring\nMLflow or Evidently.ai for tracking model performance over time\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 44,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6919954c-95d4-4dbd-b2d2-72367e89de4e",
    "text": "13.2 Common Beginner Mistakes\nGeneral ML Mistakes\nUsing test data during training\nNot normalizing/scaling features\nIgnoring class imbalance in classification tasks\nForgetting to check for data leakage\nNot splitting the dataset correctly (Train/Validation/Test)\nNeural Network Mistakes\nUsing too many/too few layers without tuning\nChoosing wrong activation/loss functions\nIgnoring overfitting (no dropout or regularization)\nNLP Mistakes\nFeeding raw text without preprocessing\nUsing TF-IDF on small datasets without context\nConfusing stemming with lemmatization\nDeployment Mistakes\nNot checking model performance after deployment\nIgnoring real-time latency\nNo monitoring/logging in place\n13.1 Practice Tasks\nTo strengthen understanding, here are simple practice tasks for each core\nconcept:\n13. PRACTIC",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 0
    }
  },
  {
    "chunk_id": "02826f6d-77a1-41a6-b61b-6a14da7d190b",
    "text": "E & COMMON BEGINNER MISTAKES\nNLP\nPCA\nTopic\nGANs\nCNNs\nRNNs\nDecision Trees\nLinear Regression\nLogistic Regression\nK-Means Clustering\nMini Practice Task\nClassify hand-written digits using the MNIST dataset.\nBuild a sentiment analysis model on product reviews.\nPredict the next word in a sentence using a small corpus.\nGenerate new handwritten digits after training on MNIST.\nReduce dimensions in the Iris dataset and visualize clusters.\nClassify if a person will buy a product based on age, income, etc.\nGroup customers by shopping behavior (customer segmentation).\nPredict house prices using a dataset with features like area, rooms,\nand location.\nBuild a binary classifier to detect spam emails.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 1
    }
  },
  {
    "chunk_id": "b09c0121-a893-4d62-a1b3-7c9e59610dcb",
    "text": "AI/ML\nCheatSheet",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e448d724-4795-4161-8a8e-492ef7698254",
    "text": "1. Introduction to AI and ML\nWhat is Artificial Intelligence (AI)?\nTypes of AI: Narrow AI vs. General AI\nWhat is Machine Learning (ML)?\nSupervised vs. Unsupervised vs. Reinforcement Learning\nKey Terminologies (Model, Feature, Target, Algorithm)\nApplications of AI and ML\nDifference Between AI, ML, and Deep Learning (DL)\n2. Mathematics for ML/AI\nLinear Algebra\nVectors, Matrices, and Tensors\nMatrix Operations\nEigenvalues and Eigenvectors\nProbability and Statistics\nMean, Variance, Standard Deviation\nBayes Theorem\nConditional Probability\nProbability Distributions (Normal, Binomial, Poisson)\nCalculus for Optimization\nDerivatives and Gradients\nGradient Descent\n3. Data Preprocessing\nData Cleaning (Missing Values, Outliers)\nData Normalization and Standardization\nEncoding Categorical Variables (One-",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c07c4f89-ad0e-4507-b279-9f96373d2c18",
    "text": "Hot Encoding, Label Encoding)\nFeature Scaling (Min-Max, Z-score)\nFeature Engineering (Polynomial Features, Binning)\nHandling Imbalanced Data (SMOTE, Undersampling, Oversampling)\n4. Supervised Learning Algorithms\nLinear Regression\nSimple vs. Multiple Linear Regression\nGradient Descent and Normal Equation\nRegularization (L1, L2)\nLogistic Regression\nBinary vs. Multiclass Classification\nSigmoid Function and Cost Function\nRegularization\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4f45653f-79c7-4186-9d4b-04eb2f0d373e",
    "text": "4. Supervised Learning Algorithms\nK-Nearest Neighbors (KNN)\nDistance Metrics (Euclidean, Manhattan)\nChoosing K\nAdvantages and Disadvantages\nSupport Vector Machines (SVM)\nHyperplanes and Margins\nLinear and Non-Linear SVM\nKernel Trick\nDecision Trees\nGini Impurity and Entropy\nOverfitting and Pruning\nRandom Forest\nBootstrapping\nBagging\nFeature Importance\nGradient Boosting Machines (GBM)\nXGBoost, LightGBM, CatBoost\nHyperparameter Tuning\nEarly Stopping\nNaive Bayes\nGaussian, Multinomial, Bernoulli Naive Bayes\nAssumptions and Applications\n5. Unsupervised Learning Algorithms\nK-Means Clustering\nAlgorithm Overview\nElbow Method\nK-Means++ Initialization\nHierarchical Clustering\nAgglomerative vs. Divisive Clustering\nDendrogram and Optimal Cut\nPrincipal Component Analysis (PCA)\nDimensionality Reduction\nEi",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1ed5ef6c-9393-4e06-8bd0-ab3a7af5d332",
    "text": "genvalue Decomposition\nScree Plot and Explained Variance\nDBSCAN\nDensity-Based Clustering\nEpsilon and MinPts Parameters\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 1
    }
  },
  {
    "chunk_id": "374eb33b-3f96-4e45-8695-9d2c53503e46",
    "text": "6. Reinforcement Learning\nIntroduction to Reinforcement Learning\nKey Concepts (Agent, Environment, State, Action, Reward)\nMarkov Decision Process (MDP)\nQ-Learning and Deep Q-Networks (DQN)\nPolicy Gradients and Actor-Critic Methods\nExploration vs. Exploitation\n7. Neural Networks & Deep Learning\nIntroduction to Neural Networks\nPerceptrons\nActivation Functions (Sigmoid, ReLU, Tanh)\nForward Propagation and Backpropagation\nLoss Functions (MSE, Cross-Entropy)\nDeep Neural Networks (DNN)\nArchitecture and Layers\nTraining Process and Optimizers\nOverfitting and Regularization (Dropout, L2 Regularization)\nConvolutional Neural Networks (CNN)\nConvolutional Layers, Pooling Layers\nFilter/Kernels and Strides\nApplications (Image Classification, Object Detection)\nRecurrent Neural Networks (RNN)\nBasic RNN vs.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b840e0c7-2e2c-41d0-a43f-f66f2caed4a0",
    "text": " LSTM vs. GRU\nTime-Series Prediction and NLP Applications\nVanishing and Exploding Gradients Problem\nGenerative Adversarial Networks (GANs)\nGenerator and Discriminator\nTraining Process\nApplications (Image Generation, Data Augmentation)\n8. Natural Language Processing (NLP)\nText Preprocessing\nTokenization, Stopwords, Lemmatization, Stemming\nBag of Words, TF-IDF\nWord Embeddings\nWord2Vec, GloVe, FastText\nSentence Embeddings (BERT, RoBERTa, GPT)\nSequence Models\nRecurrent Neural Networks (RNNs)\nLSTM and GRU\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9cbfc938-2590-42b9-891f-68fbff58db5e",
    "text": "8. Natural Language Processing (NLP)\nTransformer Architecture\nSelf-Attention Mechanism\nEncoder-Decoder Model\nBERT, GPT, T5 Models\nText Classification\nSentiment Analysis, Named Entity Recognition (NER)\nLanguage Generation\nText Summarization\nMachine Translation\n9. Model Evaluation and Metrics\nClassification Metrics\nAccuracy, Precision, Recall, F1-Score\nConfusion Matrix\nROC Curve, AUC\nRegression Metrics\nMean Absolute Error (MAE), Mean Squared Error (MSE)\nR-Squared and Adjusted R-Squared\nCross-Validation\nK-Fold Cross-Validation\nLeave-One-Out Cross-Validation\nStratified K-Fold\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\n10. Advanced Topics\nTransfer Learning\nPre-trained Models (VGG, ResNet, BERT)\nFine-Tuning and Feature Extraction\nAttention Mechanism\nSelf-Attention, Mul",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "caeaab95-ae90-42e9-8a37-2f10be73d5df",
    "text": "ti-Head Attention\nApplications in NLP and Vision\nReinforcement Learning in Deep Learning\nActor-Critic, A3C, Proximal Policy Optimization (PPO)\nFederated Learning\nDistributed Learning Frameworks\nPrivacy-Preserving ML\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "6f8216e5-63d3-4b01-aeb8-bf97d84fa88c",
    "text": "11. Tools and Libraries for AI/ML\nPython Libraries\nNumPy, Pandas\nScikit-learn\nTensorFlow, Keras, PyTorch\nOpenCV for Computer Vision\nNLTK, SpaCy for NLP\nCloud Platforms\nGoogle Colab\nAWS Sagemaker\nAzure ML\n12. Deployment and Production\nModel Serialization (Pickle, Joblib)\nFlask/Django for Model Deployment\nServing Models with TensorFlow Serving, FastAPI\nMonitoring and Maintaining Models in Production\n13. Practice & Common Beginner Mistakes\nPractice Tasks\nCommon Beginner Mistakes\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f0f87fa6-2536-48aa-853a-f273d05582f0",
    "text": "1. INTRODUCTION TO AI AND ML\n1.1 What is Artificial Intelligence (AI)?\nArtificial Intelligence (AI) is the ability of machines or computer programs\nto perform tasks that typically require human intelligence. These tasks can\ninclude understanding language, recognizing patterns, solving problems,\nand making decisions.\nSimple explanation:\nAI is when machines are made smart enough to think and act like\nhumans.\nExamples:\nVoice assistants like Alexa\nImage recognition systems\nChatbots\nSelf-driving cars\n1.2 Types of AI: Narrow AI vs. General AI\nNarrow AI (Weak AI):\nDesigned to perform one specific task\nCannot do anything beyond its programming\nExamples: Email spam filters, facial recognition, recommendation systems\nGeneral AI (Strong AI):\nStill under research and development\nCan learn and perform ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7f88cfb5-779a-4aeb-a9e8-6460b13292eb",
    "text": "any intellectual task a human can do\nWould have reasoning, memory, and decision-making abilities similar to a\nhuman\n1.3 What is Machine Learning (ML)?\nMachine Learning is a subfield of AI that allows machines to learn from\ndata and improve their performance over time without being explicitly\nprogrammed.\nSimple explanation:\nInstead of writing rules for everything, we give the machine data, and it\nfigures out the rules on its own.\nExample:\nA machine learns to identify spam emails by studying thousands of\nexamples.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7073be9e-312b-4fcc-9d7f-4e0b84c7e7d6",
    "text": "1. INTRODUCTION TO AI AND ML\n1.4 Supervised vs. Unsupervised vs. Reinforcement Learning\nSupervised Learning:\nThe training data includes both input and the correct output (labels)\nThe model learns by comparing its output with the correct output\nExample: Predicting house prices based on features like size, location, etc.\nUnsupervised Learning:\nThe data has no labels\nThe model tries to find patterns or groupings in the data\nExample: Grouping customers based on purchasing behavior\nReinforcement Learning:\nThe model learns through trial and error\nIt receives rewards or penalties based on its actions\nExample: A robot learning to walk or a program learning to play chess\n1.5 Key Terminologies\nModel:\nA program or function that makes predictions or decisions based on data.\nFeature:\nAn input variable ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a7445897-5d9c-4f9e-8746-bc6181023bf2",
    "text": "used in making predictions (e.g., age, income,\ntemperature).\nTarget:\nThe value the model is trying to predict (e.g., house price, spam or not).\nAlgorithm:\nA step-by-step method or set of rules used to train the model.\nTraining:\nThe process of teaching the model using a dataset.\nTesting:\nEvaluating the trained model on new, unseen data to measure its\naccuracy.\n1.6 Applications of AI and ML\nRecommendation systems (YouTube, Amazon, Netflix)\nFraud detection in banking\nLanguage translation\nHealthcare diagnosis\nSelf-driving vehicles\nStock market prediction\nChatbots and customer support\nSocial media content moderation",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d65f1e7a-f1a3-470b-a1ee-440797464f64",
    "text": "Term\nDeep Learning (DL)\nMachine Learning (ML)\nArtificial Intelligence (AI)\nDescription\nA subset of AI where machines learn from data\nThe overall field focused on creating intelligent machines\nA specialized type of ML that uses neural networks inspired by the\nhuman brain\n1. INTRODUCTION TO AI AND ML\nDeep Learning\nMachine Learning\nArtificial Intelligence\n1.7 Difference Between AI, ML, and Deep Learning (DL)\nVisual analogy:\nAI is the broader concept, ML is a part of AI, and DL is a part of ML.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c7b4e52c-a4ff-44e5-9951-ccfdadd515e0",
    "text": "Mathematics is the foundation of AI and Machine Learning. It helps us\nunderstand how algorithms work under the hood and how to fine-tune\nmodels for better performance.\n2.1 Linear Algebra\nLinear Algebra deals with numbers organized in arrays and how these\narrays interact. It is used in almost every ML algorithm.\n2.1.1 Vectors, Matrices, and Tensors\nVector: A 1D array of numbers. Example: [3, 5, 7]\nUsed to represent features like height, weight, age.\nMatrix: A 2D array (rows and columns).\nExample:\nUsed to store datasets or model weights.\nTensor: A generalization of vectors and matrices to more dimensions (3D\nor higher).\nExample: Used in deep learning models like images (3D tensor: width,\nheight, color channels).\n2.1.2 Matrix Operations\nAddition/Subtraction: Add or subtract corresponding elem",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "26260fe2-b5c9-421f-8f4e-6f1ad743cc4a",
    "text": "ents of two\nmatrices.\nMultiplication: Used to combine weights and inputs in ML models.\nTranspose: Flip a matrix over its diagonal.\nDot Product: Fundamental in calculating output in neural networks.\n2.1.3 Eigenvalues and Eigenvectors\nEigenvector: A direction that doesn't change during a transformation.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk.\nThese are used in algorithms like Principal Component Analysis (PCA) for\ndimensionality reduction.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "5d3799ff-736d-4a1c-9b82-028917e8964e",
    "text": "2.2 Probability and Statistics\nProbability helps machines make decisions under uncertainty, and\nstatistics helps us understand data and model performance.\n2.2.1 Mean, Variance, Standard Deviation\nMean: The average value.\nVariance: How spread out the values are from the mean.\nStandard Deviation: The square root of variance. It measures how much\nthe values vary.\nUsed to understand the distribution and behavior of features in datasets.\n2.2.2 Bayes Theorem\nA mathematical formula to calculate conditional probability:\nUsed in Naive Bayes classifiers for spam detection, document classification,\netc.\n2.2.3 Conditional Probability\nThe probability of one event occurring given that another event has\nalready occurred.\nExample:\nProbability that a user clicks an ad given that they are between 20-30\nyear",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "25b564c0-576a-42d6-8baa-49621501a34f",
    "text": "s old.\n2.2.4 Probability Distributions\nNormal Distribution: Bell-shaped curve. Common in real-world data like\nheight, exam scores.\nBinomial Distribution: Used for yes/no type outcomes. Example: Flipping a\ncoin 10 times.\nPoisson Distribution: For events happening over a time period. Example:\nNumber of customer calls per hour.\nThese distributions help in modeling randomness in data.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 1
    }
  },
  {
    "chunk_id": "5d7faa42-0e13-4c5d-b12e-5774753f33d1",
    "text": "2.3 Calculus for Optimization\nCalculus helps in training models by optimizing them to reduce errors.\n2.3.1 Derivatives and Gradients\nDerivative: Measures how a function changes as its input changes.\nGradient: A vector of derivatives that tells the slope of a function in multi-\ndimensions.\nUsed to find the direction in which the model should adjust its weights.\n2.3.2 Gradient Descent\nAn optimization algorithm used to minimize the loss (error) function.\nHow it works:\nStart with random values\nCalculate the gradient (slope)\nMove slightly in the opposite direction of the gradient\nRepeat until the loss is minimized\nGradient Descent is the core of many training algorithms in ML and DL.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5c919270-546c-4d9a-8b3c-2114eba4c41d",
    "text": "Used when data is normally distributed\nUsed when the data is not normally distributed\nStandardization (Z-score Scaling):\nCenters the data around mean = 0 and standard deviation = 1\nFormula:\nBefore feeding data into any machine learning model, it must be cleaned,\ntransformed, and prepared. This step is called data preprocessing, and it is\none of the most important stages in building accurate ML models.\n3.1 Data Cleaning\nReal-world data is often messy. Data cleaning means identifying and fixing\nerrors in the dataset.\n3.1.1 Missing Values\nMissing values can be due to incomplete forms, sensor errors, etc.\nTechniques to handle missing data:\nRemove rows/columns with too many missing values\nFill (impute) missing values using:\nMean/Median/Mode\nForward/Backward fill\nPredictive models (like KNN)\n3.1",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c3cc2da9-6ad0-4c8d-9a05-3ebfe123c81a",
    "text": ".2 Outliers\nOutliers are data points that are very different from others.\nThey can distort results and reduce model performance.\nDetection methods:\nBox plot, Z-score, IQR method\nHandling outliers:\nRemove them\nTransform data (e.g., log scaling)\nCap them (set a maximum/minimum)\n3.2 Data Normalization and Standardization\nHelps scale numeric data so that features contribute equally to the model.\nNormalization (Min-Max Scaling):\nScales all values between 0 and 1\nFormula:\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 1
    }
  },
  {
    "chunk_id": "53d4f350-a153-4126-bf97-884704fecfa2",
    "text": "One-Hot Encoding:\nCreates new binary columns for each category.\nExample:\n3.3 Encoding Categorical Variables\nML models work with numbers, not text. Categorical data needs to be\nconverted into numerical form.\nLabel Encoding:\nAssigns each unique category a number.\nExample:\nLabel encoding is good for ordinal data (ranked), while one-hot encoding\nis best for nominal data (non-ranked).\n3.4 Feature Scaling\nEnsures features are on the same scale so the model can learn effectively.\nMin-Max Scaling:\nScales features between 0 and 1.\nGood for algorithms like KNN, neural networks.\nZ-score Scaling (Standardization):\nUseful for models that assume normality, like linear regression or logistic\nregression.\nScaling is crucial for models that use distance or gradient-based optimization.\n3.5 Feature Engineerin",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "56432728-0b4e-48d7-b957-7a607ac8bb6b",
    "text": "g\nCreating new features or modifying existing ones to improve model\nperformance.\nPolynomial Features:\nCreate new features by raising existing features to a power.\nExample: From x, create x², x³\nBinning (Discretization):\nConvert continuous data into categories.\nExample: Age → [0–18], [19–35], [36–60], 60+\nFeature engineering can significantly boost the predictive power of a model.\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "23e4173d-e283-4134-82f7-aa1684ccab9f",
    "text": "3. DATA PREPROCESSING\n3.6 Handling Imbalanced Data\nIn classification, if one class dominates (e.g., 95% non-fraud, 5% fraud),\nmodels may ignore the minority class. This is called class imbalance.\nSMOTE (Synthetic Minority Oversampling Technique):\nCreates synthetic examples of the minority class using nearest neighbors.\nUndersampling:\nRemove some samples from the majority class.\nOversampling:\nDuplicate or generate more samples of the minority class.\nBalancing data improves the ability of the model to correctly predict both\nclasses.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7f469f8e-0f75-4f83-b761-a02d39721bb3",
    "text": "Works for small datasets.\n4.1.1 Simple vs. Multiple Linear Regression\nSimple Linear Regression: One input (X) to predict one output (Y).\nExample: Predicting salary from years of experience.\nMultiple Linear Regression: Multiple inputs (X1, X2, ..., Xn).\nExample: Predicting price based on area, location, and age.\n4.1.2 Gradient Descent and Normal Equation\nGradient Descent: Iterative method to minimize error (cost function).\nNormal Equation: Direct way to find weights using linear algebra:\nSupervised learning uses labeled data, meaning the model learns from input-\noutput pairs (X → y). The algorithm tries to map inputs (features) to correct\noutputs (targets/labels).\n4.1 Linear Regression\nUsed for predicting continuous values (e.g., predicting house price,\ntemperature).\n4. SUPERVISED LEARNING ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "dc711a73-3749-4deb-89d4-23ca75557e03",
    "text": "ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 1
    }
  },
  {
    "chunk_id": "2a663c7b-eaa5-428b-aa7a-19ea034e402e",
    "text": "4.1.3 Regularization (L1, L2)\nPrevents overfitting by adding a penalty:\nL1 (Lasso): Can reduce coefficients to 0 (feature selection).\nL2 (Ridge): Shrinks coefficients but doesn’t make them 0.\n4.2 Logistic Regression\nUsed for classification problems (e.g., spam vs. not spam).\n4.2.1 Binary vs. Multiclass Classification\nBinary: 2 outcomes (e.g., 0 or 1)\nMulticlass: More than 2 classes (handled using One-vs-Rest or Softmax)\n4.2.2 Sigmoid and Cost Function\nSigmoid Function: Converts outputs to values between 0 and 1.\nCost Function: Log loss used to measure prediction error.\n4.2.3 Regularization\nL1 and L2 regularization help prevent overfitting in logistic regression as\nwell.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4b1b60ee-6254-497b-9dfa-c620bdcee3d9",
    "text": "4.3.1 Distance Metrics\nEuclidean Distance: Straight line between two points.\nManhattan Distance: Sum of absolute differences.\n4.3.2 Choosing K\nK is the number of neighbors to consider.\nToo low K → sensitive to noise\nToo high K → model becomes less flexible\n4.3.3 Advantages & Disadvantages\nSimple and easy to implement\nSlow for large datasets, sensitive to irrelevant features\n4.4 Support Vector Machines (SVM)\nPowerful classification model for small to medium-sized datasets.\n4.3 K-Nearest Neighbors (KNN)\nA simple classification (or regression) algorithm that uses proximity.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "57e03f3e-a21e-4a63-9392-08bf21b7bee6",
    "text": "4.5.1 Gini Impurity and Entropy\nMeasures how pure a node is:\nGini Impurity: Probability of misclassification.\nEntropy: Measure of randomness/information.\n4.5.2 Overfitting and Pruning\nOverfitting: Tree memorizes training data.\nPruning: Removes unnecessary branches to reduce overfitting.\n4.4.1 Hyperplanes and Margins\nSVM finds the best hyperplane that separates data with maximum\nmargin.\n4.4.2 Linear vs. Non-Linear SVM\nLinear SVM: Works when data is linearly separable.\nNon-linear SVM: Uses kernel trick for complex datasets.\n4.4.3 Kernel Trick\nTransforms data into higher dimensions to make it separable.\nCommon kernels: RBF (Gaussian), Polynomial, Sigmoid\n4.5 Decision Trees\nTree-like structure used for classification and regression.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f824c094-cc1b-4e86-9ead-e6c2b214ed96",
    "text": "4.7.1 XGBoost, LightGBM, CatBoost\nAdvanced boosting libraries:\nXGBoost: Popular, fast, and accurate\nLightGBM: Faster, uses leaf-wise growth\nCatBoost: Handles categorical features automatically\n4.6.1 Bootstrapping\nRandomly selects subsets of data to train each tree.\n4.6.2 Bagging\nCombines predictions of multiple trees (majority vote or average).\n4.6.3 Feature Importance\nMeasures which features contribute most to model prediction.\n4.7 Gradient Boosting Machines (GBM)\nBoosting is an ensemble method where models are trained sequentially.\n4.6 Random Forest\nAn ensemble of decision trees to improve accuracy and reduce overfitting.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 20,
      "chunk": 0
    }
  },
  {
    "chunk_id": "18b9f54d-05a4-4a08-816e-5692b087c95a",
    "text": "4.8.1 Gaussian, Multinomial, Bernoulli\nGaussian NB: For continuous features (assumes normal distribution)\nMultinomial NB: For text data, counts of words\nBernoulli NB: For binary features (0/1)\n4.8.2 Assumptions and Applications\nAssumes all features are independent (rarely true but still works well)\nCommonly used in spam detection, sentiment analysis, document\nclassif ication\n4.7.2 Hyperparameter Tuning\nAdjust parameters like:\nLearning rate\nNumber of estimators (trees)\nMax depth\nTools: GridSearchCV, RandomSearchCV\n4.7.3 Early Stopping\nStops training if the model stops improving on validation set.\n4.8 Naive Bayes\nProbabilistic classifier based on Bayes' Theorem and strong independence\nassumption.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c017c826-84b7-4737-8627-cc4a7fa130c2",
    "text": "Unsupervised learning finds hidden patterns in unlabeled data. Unlike\nsupervised learning, it doesn't rely on labeled outputs (no predefined target).\n5.1 K-Means Clustering\n5.1.1 Algorithm Overview\nK-Means is a clustering algorithm that divides data into K clusters based\non similarity.\nIt works by:\na.Selecting K random centroids.\nb.Assigning each point to the nearest centroid.\nc.Updating the centroid to the mean of its assigned points.\nd.Repeating steps 2–3 until the centroids stop changing.\n5.1.2 Elbow Method\nUsed to choose the optimal number of clusters (K).\nPlot the number of clusters (K) vs. Within-Cluster-Sum-of-Squares (WCSS).\nThe point where the WCSS curve bends (elbow) is the best K.\n5.1.3 K-Means++ Initialization\nImproves basic K-Means by smartly selecting initial centroids, reduc",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "773b549f-43f4-4673-b4bd-6629407918d1",
    "text": "ing the\nchance of poor clustering.\nStarts with one random centroid, then selects the next ones based on\ndistance from the current ones (probabilistically).\n5.2 Hierarchical Clustering\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 1
    }
  },
  {
    "chunk_id": "f8cb2192-b15e-47b6-86cf-d656b0ba3ef0",
    "text": "5.2.1 Agglomerative vs. Divisive Clustering\nAgglomerative (bottom-up): Start with each point as its own cluster and\nmerge the closest clusters.\nDivisive (top-down): Start with one large cluster and recursively split it.\nAgglomerative is more commonly used.\n5.2.2 Dendrogram and Optimal Cut\nA dendrogram is a tree-like diagram that shows how clusters are formed\nat each step.\nThe height of branches represents the distance between clusters.\nCutting the dendrogram at a certain height gives the desired number of\nclusters.\n5.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique used to simplify datasets\nwhile retaining most of the important information.\n5.3.1 Dimensionality Reduction\nPCA transforms the data into a new coordinate system with fewer\ndimensions (called princ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cb5e0934-2427-49cd-ae33-2d37a6cc77b0",
    "text": "ipal components).\nUseful for visualization, speeding up algorithms, and avoiding the curse of\ndimensionality.\n5.3.2 Eigenvalue Decomposition\nPCA is based on eigenvectors and eigenvalues of the covariance matrix of\nthe data.\nThe eigenvectors define the new axes (principal components).\nThe eigenvalues indicate the amount of variance each component\ncaptures.\n5.3.3 Scree Plot and Explained Variance\nScree Plot: A plot of eigenvalues to help decide how many components to\nkeep.\nThe explained variance ratio shows how much of the data’s variance is\ncaptured by each component.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 1
    }
  },
  {
    "chunk_id": "ae33052d-a415-4336-8bbc-f793bb6fd8cd",
    "text": "5.4.1 Density-Based Clustering\nUnlike K-Means, DBSCAN doesn't require specifying the number of\nclusters.\nClusters are formed based on dense regions in the data.\n5.4.2 Epsilon and MinPts Parameters\nEpsilon (ε): Radius around a point to search for neighbors.\nMinPts: Minimum number of points required to form a dense region.\nPoints are classified as:\nCore Point: Has MinPts within ε.\nBorder Point: Not a core but within ε of a core.\nNoise: Neither core nor border.\n5.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups closely\npacked points and marks outliers as noise.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 24,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8169847d-d84e-47a8-94e7-7a94d87874c7",
    "text": "6.2 Key Concepts\nAgent: The learner or decision maker (e.g., a robot, self-driving car).\nEnvironment: Everything the agent interacts with (e.g., a maze, a game).\nState: A snapshot of the current situation (e.g., position in a maze).\nAction: A move or decision made by the agent (e.g., turn left).\nReward: Feedback from the environment (e.g., +1 for reaching goal, -1 for\nhitting a wall).\nThe goal of the agent is to maximize cumulative rewards over time.\n6.3 Markov Decision Process (MDP)\nRL problems are often modeled as Markov Decision Processes (MDPs).\nAn MDP includes: S: Set of states A: Set of actions P: Transition\nprobabilities (P(s' | s, a)) R: Reward function γ (gamma): Discount factor\n(how much future rewards are valued) The \"Markov\" property means the\nnext state only depends on the cur",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 0
    }
  },
  {
    "chunk_id": "bba457d9-b709-49c4-add3-f7a98ef1958a",
    "text": "rent state and action, not on previous\nones.\n6.1 Introduction to Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent\nlearns to make decisions by interacting with an environment. It is inspired\nby how humans learn from experience — by trial and error.\nThe agent performs an action\nThe environment responds with a reward\nThe agent uses this feedback to learn better actions over time\nUnlike supervised learning, RL doesn’t rely on labeled data. Instead, it uses\nrewards or penalties to guide learning.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 1
    }
  },
  {
    "chunk_id": "096f5775-9dc7-4fd7-b897-5c7a0b85c8f4",
    "text": "Where:\ns: current state\na: action\nr: reward\ns': next state\nα: learning rate\nγ: discount factor\nDeep Q-Networks (DQN):\nWhen the state/action space is too large, a neural network is used to\napproximate Q-values.\nCombines Q-Learning with Deep Learning.\nUsed in Atari games and robotics.\n6.4 Q-Learning and Deep Q-Networks (DQN)\nQ-Learning:\nA model-free algorithm that learns the value (Q-value) of taking an action\nin a state.\nUses the formula:\n6.5 Policy Gradients and Actor-Critic Methods\nPolicy Gradients:\nInstead of learning value functions, learn the policy directly (probability\ndistribution over actions).\nUse gradient ascent to improve the policy based on the reward received.\nActor-Critic Methods:\nCombine the best of both worlds:\nActor: chooses the action\nCritic: evaluates how good the action",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4a46b482-5232-489e-a209-59ab4f5834b2",
    "text": " was (value function)\nMore stable and efficient than pure policy gradients.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 1
    }
  },
  {
    "chunk_id": "722bce05-8323-47a3-bfb4-25e68bdb7e1e",
    "text": "6.6 Exploration vs. Exploitation\nExploration: Trying new actions to discover their effects (important early in\ntraining).\nExploitation: Choosing the best-known action for maximum reward.\nRL must balance both:\nToo much exploration = slow learning\nToo much exploitation = stuck in local optimum\nCommon strategy: ε-greedy\nChoose a random action with probability ε\nOtherwise, choose the best-known action\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 27,
      "chunk": 0
    }
  },
  {
    "chunk_id": "bcbbe6f1-5816-42a6-b18d-e15d5e8fee50",
    "text": "7.1 Introduction to Neural Networks\nA neural network is a computer model inspired by the human brain. It\nconsists of neurons (nodes) organized in layers, capable of learning\npatterns from data.\n7.1.1 Perceptrons\nThe perceptron is the simplest type of neural network, with:\nInputs → Weights → Summation → Activation Function → Output\nIt's like a yes/no decision maker (binary classification).\n7.1.2 Activation Functions\nThese introduce non-linearity, allowing the network to learn complex\nfunctions:\nSigmoid: Outputs between 0 and 1. Good for probability-based outputs.\nReLU (Rectified Linear Unit): Most popular. Fast, reduces vanishing\ngradient.\nReLU(x) = max(0, x)\nTanh: Like sigmoid but outputs between -1 and 1.\n7.1.3 Forward Propagation and Backpropagation\nForward Propagation: Input data flows ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b052cb09-173c-4f54-b870-d72d9229e666",
    "text": "through the network to produce an\noutput.\nBackpropagation: Calculates the error and updates weights using\ngradients (from loss function).\nThis is how neural networks learn from data.\n7.1.4 Loss Functions\nThey measure how far off the prediction is from the actual result.\nMSE (Mean Squared Error): Used in regression problems.\nCross-Entropy Loss: Used in classification tasks.\n7.2 Deep Neural Networks (DNN)\nA Deep Neural Network has multiple hidden layers between input and\noutput.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e5792bb5-f2e6-4094-ba68-0f2b7f1290da",
    "text": "7.3.1 Convolutional Layers, Pooling Layers\nConvolutional Layers: Apply filters to detect features (edges, corners).\nPooling Layers: Reduce size of feature maps (e.g., Max Pooling).\n7.3.2 Filters/Kernels and Strides\nFilters: Small matrix to slide over input to extract features.\nStrides: Step size of the filter as it moves.\n7.3.3 Applications\nImage Classification\nFace Recognition\nObject Detection\n7.2.1 Architecture and Layers\nInput Layer: Where the data comes in\nHidden Layers: Where computation happens (many neurons per layer)\nOutput Layer: Final predictions\n7.2.2 Training Process and Optimizers\nDuring training, the network:\nMakes predictions\nCalculates the loss\nUpdates weights via optimizers like:\nSGD (Stochastic Gradient Descent)\nAdam (adaptive learning rate)\nRMSProp\n7.2.3 Overfitting and ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1161f2db-587f-4152-a290-a72a129cf128",
    "text": "Regularization\nOverfitting happens when the model learns noise instead of patterns.\nRegularization techniques help:\nDropout: Randomly turns off neurons during training.\nL2 Regularization: Penalizes large weights (weight decay).\n7.3 Convolutional Neural Networks (CNN)\nCNNs are specialized for image data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 1
    }
  },
  {
    "chunk_id": "587238cf-bd1c-4b15-8a02-3c6b0724b5b4",
    "text": "7.4 Recurrent Neural Networks (RNN)\nRNNs are designed for sequential data (time series, text, etc.).\n7.4.1 Basic RNN vs. LSTM vs. GRU\nBasic RNN: Loops through time steps but suffers from memory issues.\nLSTM (Long Short-Term Memory): Handles long dependencies well.\nGRU (Gated Recurrent Unit): Similar to LSTM but faster.\n7.4.2 Time-Series Prediction and NLP Applications\nPredict stock prices, weather, or language sequences.\nUsed in chatbots, translation, and speech recognition.\n7.4.3 Vanishing and Exploding Gradients\nProblem during training of RNNs where gradients shrink (vanish) or\nexplode.\nLSTM and GRU solve this with gate mechanisms.\n7.5 Generative Adversarial Networks (GANs)\nGANs are powerful models for generating new data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 30,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5067f612-be01-4dd6-bc5a-7de6f6a6d5b9",
    "text": "7.5.1 Generator and Discriminator\nGenerator: Creates fake data\nDiscriminator: Tries to distinguish real from fake data\nThey compete with each other (like a forger and a detective).\n7.5.2 Training Process\nGenerator tries to fool the discriminator\nDiscriminator improves to detect fakes\nThey both improve over time — leading to realistic generated data\n7.5.3 Applications\nImage Generation (e.g., fake faces)\nArt and Style Transfer\nData Augmentation for training other ML models\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 31,
      "chunk": 0
    }
  },
  {
    "chunk_id": "137c21ae-4213-4181-8265-ed13023d3805",
    "text": "NLP helps computers understand, interpret, and generate human language.\nIt's widely used in applications like chatbots, translation tools, and voice\nassistants.\n8.1 Text Preprocessing\nBefore using text in machine learning models, we need to clean and\nconvert it into a format the computer understands.\n8.1.1 Tokenization\nBreaking text into smaller parts like words or sentences.\nExample: \"I love AI\" → [\"I\", \"love\", \"AI\"]\n8.1.2 Stopwords\nRemoving common words that do not add much meaning (like “is”, “the”,\n“and”).\n8.1.3 Stemming\nCutting words down to their root form.\nExample: “playing”, “played” → “play”\n8.1.4 Lemmatization\nSimilar to stemming but uses grammar to find the proper base word.\nExample: “better” → “good”\n8.1.5 Bag of Words (BoW)\nConverts text into numbers based on word counts in a ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c0e76b2a-a359-4490-b41a-807e2f6df71b",
    "text": "document.\n8.1.6 TF-IDF\nGives importance to words that appear often in one document but not in\nothers. Helps identify keywords.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 1
    }
  },
  {
    "chunk_id": "66b2e899-26ea-4cbf-88aa-50c1db3335e7",
    "text": "8.2 Word Embeddings\nWord embeddings turn words into vectors (numbers) so that a machine\ncan understand their meaning and context.\n8.2.1 Word2Vec\nA model that learns how words are related based on their surrounding\nwords.\n8.2.2 GloVe\nLearns word meanings by looking at how often words appear together.\n8.2.3 FastText\nSimilar to Word2Vec but also looks at parts of words, which helps with\nunknown words.\n8.2.4 Sentence Embeddings (BERT, RoBERTa, GPT)\nThese models convert full sentences into vectors. They understand context\nmuch better than older models.\n8.3 Sequence Models\nThese models are good for processing data where order matters, like text.\n8.3.1 RNN (Recurrent Neural Networks)\nGood for learning from sequences, such as sentences.\n8.3.2 LSTM (Long Short-Term Memory)\nAn advanced RNN that reme",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c4825448-52fd-463a-91c5-4abfd0c1d6f7",
    "text": "mbers long-term information.\n8.3.3 GRU (Gated Recurrent Unit)\nA simpler version of LSTM that works faster and often just as well.\n8.4 Transformer Architecture\nTransformers are a powerful model used in almost all modern NLP\nsystems.\n8.4.1 Self-Attention Mechanism\nThis allows the model to focus on important words in a sentence, no\nmatter where they appear.\n8.4.2 Encoder-Decoder Model\nUsed in tasks like translation where the model reads input (encoder) and\ngenerates output (decoder).\n8.4.3 Examples:\nBERT: Great for understanding text.\nGPT: Great for generating text.\nT5: Can both understand and generate text for many tasks.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 1
    }
  },
  {
    "chunk_id": "f8e0c63d-aeae-425b-b7b8-3247849c5339",
    "text": "8. NATURAL LANGUAGE PROCESSING (NLP)\n8.5 Text Classification\nClassify text into categories.\nExamples:\nSentiment Analysis: Is a review positive or negative?\nNamed Entity Recognition (NER): Find names, places, dates, etc. in text.\n8.6 Language Generation\nGenerate new text from existing input.\n8.6.1 Text Summarization\nShortens a long document while keeping important points.\n8.6.2 Machine Translation\nTranslates text from one language to another (like English to Hindi).",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 34,
      "chunk": 0
    }
  },
  {
    "chunk_id": "abf4bfac-b6e3-422f-a8c9-90eb3ab29f45",
    "text": "9. MODEL EVALUATION AND METRICS\nModel evaluation helps us check how well our machine learning models are\nperforming. We use different metrics depending on whether it's a\nclassification or regression problem.\n9.1 Classification Metrics\nUsed when your model predicts categories or classes (e.g., spam or not\nspam).\n9.1.1 Accuracy\nHow often the model is correct.\nFormula: (Correct Predictions) / (Total Predictions)\n9.1.2 Precision\nOut of all predicted positives, how many were actually positive?\nUsed when false positives are costly.\nFormula: TP / (TP + FP)\n9.1.3 Recall (Sensitivity)\nOut of all actual positives, how many were predicted correctly?\nUsed when missing positives is costly.\nFormula: TP / (TP + FN)\n9.1.4 F1-Score\nBalance between precision and recall.\nFormula: 2 * (Precision * Recall) / (",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 0
    }
  },
  {
    "chunk_id": "d9e79d00-e272-48a2-92a2-a8c5d2538860",
    "text": "Precision + Recall)\n9.1.5 Confusion Matrix\nA table showing True Positives, False Positives, False Negatives, and True\nNegatives.\n9.1.6 ROC Curve (Receiver Operating Characteristic)\nShows the trade-off between True Positive Rate and False Positive Rate.\n9.1.7 AUC (Area Under the Curve)\nMeasures the entire area under the ROC curve.\nHigher AUC = better performance.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 1
    }
  },
  {
    "chunk_id": "ca554ae1-4bfd-4771-998a-63d6b18b469e",
    "text": "9. MODEL EVALUATION AND METRICS\n9.2 Regression Metrics\nUsed when the model predicts continuous values (like house price,\ntemperature).\n9.2.1 Mean Absolute Error (MAE)\nAverage of the absolute errors.\nEasy to understand.\n9.2.2 Mean Squared Error (MSE)\nAverage of squared errors.\nPenalizes large errors more than MAE.\n9.2.3 R-Squared (R²)\nExplains how much variance in the output is explained by the model.\nRanges from 0 to 1 (higher is better).\n9.2.4 Adjusted R-Squared\nLike R², but adjusts for the number of predictors (features).\nUseful when comparing models with different numbers of features.\n9.3 Cross-Validation\nUsed to test model performance on different splits of the data.\n9.3.1 K-Fold Cross-Validation\nSplit data into k equal parts. Train on k-1 and test on the remaining part.\nRepeat k times",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f84bc728-eb65-48ff-af7d-67011b16479f",
    "text": ".\n9.3.2 Leave-One-Out Cross-Validation (LOOCV)\nA special case of K-Fold where k = number of data points. Very slow but\nthorough.\n9.3.3 Stratified K-Fold\nSame as K-Fold, but keeps the ratio of classes the same in each fold.\nUseful for imbalanced datasets.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 1
    }
  },
  {
    "chunk_id": "41021d5c-7f46-4d5e-8c8f-36f6b92ff343",
    "text": "9. MODEL EVALUATION AND METRICS\n9.4 Hyperparameter Tuning\nHyperparameters are settings that control how a model learns (like\nlearning rate, depth of a tree, etc.).\n9.4.1 Grid Search\nTests all combinations of given hyperparameter values.\n9.4.2 Random Search\nRandomly selects combinations. Faster than Grid Search.\n9.4.3 Bayesian Optimization\nUses past results to pick the next best combination. Smart and efficient.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 37,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f75acfa5-c8c4-4530-b644-154371a29bc9",
    "text": "10. ADVANCED TOPICS\nThese are modern machine learning methods used in advanced real-world\napplications such as chatbots, recommendation systems, self-driving cars,\nand privacy-focused AI.\n10.1 Transfer Learning\nInstead of training a model from scratch, we use a model that has already\nbeen trained on a large dataset and apply it to a new, similar task.\nPre-trained Models\nThese are models trained on huge datasets.\nExamples:\nVGG, ResNet – for images\nBERT – for text\nFine-Tuning\nSlightly updating the pre-trained model using your own smaller dataset.\nFeature Extraction\nUsing the pre-trained model to extract useful features and then using\nthose features for your own model or task.\nBenefit: Saves training time and works well even with limited data.\n10.2 Attention Mechanism\nThis helps the model dec",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 0
    }
  },
  {
    "chunk_id": "de6aee7a-7dbd-4430-9ca9-9bb46ab5c033",
    "text": "ide which parts of the input data are most\nimportant.\nSelf-Attention\nEvery part of the input focuses on every other part to understand context\nbetter.\nUsed in NLP (Natural Language Processing) and transformers.\nMulti-Head Attention\nApplies attention multiple times in parallel to capture different\nrelationships within the data.\nApplications\nIn NLP: translation, summarization, question-answering\nIn vision: image recognition with Vision Transformers",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 1
    }
  },
  {
    "chunk_id": "912ca3be-5ff7-461c-9a03-49e28a1ce383",
    "text": "10.2 Attention Mechanism\n10.3 Reinforcement Learning in Deep Learning\nCombining deep learning with reinforcement learning for decision-\nmaking tasks.\nActor-Critic\nTwo models work together:\nActor: selects the best action\nCritic: evaluates how good the action was\nA3C (Asynchronous Advantage Actor-Critic)\nUses multiple agents to learn in parallel, which speeds up learning and\nincreases stability.\nPPO (Proximal Policy Optimization)\nAn improved and stable way to train reinforcement learning agents. Used\nin games, robotics, etc.\n10. ADVANCED TOPICS\nMachine\nLearning (ML)\nArtificial\nIntelligence (AI)\nDeep\nLearning (DL)\nReinforcement\nLearning (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 39,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cedb9226-5ef3-4238-9c15-b7d9bbefe538",
    "text": "10.4 Federated Learning\nModel training happens across many devices without collecting data in a\ncentral server. Each device keeps its data private and only sends model\nupdates.\nDistributed Learning Frameworks\nUsed when data is spread across users, hospitals, or devices. Examples\ninclude Google’s keyboard predictions.\nPrivacy-Preserving ML\nSince data never leaves the device, user privacy is protected. This is useful\nin healthcare, banking, and personal mobile applications.\n10. ADVANCED TOPICS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 40,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f72865ce-c0ff-4314-94ac-dc7f55e625e0",
    "text": "These tools help you build, train, and deploy AI/ML models more efficiently.\nThey provide ready-to-use functions so you don’t need to code everything\nfrom scratch.\n11.1 Python Libraries\nPython is the most popular language in AI/ML. These libraries make your\nwork faster and easier:\nNumPy\nUsed for numerical computing.\nSupports arrays, matrices, and linear algebra operations.\nPandas\nUsed for data manipulation and analysis.\nYou can load data, clean it, and analyze it in tabular formats (DataFrames).\nScikit-learn\nA powerful machine learning library.\nIncludes ready-to-use models like Linear Regression, SVM, Random Forest,\nKNN, and more.\nTensorFlow & Keras\nUsed for building deep learning models.\nTensorFlow: low-level control\nKeras: high-level interface, easier to use\nPyTorch\nAn alternative to Ten",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4925f2c6-9549-4a34-9ed8-b23e39f7c4e0",
    "text": "sorFlow, widely used in research and development.\nIt’s flexible, fast, and dynamic (supports on-the-fly computation graphs).\nOpenCV\nUsed for computer vision tasks like image processing, object detection,\nface recognition, etc.\nNLTK & SpaCy\nNatural Language Processing (NLP) libraries.\nNLTK: good for learning, includes many basic NLP tasks\nSpaCy: industrial-strength NLP, faster and more efficient\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 1
    }
  },
  {
    "chunk_id": "53f02b37-4a15-4832-9c4f-554953afbb40",
    "text": "Google Colab\nFree online Jupyter Notebook\nSupports GPU/TPU\nGreat for students and beginners\nAWS SageMaker\nAmazon’s cloud ML platform\nSupports training, tuning, and deploying models at scale\nUsed in enterprise-level applications\nAzure ML\nMicrosoft’s machine learning platform\nIntegrates well with other Microsoft tools (e.g., Excel, Power BI)\nProvides autoML, drag-and-drop pipelines, and more\n11.2 Cloud Platforms\nCloud platforms help you run your models on powerful servers without\nneeding your own hardware.\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 42,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7570ac46-17e8-4f2c-b5dc-5b2c5b6b64e6",
    "text": "12.1 Model Serialization\nWhat it means:\nAfter training your machine learning model, you save (serialize) it to use\nlater without retraining.\nPopular tools:\nPickle – A Python library to serialize and deserialize Python objects.\nJoblib – Similar to Pickle but better for large NumPy arrays.\nExample:\n12.2 Flask/Django for Model Deployment\nThese are web frameworks that let you expose your model as an API\nendpoint, so other apps or users can access it via the internet.\nFlask: Lightweight and easier for quick ML model APIs.\nDjango: Heavier but better for large web applications with built-in admin,\nsecurity, and ORM.\nFlask Example:\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 43,
      "chunk": 0
    }
  },
  {
    "chunk_id": "429cd7bd-84f6-4d0f-8011-668397d2e5a5",
    "text": "12.3 Serving Models with TensorFlow Serving, FastAPI\nTensorFlow Serving:\nUsed to deploy TensorFlow models in production. It supports versioning\nand high-performance serving with REST/gRPC.\nFastAPI:\nA modern, fast (high-performance) framework for building APIs with\nautomatic docs, great for production-grade ML APIs.\nFastAPI Example:\n12.4 Monitoring and Maintaining Models in Production\nOnce your model is live, you need to ensure it continues to perform well.\nWhat to monitor:\nModel accuracy degradation (due to data drift)\nResponse time\nError rates\nSystem metrics (CPU, memory)\nTools:\nPrometheus + Grafana for system and application monitoring\nMLflow or Evidently.ai for tracking model performance over time\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 44,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ce9a7587-fed1-49fd-8520-1b6b3508df22",
    "text": "13.2 Common Beginner Mistakes\nGeneral ML Mistakes\nUsing test data during training\nNot normalizing/scaling features\nIgnoring class imbalance in classification tasks\nForgetting to check for data leakage\nNot splitting the dataset correctly (Train/Validation/Test)\nNeural Network Mistakes\nUsing too many/too few layers without tuning\nChoosing wrong activation/loss functions\nIgnoring overfitting (no dropout or regularization)\nNLP Mistakes\nFeeding raw text without preprocessing\nUsing TF-IDF on small datasets without context\nConfusing stemming with lemmatization\nDeployment Mistakes\nNot checking model performance after deployment\nIgnoring real-time latency\nNo monitoring/logging in place\n13.1 Practice Tasks\nTo strengthen understanding, here are simple practice tasks for each core\nconcept:\n13. PRACTIC",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6b7f48a6-4bf4-4d1e-ad9f-5f2ac79127a1",
    "text": "E & COMMON BEGINNER MISTAKES\nNLP\nPCA\nTopic\nGANs\nCNNs\nRNNs\nDecision Trees\nLinear Regression\nLogistic Regression\nK-Means Clustering\nMini Practice Task\nClassify hand-written digits using the MNIST dataset.\nBuild a sentiment analysis model on product reviews.\nPredict the next word in a sentence using a small corpus.\nGenerate new handwritten digits after training on MNIST.\nReduce dimensions in the Iris dataset and visualize clusters.\nClassify if a person will buy a product based on age, income, etc.\nGroup customers by shopping behavior (customer segmentation).\nPredict house prices using a dataset with features like area, rooms,\nand location.\nBuild a binary classifier to detect spam emails.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 1
    }
  },
  {
    "chunk_id": "911579a8-984c-4510-8e4e-cfd5a7c35905",
    "text": "AI/ML\nCheatSheet",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "3e883f05-8aba-4992-b9ae-67c6f0e7bba1",
    "text": "1. Introduction to AI and ML\nWhat is Artificial Intelligence (AI)?\nTypes of AI: Narrow AI vs. General AI\nWhat is Machine Learning (ML)?\nSupervised vs. Unsupervised vs. Reinforcement Learning\nKey Terminologies (Model, Feature, Target, Algorithm)\nApplications of AI and ML\nDifference Between AI, ML, and Deep Learning (DL)\n2. Mathematics for ML/AI\nLinear Algebra\nVectors, Matrices, and Tensors\nMatrix Operations\nEigenvalues and Eigenvectors\nProbability and Statistics\nMean, Variance, Standard Deviation\nBayes Theorem\nConditional Probability\nProbability Distributions (Normal, Binomial, Poisson)\nCalculus for Optimization\nDerivatives and Gradients\nGradient Descent\n3. Data Preprocessing\nData Cleaning (Missing Values, Outliers)\nData Normalization and Standardization\nEncoding Categorical Variables (One-",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8deff592-4e76-4c9b-8545-7d9ed07866b6",
    "text": "Hot Encoding, Label Encoding)\nFeature Scaling (Min-Max, Z-score)\nFeature Engineering (Polynomial Features, Binning)\nHandling Imbalanced Data (SMOTE, Undersampling, Oversampling)\n4. Supervised Learning Algorithms\nLinear Regression\nSimple vs. Multiple Linear Regression\nGradient Descent and Normal Equation\nRegularization (L1, L2)\nLogistic Regression\nBinary vs. Multiclass Classification\nSigmoid Function and Cost Function\nRegularization\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "932ef025-69d6-4699-9fbf-564c5463ec67",
    "text": "4. Supervised Learning Algorithms\nK-Nearest Neighbors (KNN)\nDistance Metrics (Euclidean, Manhattan)\nChoosing K\nAdvantages and Disadvantages\nSupport Vector Machines (SVM)\nHyperplanes and Margins\nLinear and Non-Linear SVM\nKernel Trick\nDecision Trees\nGini Impurity and Entropy\nOverfitting and Pruning\nRandom Forest\nBootstrapping\nBagging\nFeature Importance\nGradient Boosting Machines (GBM)\nXGBoost, LightGBM, CatBoost\nHyperparameter Tuning\nEarly Stopping\nNaive Bayes\nGaussian, Multinomial, Bernoulli Naive Bayes\nAssumptions and Applications\n5. Unsupervised Learning Algorithms\nK-Means Clustering\nAlgorithm Overview\nElbow Method\nK-Means++ Initialization\nHierarchical Clustering\nAgglomerative vs. Divisive Clustering\nDendrogram and Optimal Cut\nPrincipal Component Analysis (PCA)\nDimensionality Reduction\nEi",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a371682f-6c60-4077-b316-77db66908410",
    "text": "genvalue Decomposition\nScree Plot and Explained Variance\nDBSCAN\nDensity-Based Clustering\nEpsilon and MinPts Parameters\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 3,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d258c4bf-0262-4c6c-b5f2-60050aaebcf0",
    "text": "6. Reinforcement Learning\nIntroduction to Reinforcement Learning\nKey Concepts (Agent, Environment, State, Action, Reward)\nMarkov Decision Process (MDP)\nQ-Learning and Deep Q-Networks (DQN)\nPolicy Gradients and Actor-Critic Methods\nExploration vs. Exploitation\n7. Neural Networks & Deep Learning\nIntroduction to Neural Networks\nPerceptrons\nActivation Functions (Sigmoid, ReLU, Tanh)\nForward Propagation and Backpropagation\nLoss Functions (MSE, Cross-Entropy)\nDeep Neural Networks (DNN)\nArchitecture and Layers\nTraining Process and Optimizers\nOverfitting and Regularization (Dropout, L2 Regularization)\nConvolutional Neural Networks (CNN)\nConvolutional Layers, Pooling Layers\nFilter/Kernels and Strides\nApplications (Image Classification, Object Detection)\nRecurrent Neural Networks (RNN)\nBasic RNN vs.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "171a3405-90b3-4d56-a119-f30075f0f289",
    "text": " LSTM vs. GRU\nTime-Series Prediction and NLP Applications\nVanishing and Exploding Gradients Problem\nGenerative Adversarial Networks (GANs)\nGenerator and Discriminator\nTraining Process\nApplications (Image Generation, Data Augmentation)\n8. Natural Language Processing (NLP)\nText Preprocessing\nTokenization, Stopwords, Lemmatization, Stemming\nBag of Words, TF-IDF\nWord Embeddings\nWord2Vec, GloVe, FastText\nSentence Embeddings (BERT, RoBERTa, GPT)\nSequence Models\nRecurrent Neural Networks (RNNs)\nLSTM and GRU\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9ff132b3-25de-476c-aa10-23e6ff4cf446",
    "text": "8. Natural Language Processing (NLP)\nTransformer Architecture\nSelf-Attention Mechanism\nEncoder-Decoder Model\nBERT, GPT, T5 Models\nText Classification\nSentiment Analysis, Named Entity Recognition (NER)\nLanguage Generation\nText Summarization\nMachine Translation\n9. Model Evaluation and Metrics\nClassification Metrics\nAccuracy, Precision, Recall, F1-Score\nConfusion Matrix\nROC Curve, AUC\nRegression Metrics\nMean Absolute Error (MAE), Mean Squared Error (MSE)\nR-Squared and Adjusted R-Squared\nCross-Validation\nK-Fold Cross-Validation\nLeave-One-Out Cross-Validation\nStratified K-Fold\nHyperparameter Tuning\nGrid Search\nRandom Search\nBayesian Optimization\n10. Advanced Topics\nTransfer Learning\nPre-trained Models (VGG, ResNet, BERT)\nFine-Tuning and Feature Extraction\nAttention Mechanism\nSelf-Attention, Mul",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "b0dd1edf-220e-4fb6-889c-cdae3f0f095a",
    "text": "ti-Head Attention\nApplications in NLP and Vision\nReinforcement Learning in Deep Learning\nActor-Critic, A3C, Proximal Policy Optimization (PPO)\nFederated Learning\nDistributed Learning Frameworks\nPrivacy-Preserving ML\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9658788f-4d58-4575-b1a5-7ff89e33e456",
    "text": "11. Tools and Libraries for AI/ML\nPython Libraries\nNumPy, Pandas\nScikit-learn\nTensorFlow, Keras, PyTorch\nOpenCV for Computer Vision\nNLTK, SpaCy for NLP\nCloud Platforms\nGoogle Colab\nAWS Sagemaker\nAzure ML\n12. Deployment and Production\nModel Serialization (Pickle, Joblib)\nFlask/Django for Model Deployment\nServing Models with TensorFlow Serving, FastAPI\nMonitoring and Maintaining Models in Production\n13. Practice & Common Beginner Mistakes\nPractice Tasks\nCommon Beginner Mistakes\nTABLE OF CONTENTS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f9f691e0-0065-463a-95fb-16d7b4b46a17",
    "text": "1. INTRODUCTION TO AI AND ML\n1.1 What is Artificial Intelligence (AI)?\nArtificial Intelligence (AI) is the ability of machines or computer programs\nto perform tasks that typically require human intelligence. These tasks can\ninclude understanding language, recognizing patterns, solving problems,\nand making decisions.\nSimple explanation:\nAI is when machines are made smart enough to think and act like\nhumans.\nExamples:\nVoice assistants like Alexa\nImage recognition systems\nChatbots\nSelf-driving cars\n1.2 Types of AI: Narrow AI vs. General AI\nNarrow AI (Weak AI):\nDesigned to perform one specific task\nCannot do anything beyond its programming\nExamples: Email spam filters, facial recognition, recommendation systems\nGeneral AI (Strong AI):\nStill under research and development\nCan learn and perform ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1bcb909b-fb84-4d77-8210-1a903f6beec6",
    "text": "any intellectual task a human can do\nWould have reasoning, memory, and decision-making abilities similar to a\nhuman\n1.3 What is Machine Learning (ML)?\nMachine Learning is a subfield of AI that allows machines to learn from\ndata and improve their performance over time without being explicitly\nprogrammed.\nSimple explanation:\nInstead of writing rules for everything, we give the machine data, and it\nfigures out the rules on its own.\nExample:\nA machine learns to identify spam emails by studying thousands of\nexamples.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "c4e15c17-bcce-4f46-a762-013c07caf7f4",
    "text": "1. INTRODUCTION TO AI AND ML\n1.4 Supervised vs. Unsupervised vs. Reinforcement Learning\nSupervised Learning:\nThe training data includes both input and the correct output (labels)\nThe model learns by comparing its output with the correct output\nExample: Predicting house prices based on features like size, location, etc.\nUnsupervised Learning:\nThe data has no labels\nThe model tries to find patterns or groupings in the data\nExample: Grouping customers based on purchasing behavior\nReinforcement Learning:\nThe model learns through trial and error\nIt receives rewards or penalties based on its actions\nExample: A robot learning to walk or a program learning to play chess\n1.5 Key Terminologies\nModel:\nA program or function that makes predictions or decisions based on data.\nFeature:\nAn input variable ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "cdfef520-9fb3-43d8-a0ef-4f9e93cde643",
    "text": "used in making predictions (e.g., age, income,\ntemperature).\nTarget:\nThe value the model is trying to predict (e.g., house price, spam or not).\nAlgorithm:\nA step-by-step method or set of rules used to train the model.\nTraining:\nThe process of teaching the model using a dataset.\nTesting:\nEvaluating the trained model on new, unseen data to measure its\naccuracy.\n1.6 Applications of AI and ML\nRecommendation systems (YouTube, Amazon, Netflix)\nFraud detection in banking\nLanguage translation\nHealthcare diagnosis\nSelf-driving vehicles\nStock market prediction\nChatbots and customer support\nSocial media content moderation",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 8,
      "chunk": 1
    }
  },
  {
    "chunk_id": "99ea03c6-95c9-4a80-8843-db281bbffca0",
    "text": "Term\nDeep Learning (DL)\nMachine Learning (ML)\nArtificial Intelligence (AI)\nDescription\nA subset of AI where machines learn from data\nThe overall field focused on creating intelligent machines\nA specialized type of ML that uses neural networks inspired by the\nhuman brain\n1. INTRODUCTION TO AI AND ML\nDeep Learning\nMachine Learning\nArtificial Intelligence\n1.7 Difference Between AI, ML, and Deep Learning (DL)\nVisual analogy:\nAI is the broader concept, ML is a part of AI, and DL is a part of ML.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "64661957-8b15-443e-99b2-0bb584211314",
    "text": "Mathematics is the foundation of AI and Machine Learning. It helps us\nunderstand how algorithms work under the hood and how to fine-tune\nmodels for better performance.\n2.1 Linear Algebra\nLinear Algebra deals with numbers organized in arrays and how these\narrays interact. It is used in almost every ML algorithm.\n2.1.1 Vectors, Matrices, and Tensors\nVector: A 1D array of numbers. Example: [3, 5, 7]\nUsed to represent features like height, weight, age.\nMatrix: A 2D array (rows and columns).\nExample:\nUsed to store datasets or model weights.\nTensor: A generalization of vectors and matrices to more dimensions (3D\nor higher).\nExample: Used in deep learning models like images (3D tensor: width,\nheight, color channels).\n2.1.2 Matrix Operations\nAddition/Subtraction: Add or subtract corresponding elem",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "787e8b9b-78b3-45f8-a249-11afb1a4f17e",
    "text": "ents of two\nmatrices.\nMultiplication: Used to combine weights and inputs in ML models.\nTranspose: Flip a matrix over its diagonal.\nDot Product: Fundamental in calculating output in neural networks.\n2.1.3 Eigenvalues and Eigenvectors\nEigenvector: A direction that doesn't change during a transformation.\nEigenvalue: Tells how much the eigenvector is stretched or shrunk.\nThese are used in algorithms like Principal Component Analysis (PCA) for\ndimensionality reduction.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7ea13e79-3007-423a-bde1-500e90d67eb9",
    "text": "2.2 Probability and Statistics\nProbability helps machines make decisions under uncertainty, and\nstatistics helps us understand data and model performance.\n2.2.1 Mean, Variance, Standard Deviation\nMean: The average value.\nVariance: How spread out the values are from the mean.\nStandard Deviation: The square root of variance. It measures how much\nthe values vary.\nUsed to understand the distribution and behavior of features in datasets.\n2.2.2 Bayes Theorem\nA mathematical formula to calculate conditional probability:\nUsed in Naive Bayes classifiers for spam detection, document classification,\netc.\n2.2.3 Conditional Probability\nThe probability of one event occurring given that another event has\nalready occurred.\nExample:\nProbability that a user clicks an ad given that they are between 20-30\nyear",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4f5d4a37-a4c3-4a56-881b-cf6e9198f9d6",
    "text": "s old.\n2.2.4 Probability Distributions\nNormal Distribution: Bell-shaped curve. Common in real-world data like\nheight, exam scores.\nBinomial Distribution: Used for yes/no type outcomes. Example: Flipping a\ncoin 10 times.\nPoisson Distribution: For events happening over a time period. Example:\nNumber of customer calls per hour.\nThese distributions help in modeling randomness in data.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 11,
      "chunk": 1
    }
  },
  {
    "chunk_id": "35f26ef2-c66d-4d9c-967e-ccbd05f29383",
    "text": "2.3 Calculus for Optimization\nCalculus helps in training models by optimizing them to reduce errors.\n2.3.1 Derivatives and Gradients\nDerivative: Measures how a function changes as its input changes.\nGradient: A vector of derivatives that tells the slope of a function in multi-\ndimensions.\nUsed to find the direction in which the model should adjust its weights.\n2.3.2 Gradient Descent\nAn optimization algorithm used to minimize the loss (error) function.\nHow it works:\nStart with random values\nCalculate the gradient (slope)\nMove slightly in the opposite direction of the gradient\nRepeat until the loss is minimized\nGradient Descent is the core of many training algorithms in ML and DL.\n2. MATHEMATICS FOR ML/AI",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "89852ed4-18f9-4355-ab0f-e0a64e6c1287",
    "text": "Used when data is normally distributed\nUsed when the data is not normally distributed\nStandardization (Z-score Scaling):\nCenters the data around mean = 0 and standard deviation = 1\nFormula:\nBefore feeding data into any machine learning model, it must be cleaned,\ntransformed, and prepared. This step is called data preprocessing, and it is\none of the most important stages in building accurate ML models.\n3.1 Data Cleaning\nReal-world data is often messy. Data cleaning means identifying and fixing\nerrors in the dataset.\n3.1.1 Missing Values\nMissing values can be due to incomplete forms, sensor errors, etc.\nTechniques to handle missing data:\nRemove rows/columns with too many missing values\nFill (impute) missing values using:\nMean/Median/Mode\nForward/Backward fill\nPredictive models (like KNN)\n3.1",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "712ef743-6e3d-41f8-91db-dc8afa9a469c",
    "text": ".2 Outliers\nOutliers are data points that are very different from others.\nThey can distort results and reduce model performance.\nDetection methods:\nBox plot, Z-score, IQR method\nHandling outliers:\nRemove them\nTransform data (e.g., log scaling)\nCap them (set a maximum/minimum)\n3.2 Data Normalization and Standardization\nHelps scale numeric data so that features contribute equally to the model.\nNormalization (Min-Max Scaling):\nScales all values between 0 and 1\nFormula:\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 13,
      "chunk": 1
    }
  },
  {
    "chunk_id": "98ded92a-45fa-40f4-837b-1e8553f1e8ed",
    "text": "One-Hot Encoding:\nCreates new binary columns for each category.\nExample:\n3.3 Encoding Categorical Variables\nML models work with numbers, not text. Categorical data needs to be\nconverted into numerical form.\nLabel Encoding:\nAssigns each unique category a number.\nExample:\nLabel encoding is good for ordinal data (ranked), while one-hot encoding\nis best for nominal data (non-ranked).\n3.4 Feature Scaling\nEnsures features are on the same scale so the model can learn effectively.\nMin-Max Scaling:\nScales features between 0 and 1.\nGood for algorithms like KNN, neural networks.\nZ-score Scaling (Standardization):\nUseful for models that assume normality, like linear regression or logistic\nregression.\nScaling is crucial for models that use distance or gradient-based optimization.\n3.5 Feature Engineerin",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "77eb2ed4-1649-4e27-9ef6-3027be8115a1",
    "text": "g\nCreating new features or modifying existing ones to improve model\nperformance.\nPolynomial Features:\nCreate new features by raising existing features to a power.\nExample: From x, create x², x³\nBinning (Discretization):\nConvert continuous data into categories.\nExample: Age → [0–18], [19–35], [36–60], 60+\nFeature engineering can significantly boost the predictive power of a model.\n3. DATA PREPROCESSING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e88a481c-8baa-41e9-ab2e-d2ff27127415",
    "text": "3. DATA PREPROCESSING\n3.6 Handling Imbalanced Data\nIn classification, if one class dominates (e.g., 95% non-fraud, 5% fraud),\nmodels may ignore the minority class. This is called class imbalance.\nSMOTE (Synthetic Minority Oversampling Technique):\nCreates synthetic examples of the minority class using nearest neighbors.\nUndersampling:\nRemove some samples from the majority class.\nOversampling:\nDuplicate or generate more samples of the minority class.\nBalancing data improves the ability of the model to correctly predict both\nclasses.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "8f0e16ed-562b-4ed0-bf0d-3b8c245a04f1",
    "text": "Works for small datasets.\n4.1.1 Simple vs. Multiple Linear Regression\nSimple Linear Regression: One input (X) to predict one output (Y).\nExample: Predicting salary from years of experience.\nMultiple Linear Regression: Multiple inputs (X1, X2, ..., Xn).\nExample: Predicting price based on area, location, and age.\n4.1.2 Gradient Descent and Normal Equation\nGradient Descent: Iterative method to minimize error (cost function).\nNormal Equation: Direct way to find weights using linear algebra:\nSupervised learning uses labeled data, meaning the model learns from input-\noutput pairs (X → y). The algorithm tries to map inputs (features) to correct\noutputs (targets/labels).\n4.1 Linear Regression\nUsed for predicting continuous values (e.g., predicting house price,\ntemperature).\n4. SUPERVISED LEARNING ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5b3b619c-4dc9-48dc-9e26-1db2e0488b21",
    "text": "ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 16,
      "chunk": 1
    }
  },
  {
    "chunk_id": "fcb55e9d-dcbe-4752-8118-2e127de16a60",
    "text": "4.1.3 Regularization (L1, L2)\nPrevents overfitting by adding a penalty:\nL1 (Lasso): Can reduce coefficients to 0 (feature selection).\nL2 (Ridge): Shrinks coefficients but doesn’t make them 0.\n4.2 Logistic Regression\nUsed for classification problems (e.g., spam vs. not spam).\n4.2.1 Binary vs. Multiclass Classification\nBinary: 2 outcomes (e.g., 0 or 1)\nMulticlass: More than 2 classes (handled using One-vs-Rest or Softmax)\n4.2.2 Sigmoid and Cost Function\nSigmoid Function: Converts outputs to values between 0 and 1.\nCost Function: Log loss used to measure prediction error.\n4.2.3 Regularization\nL1 and L2 regularization help prevent overfitting in logistic regression as\nwell.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "476799d2-ea97-4175-b232-75dcba94a072",
    "text": "4.3.1 Distance Metrics\nEuclidean Distance: Straight line between two points.\nManhattan Distance: Sum of absolute differences.\n4.3.2 Choosing K\nK is the number of neighbors to consider.\nToo low K → sensitive to noise\nToo high K → model becomes less flexible\n4.3.3 Advantages & Disadvantages\nSimple and easy to implement\nSlow for large datasets, sensitive to irrelevant features\n4.4 Support Vector Machines (SVM)\nPowerful classification model for small to medium-sized datasets.\n4.3 K-Nearest Neighbors (KNN)\nA simple classification (or regression) algorithm that uses proximity.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a1797088-faf0-4317-88d9-20d5063bf49f",
    "text": "4.5.1 Gini Impurity and Entropy\nMeasures how pure a node is:\nGini Impurity: Probability of misclassification.\nEntropy: Measure of randomness/information.\n4.5.2 Overfitting and Pruning\nOverfitting: Tree memorizes training data.\nPruning: Removes unnecessary branches to reduce overfitting.\n4.4.1 Hyperplanes and Margins\nSVM finds the best hyperplane that separates data with maximum\nmargin.\n4.4.2 Linear vs. Non-Linear SVM\nLinear SVM: Works when data is linearly separable.\nNon-linear SVM: Uses kernel trick for complex datasets.\n4.4.3 Kernel Trick\nTransforms data into higher dimensions to make it separable.\nCommon kernels: RBF (Gaussian), Polynomial, Sigmoid\n4.5 Decision Trees\nTree-like structure used for classification and regression.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "bc3442b5-aecd-47c2-8cd2-b034a31265f5",
    "text": "4.7.1 XGBoost, LightGBM, CatBoost\nAdvanced boosting libraries:\nXGBoost: Popular, fast, and accurate\nLightGBM: Faster, uses leaf-wise growth\nCatBoost: Handles categorical features automatically\n4.6.1 Bootstrapping\nRandomly selects subsets of data to train each tree.\n4.6.2 Bagging\nCombines predictions of multiple trees (majority vote or average).\n4.6.3 Feature Importance\nMeasures which features contribute most to model prediction.\n4.7 Gradient Boosting Machines (GBM)\nBoosting is an ensemble method where models are trained sequentially.\n4.6 Random Forest\nAn ensemble of decision trees to improve accuracy and reduce overfitting.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 20,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9b20c350-1b9d-4dcd-8d25-113986239289",
    "text": "4.8.1 Gaussian, Multinomial, Bernoulli\nGaussian NB: For continuous features (assumes normal distribution)\nMultinomial NB: For text data, counts of words\nBernoulli NB: For binary features (0/1)\n4.8.2 Assumptions and Applications\nAssumes all features are independent (rarely true but still works well)\nCommonly used in spam detection, sentiment analysis, document\nclassif ication\n4.7.2 Hyperparameter Tuning\nAdjust parameters like:\nLearning rate\nNumber of estimators (trees)\nMax depth\nTools: GridSearchCV, RandomSearchCV\n4.7.3 Early Stopping\nStops training if the model stops improving on validation set.\n4.8 Naive Bayes\nProbabilistic classifier based on Bayes' Theorem and strong independence\nassumption.\n4. SUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "10808180-7109-4663-920c-1b00bd0b6fa3",
    "text": "Unsupervised learning finds hidden patterns in unlabeled data. Unlike\nsupervised learning, it doesn't rely on labeled outputs (no predefined target).\n5.1 K-Means Clustering\n5.1.1 Algorithm Overview\nK-Means is a clustering algorithm that divides data into K clusters based\non similarity.\nIt works by:\na.Selecting K random centroids.\nb.Assigning each point to the nearest centroid.\nc.Updating the centroid to the mean of its assigned points.\nd.Repeating steps 2–3 until the centroids stop changing.\n5.1.2 Elbow Method\nUsed to choose the optimal number of clusters (K).\nPlot the number of clusters (K) vs. Within-Cluster-Sum-of-Squares (WCSS).\nThe point where the WCSS curve bends (elbow) is the best K.\n5.1.3 K-Means++ Initialization\nImproves basic K-Means by smartly selecting initial centroids, reduc",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4976a8c1-c27e-4e87-8f82-6d4ea9033a46",
    "text": "ing the\nchance of poor clustering.\nStarts with one random centroid, then selects the next ones based on\ndistance from the current ones (probabilistically).\n5.2 Hierarchical Clustering\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 22,
      "chunk": 1
    }
  },
  {
    "chunk_id": "c2bbed51-53f8-4450-9c42-c060f4b501cc",
    "text": "5.2.1 Agglomerative vs. Divisive Clustering\nAgglomerative (bottom-up): Start with each point as its own cluster and\nmerge the closest clusters.\nDivisive (top-down): Start with one large cluster and recursively split it.\nAgglomerative is more commonly used.\n5.2.2 Dendrogram and Optimal Cut\nA dendrogram is a tree-like diagram that shows how clusters are formed\nat each step.\nThe height of branches represents the distance between clusters.\nCutting the dendrogram at a certain height gives the desired number of\nclusters.\n5.3 Principal Component Analysis (PCA)\nPCA is a dimensionality reduction technique used to simplify datasets\nwhile retaining most of the important information.\n5.3.1 Dimensionality Reduction\nPCA transforms the data into a new coordinate system with fewer\ndimensions (called princ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 0
    }
  },
  {
    "chunk_id": "349c430a-68fa-4f27-9a5b-dc59dcdbe446",
    "text": "ipal components).\nUseful for visualization, speeding up algorithms, and avoiding the curse of\ndimensionality.\n5.3.2 Eigenvalue Decomposition\nPCA is based on eigenvectors and eigenvalues of the covariance matrix of\nthe data.\nThe eigenvectors define the new axes (principal components).\nThe eigenvalues indicate the amount of variance each component\ncaptures.\n5.3.3 Scree Plot and Explained Variance\nScree Plot: A plot of eigenvalues to help decide how many components to\nkeep.\nThe explained variance ratio shows how much of the data’s variance is\ncaptured by each component.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 23,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9e80334c-2d28-4186-be8b-0604eeabf9e4",
    "text": "5.4.1 Density-Based Clustering\nUnlike K-Means, DBSCAN doesn't require specifying the number of\nclusters.\nClusters are formed based on dense regions in the data.\n5.4.2 Epsilon and MinPts Parameters\nEpsilon (ε): Radius around a point to search for neighbors.\nMinPts: Minimum number of points required to form a dense region.\nPoints are classified as:\nCore Point: Has MinPts within ε.\nBorder Point: Not a core but within ε of a core.\nNoise: Neither core nor border.\n5.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN is a density-based clustering algorithm that groups closely\npacked points and marks outliers as noise.\n5. UNSUPERVISED LEARNING ALGORITHMS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 24,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7bc510ac-6018-4eea-af93-bc8e60fcd169",
    "text": "6.2 Key Concepts\nAgent: The learner or decision maker (e.g., a robot, self-driving car).\nEnvironment: Everything the agent interacts with (e.g., a maze, a game).\nState: A snapshot of the current situation (e.g., position in a maze).\nAction: A move or decision made by the agent (e.g., turn left).\nReward: Feedback from the environment (e.g., +1 for reaching goal, -1 for\nhitting a wall).\nThe goal of the agent is to maximize cumulative rewards over time.\n6.3 Markov Decision Process (MDP)\nRL problems are often modeled as Markov Decision Processes (MDPs).\nAn MDP includes: S: Set of states A: Set of actions P: Transition\nprobabilities (P(s' | s, a)) R: Reward function γ (gamma): Discount factor\n(how much future rewards are valued) The \"Markov\" property means the\nnext state only depends on the cur",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 0
    }
  },
  {
    "chunk_id": "3e030d93-18c9-414f-b601-c07f6d3abb79",
    "text": "rent state and action, not on previous\nones.\n6.1 Introduction to Reinforcement Learning\nReinforcement Learning (RL) is a type of machine learning where an agent\nlearns to make decisions by interacting with an environment. It is inspired\nby how humans learn from experience — by trial and error.\nThe agent performs an action\nThe environment responds with a reward\nThe agent uses this feedback to learn better actions over time\nUnlike supervised learning, RL doesn’t rely on labeled data. Instead, it uses\nrewards or penalties to guide learning.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 25,
      "chunk": 1
    }
  },
  {
    "chunk_id": "d08305c6-dbe9-42b0-90bc-81247c7bd94f",
    "text": "Where:\ns: current state\na: action\nr: reward\ns': next state\nα: learning rate\nγ: discount factor\nDeep Q-Networks (DQN):\nWhen the state/action space is too large, a neural network is used to\napproximate Q-values.\nCombines Q-Learning with Deep Learning.\nUsed in Atari games and robotics.\n6.4 Q-Learning and Deep Q-Networks (DQN)\nQ-Learning:\nA model-free algorithm that learns the value (Q-value) of taking an action\nin a state.\nUses the formula:\n6.5 Policy Gradients and Actor-Critic Methods\nPolicy Gradients:\nInstead of learning value functions, learn the policy directly (probability\ndistribution over actions).\nUse gradient ascent to improve the policy based on the reward received.\nActor-Critic Methods:\nCombine the best of both worlds:\nActor: chooses the action\nCritic: evaluates how good the action",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ac0d0944-6816-47fe-844b-875a55366717",
    "text": " was (value function)\nMore stable and efficient than pure policy gradients.\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 26,
      "chunk": 1
    }
  },
  {
    "chunk_id": "de196abd-afa9-4b35-bc69-0c5068165954",
    "text": "6.6 Exploration vs. Exploitation\nExploration: Trying new actions to discover their effects (important early in\ntraining).\nExploitation: Choosing the best-known action for maximum reward.\nRL must balance both:\nToo much exploration = slow learning\nToo much exploitation = stuck in local optimum\nCommon strategy: ε-greedy\nChoose a random action with probability ε\nOtherwise, choose the best-known action\n6. REINFORCEMENT LEARNING (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 27,
      "chunk": 0
    }
  },
  {
    "chunk_id": "570963b9-d7a2-4acf-9c03-45a59b574cd5",
    "text": "7.1 Introduction to Neural Networks\nA neural network is a computer model inspired by the human brain. It\nconsists of neurons (nodes) organized in layers, capable of learning\npatterns from data.\n7.1.1 Perceptrons\nThe perceptron is the simplest type of neural network, with:\nInputs → Weights → Summation → Activation Function → Output\nIt's like a yes/no decision maker (binary classification).\n7.1.2 Activation Functions\nThese introduce non-linearity, allowing the network to learn complex\nfunctions:\nSigmoid: Outputs between 0 and 1. Good for probability-based outputs.\nReLU (Rectified Linear Unit): Most popular. Fast, reduces vanishing\ngradient.\nReLU(x) = max(0, x)\nTanh: Like sigmoid but outputs between -1 and 1.\n7.1.3 Forward Propagation and Backpropagation\nForward Propagation: Input data flows ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9d0199bd-fa7f-4924-bffb-73f4ffa850ee",
    "text": "through the network to produce an\noutput.\nBackpropagation: Calculates the error and updates weights using\ngradients (from loss function).\nThis is how neural networks learn from data.\n7.1.4 Loss Functions\nThey measure how far off the prediction is from the actual result.\nMSE (Mean Squared Error): Used in regression problems.\nCross-Entropy Loss: Used in classification tasks.\n7.2 Deep Neural Networks (DNN)\nA Deep Neural Network has multiple hidden layers between input and\noutput.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 28,
      "chunk": 1
    }
  },
  {
    "chunk_id": "3eb8a487-6896-4995-9b3a-f1733118c4db",
    "text": "7.3.1 Convolutional Layers, Pooling Layers\nConvolutional Layers: Apply filters to detect features (edges, corners).\nPooling Layers: Reduce size of feature maps (e.g., Max Pooling).\n7.3.2 Filters/Kernels and Strides\nFilters: Small matrix to slide over input to extract features.\nStrides: Step size of the filter as it moves.\n7.3.3 Applications\nImage Classification\nFace Recognition\nObject Detection\n7.2.1 Architecture and Layers\nInput Layer: Where the data comes in\nHidden Layers: Where computation happens (many neurons per layer)\nOutput Layer: Final predictions\n7.2.2 Training Process and Optimizers\nDuring training, the network:\nMakes predictions\nCalculates the loss\nUpdates weights via optimizers like:\nSGD (Stochastic Gradient Descent)\nAdam (adaptive learning rate)\nRMSProp\n7.2.3 Overfitting and ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c80992cb-11e9-4393-9a55-2c28d4d36a52",
    "text": "Regularization\nOverfitting happens when the model learns noise instead of patterns.\nRegularization techniques help:\nDropout: Randomly turns off neurons during training.\nL2 Regularization: Penalizes large weights (weight decay).\n7.3 Convolutional Neural Networks (CNN)\nCNNs are specialized for image data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 29,
      "chunk": 1
    }
  },
  {
    "chunk_id": "8957b611-7439-4c6c-8d2a-fb90e6d11875",
    "text": "7.4 Recurrent Neural Networks (RNN)\nRNNs are designed for sequential data (time series, text, etc.).\n7.4.1 Basic RNN vs. LSTM vs. GRU\nBasic RNN: Loops through time steps but suffers from memory issues.\nLSTM (Long Short-Term Memory): Handles long dependencies well.\nGRU (Gated Recurrent Unit): Similar to LSTM but faster.\n7.4.2 Time-Series Prediction and NLP Applications\nPredict stock prices, weather, or language sequences.\nUsed in chatbots, translation, and speech recognition.\n7.4.3 Vanishing and Exploding Gradients\nProblem during training of RNNs where gradients shrink (vanish) or\nexplode.\nLSTM and GRU solve this with gate mechanisms.\n7.5 Generative Adversarial Networks (GANs)\nGANs are powerful models for generating new data.\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 30,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ef3d00fc-52ee-4ce2-bf1a-cbaf72f7af9f",
    "text": "7.5.1 Generator and Discriminator\nGenerator: Creates fake data\nDiscriminator: Tries to distinguish real from fake data\nThey compete with each other (like a forger and a detective).\n7.5.2 Training Process\nGenerator tries to fool the discriminator\nDiscriminator improves to detect fakes\nThey both improve over time — leading to realistic generated data\n7.5.3 Applications\nImage Generation (e.g., fake faces)\nArt and Style Transfer\nData Augmentation for training other ML models\n7. NEURAL NETWORKS & DEEP LEARNING",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 31,
      "chunk": 0
    }
  },
  {
    "chunk_id": "0cbdddaf-e3b5-4013-af4e-fd1528a19efe",
    "text": "NLP helps computers understand, interpret, and generate human language.\nIt's widely used in applications like chatbots, translation tools, and voice\nassistants.\n8.1 Text Preprocessing\nBefore using text in machine learning models, we need to clean and\nconvert it into a format the computer understands.\n8.1.1 Tokenization\nBreaking text into smaller parts like words or sentences.\nExample: \"I love AI\" → [\"I\", \"love\", \"AI\"]\n8.1.2 Stopwords\nRemoving common words that do not add much meaning (like “is”, “the”,\n“and”).\n8.1.3 Stemming\nCutting words down to their root form.\nExample: “playing”, “played” → “play”\n8.1.4 Lemmatization\nSimilar to stemming but uses grammar to find the proper base word.\nExample: “better” → “good”\n8.1.5 Bag of Words (BoW)\nConverts text into numbers based on word counts in a ",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 0
    }
  },
  {
    "chunk_id": "92568b4e-bb9a-4191-beaf-4c05bc6e4591",
    "text": "document.\n8.1.6 TF-IDF\nGives importance to words that appear often in one document but not in\nothers. Helps identify keywords.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 32,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4aaa367f-cf5b-453e-91e3-b0a788228f25",
    "text": "8.2 Word Embeddings\nWord embeddings turn words into vectors (numbers) so that a machine\ncan understand their meaning and context.\n8.2.1 Word2Vec\nA model that learns how words are related based on their surrounding\nwords.\n8.2.2 GloVe\nLearns word meanings by looking at how often words appear together.\n8.2.3 FastText\nSimilar to Word2Vec but also looks at parts of words, which helps with\nunknown words.\n8.2.4 Sentence Embeddings (BERT, RoBERTa, GPT)\nThese models convert full sentences into vectors. They understand context\nmuch better than older models.\n8.3 Sequence Models\nThese models are good for processing data where order matters, like text.\n8.3.1 RNN (Recurrent Neural Networks)\nGood for learning from sequences, such as sentences.\n8.3.2 LSTM (Long Short-Term Memory)\nAn advanced RNN that reme",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f898690e-5779-467a-aeda-5ac19a460e43",
    "text": "mbers long-term information.\n8.3.3 GRU (Gated Recurrent Unit)\nA simpler version of LSTM that works faster and often just as well.\n8.4 Transformer Architecture\nTransformers are a powerful model used in almost all modern NLP\nsystems.\n8.4.1 Self-Attention Mechanism\nThis allows the model to focus on important words in a sentence, no\nmatter where they appear.\n8.4.2 Encoder-Decoder Model\nUsed in tasks like translation where the model reads input (encoder) and\ngenerates output (decoder).\n8.4.3 Examples:\nBERT: Great for understanding text.\nGPT: Great for generating text.\nT5: Can both understand and generate text for many tasks.\n8. NATURAL LANGUAGE PROCESSING (NLP)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 33,
      "chunk": 1
    }
  },
  {
    "chunk_id": "971000eb-5ea1-487a-b791-1b91994afd57",
    "text": "8. NATURAL LANGUAGE PROCESSING (NLP)\n8.5 Text Classification\nClassify text into categories.\nExamples:\nSentiment Analysis: Is a review positive or negative?\nNamed Entity Recognition (NER): Find names, places, dates, etc. in text.\n8.6 Language Generation\nGenerate new text from existing input.\n8.6.1 Text Summarization\nShortens a long document while keeping important points.\n8.6.2 Machine Translation\nTranslates text from one language to another (like English to Hindi).",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 34,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c3c167e8-0a0b-4ba6-bedd-63742530d169",
    "text": "9. MODEL EVALUATION AND METRICS\nModel evaluation helps us check how well our machine learning models are\nperforming. We use different metrics depending on whether it's a\nclassification or regression problem.\n9.1 Classification Metrics\nUsed when your model predicts categories or classes (e.g., spam or not\nspam).\n9.1.1 Accuracy\nHow often the model is correct.\nFormula: (Correct Predictions) / (Total Predictions)\n9.1.2 Precision\nOut of all predicted positives, how many were actually positive?\nUsed when false positives are costly.\nFormula: TP / (TP + FP)\n9.1.3 Recall (Sensitivity)\nOut of all actual positives, how many were predicted correctly?\nUsed when missing positives is costly.\nFormula: TP / (TP + FN)\n9.1.4 F1-Score\nBalance between precision and recall.\nFormula: 2 * (Precision * Recall) / (",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 0
    }
  },
  {
    "chunk_id": "3f23b9e9-8ce1-4129-bc80-b478e45c0eb8",
    "text": "Precision + Recall)\n9.1.5 Confusion Matrix\nA table showing True Positives, False Positives, False Negatives, and True\nNegatives.\n9.1.6 ROC Curve (Receiver Operating Characteristic)\nShows the trade-off between True Positive Rate and False Positive Rate.\n9.1.7 AUC (Area Under the Curve)\nMeasures the entire area under the ROC curve.\nHigher AUC = better performance.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 35,
      "chunk": 1
    }
  },
  {
    "chunk_id": "4684e4ca-a447-4b52-9d1b-f89b9f4fa8de",
    "text": "9. MODEL EVALUATION AND METRICS\n9.2 Regression Metrics\nUsed when the model predicts continuous values (like house price,\ntemperature).\n9.2.1 Mean Absolute Error (MAE)\nAverage of the absolute errors.\nEasy to understand.\n9.2.2 Mean Squared Error (MSE)\nAverage of squared errors.\nPenalizes large errors more than MAE.\n9.2.3 R-Squared (R²)\nExplains how much variance in the output is explained by the model.\nRanges from 0 to 1 (higher is better).\n9.2.4 Adjusted R-Squared\nLike R², but adjusts for the number of predictors (features).\nUseful when comparing models with different numbers of features.\n9.3 Cross-Validation\nUsed to test model performance on different splits of the data.\n9.3.1 K-Fold Cross-Validation\nSplit data into k equal parts. Train on k-1 and test on the remaining part.\nRepeat k times",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 0
    }
  },
  {
    "chunk_id": "3fbd0684-0a37-43e9-9d6b-df151fc82433",
    "text": ".\n9.3.2 Leave-One-Out Cross-Validation (LOOCV)\nA special case of K-Fold where k = number of data points. Very slow but\nthorough.\n9.3.3 Stratified K-Fold\nSame as K-Fold, but keeps the ratio of classes the same in each fold.\nUseful for imbalanced datasets.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 36,
      "chunk": 1
    }
  },
  {
    "chunk_id": "83d1f7c6-b029-4115-96a2-8ba56180d1bf",
    "text": "9. MODEL EVALUATION AND METRICS\n9.4 Hyperparameter Tuning\nHyperparameters are settings that control how a model learns (like\nlearning rate, depth of a tree, etc.).\n9.4.1 Grid Search\nTests all combinations of given hyperparameter values.\n9.4.2 Random Search\nRandomly selects combinations. Faster than Grid Search.\n9.4.3 Bayesian Optimization\nUses past results to pick the next best combination. Smart and efficient.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 37,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4d268c9b-56d3-46d1-b156-eb9c06a46313",
    "text": "10. ADVANCED TOPICS\nThese are modern machine learning methods used in advanced real-world\napplications such as chatbots, recommendation systems, self-driving cars,\nand privacy-focused AI.\n10.1 Transfer Learning\nInstead of training a model from scratch, we use a model that has already\nbeen trained on a large dataset and apply it to a new, similar task.\nPre-trained Models\nThese are models trained on huge datasets.\nExamples:\nVGG, ResNet – for images\nBERT – for text\nFine-Tuning\nSlightly updating the pre-trained model using your own smaller dataset.\nFeature Extraction\nUsing the pre-trained model to extract useful features and then using\nthose features for your own model or task.\nBenefit: Saves training time and works well even with limited data.\n10.2 Attention Mechanism\nThis helps the model dec",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 0
    }
  },
  {
    "chunk_id": "81681a9c-4fb4-4acd-bc91-9329ecfa365f",
    "text": "ide which parts of the input data are most\nimportant.\nSelf-Attention\nEvery part of the input focuses on every other part to understand context\nbetter.\nUsed in NLP (Natural Language Processing) and transformers.\nMulti-Head Attention\nApplies attention multiple times in parallel to capture different\nrelationships within the data.\nApplications\nIn NLP: translation, summarization, question-answering\nIn vision: image recognition with Vision Transformers",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 38,
      "chunk": 1
    }
  },
  {
    "chunk_id": "269c6681-470d-4d41-8453-ca19856747e4",
    "text": "10.2 Attention Mechanism\n10.3 Reinforcement Learning in Deep Learning\nCombining deep learning with reinforcement learning for decision-\nmaking tasks.\nActor-Critic\nTwo models work together:\nActor: selects the best action\nCritic: evaluates how good the action was\nA3C (Asynchronous Advantage Actor-Critic)\nUses multiple agents to learn in parallel, which speeds up learning and\nincreases stability.\nPPO (Proximal Policy Optimization)\nAn improved and stable way to train reinforcement learning agents. Used\nin games, robotics, etc.\n10. ADVANCED TOPICS\nMachine\nLearning (ML)\nArtificial\nIntelligence (AI)\nDeep\nLearning (DL)\nReinforcement\nLearning (RL)",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 39,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f64a116b-e104-4679-90c7-f5940b665f24",
    "text": "10.4 Federated Learning\nModel training happens across many devices without collecting data in a\ncentral server. Each device keeps its data private and only sends model\nupdates.\nDistributed Learning Frameworks\nUsed when data is spread across users, hospitals, or devices. Examples\ninclude Google’s keyboard predictions.\nPrivacy-Preserving ML\nSince data never leaves the device, user privacy is protected. This is useful\nin healthcare, banking, and personal mobile applications.\n10. ADVANCED TOPICS",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 40,
      "chunk": 0
    }
  },
  {
    "chunk_id": "c187c9ea-611a-4dc7-a704-7e103e1bd0a2",
    "text": "These tools help you build, train, and deploy AI/ML models more efficiently.\nThey provide ready-to-use functions so you don’t need to code everything\nfrom scratch.\n11.1 Python Libraries\nPython is the most popular language in AI/ML. These libraries make your\nwork faster and easier:\nNumPy\nUsed for numerical computing.\nSupports arrays, matrices, and linear algebra operations.\nPandas\nUsed for data manipulation and analysis.\nYou can load data, clean it, and analyze it in tabular formats (DataFrames).\nScikit-learn\nA powerful machine learning library.\nIncludes ready-to-use models like Linear Regression, SVM, Random Forest,\nKNN, and more.\nTensorFlow & Keras\nUsed for building deep learning models.\nTensorFlow: low-level control\nKeras: high-level interface, easier to use\nPyTorch\nAn alternative to Ten",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 0
    }
  },
  {
    "chunk_id": "250a581c-bc0c-4d9b-a0ee-d55dedbc4eb9",
    "text": "sorFlow, widely used in research and development.\nIt’s flexible, fast, and dynamic (supports on-the-fly computation graphs).\nOpenCV\nUsed for computer vision tasks like image processing, object detection,\nface recognition, etc.\nNLTK & SpaCy\nNatural Language Processing (NLP) libraries.\nNLTK: good for learning, includes many basic NLP tasks\nSpaCy: industrial-strength NLP, faster and more efficient\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 41,
      "chunk": 1
    }
  },
  {
    "chunk_id": "72f2f70a-5e98-4664-8621-368c1f8ce5b1",
    "text": "Google Colab\nFree online Jupyter Notebook\nSupports GPU/TPU\nGreat for students and beginners\nAWS SageMaker\nAmazon’s cloud ML platform\nSupports training, tuning, and deploying models at scale\nUsed in enterprise-level applications\nAzure ML\nMicrosoft’s machine learning platform\nIntegrates well with other Microsoft tools (e.g., Excel, Power BI)\nProvides autoML, drag-and-drop pipelines, and more\n11.2 Cloud Platforms\nCloud platforms help you run your models on powerful servers without\nneeding your own hardware.\n11. TOOLS AND LIBRARIES FOR AI/ML",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 42,
      "chunk": 0
    }
  },
  {
    "chunk_id": "90926ae4-95b6-430d-b8e9-bbe8a16a24ca",
    "text": "12.1 Model Serialization\nWhat it means:\nAfter training your machine learning model, you save (serialize) it to use\nlater without retraining.\nPopular tools:\nPickle – A Python library to serialize and deserialize Python objects.\nJoblib – Similar to Pickle but better for large NumPy arrays.\nExample:\n12.2 Flask/Django for Model Deployment\nThese are web frameworks that let you expose your model as an API\nendpoint, so other apps or users can access it via the internet.\nFlask: Lightweight and easier for quick ML model APIs.\nDjango: Heavier but better for large web applications with built-in admin,\nsecurity, and ORM.\nFlask Example:\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 43,
      "chunk": 0
    }
  },
  {
    "chunk_id": "a7c6ed26-4b31-4009-9faf-190520c4eb8f",
    "text": "12.3 Serving Models with TensorFlow Serving, FastAPI\nTensorFlow Serving:\nUsed to deploy TensorFlow models in production. It supports versioning\nand high-performance serving with REST/gRPC.\nFastAPI:\nA modern, fast (high-performance) framework for building APIs with\nautomatic docs, great for production-grade ML APIs.\nFastAPI Example:\n12.4 Monitoring and Maintaining Models in Production\nOnce your model is live, you need to ensure it continues to perform well.\nWhat to monitor:\nModel accuracy degradation (due to data drift)\nResponse time\nError rates\nSystem metrics (CPU, memory)\nTools:\nPrometheus + Grafana for system and application monitoring\nMLflow or Evidently.ai for tracking model performance over time\n12. DEPLOYMENT AND PRODUCTION",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 44,
      "chunk": 0
    }
  },
  {
    "chunk_id": "981a4ac5-83a1-49f8-bfe0-2555f0ff52e6",
    "text": "13.2 Common Beginner Mistakes\nGeneral ML Mistakes\nUsing test data during training\nNot normalizing/scaling features\nIgnoring class imbalance in classification tasks\nForgetting to check for data leakage\nNot splitting the dataset correctly (Train/Validation/Test)\nNeural Network Mistakes\nUsing too many/too few layers without tuning\nChoosing wrong activation/loss functions\nIgnoring overfitting (no dropout or regularization)\nNLP Mistakes\nFeeding raw text without preprocessing\nUsing TF-IDF on small datasets without context\nConfusing stemming with lemmatization\nDeployment Mistakes\nNot checking model performance after deployment\nIgnoring real-time latency\nNo monitoring/logging in place\n13.1 Practice Tasks\nTo strengthen understanding, here are simple practice tasks for each core\nconcept:\n13. PRACTIC",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 0
    }
  },
  {
    "chunk_id": "616b3694-8c83-4512-ad0f-14d3687d816f",
    "text": "E & COMMON BEGINNER MISTAKES\nNLP\nPCA\nTopic\nGANs\nCNNs\nRNNs\nDecision Trees\nLinear Regression\nLogistic Regression\nK-Means Clustering\nMini Practice Task\nClassify hand-written digits using the MNIST dataset.\nBuild a sentiment analysis model on product reviews.\nPredict the next word in a sentence using a small corpus.\nGenerate new handwritten digits after training on MNIST.\nReduce dimensions in the Iris dataset and visualize clusters.\nClassify if a person will buy a product based on age, income, etc.\nGroup customers by shopping behavior (customer segmentation).\nPredict house prices using a dataset with features like area, rooms,\nand location.\nBuild a binary classifier to detect spam emails.",
    "metadata": {
      "filename": "AI_ML Cheat Sheet.pdf",
      "page": 45,
      "chunk": 1
    }
  },
  {
    "chunk_id": "9dde51a1-9883-41fe-ace0-ac4a8d3d6bf8",
    "text": "GIT CHEAT SHEET\nSTAGE & SNAPSHOT\nWorking with snapshots and the Git staging area\ngit status\nshow modiﬁed ﬁles in working directory, staged for your next commit\ngit add [file]\nadd a ﬁle as it looks now to your next commit (stage)\ngit reset [file]\nunstage a ﬁle while retaining the changes in working directory\ngit diff\ndiﬀ of what is changed but not staged\ngit diff --staged\ndiﬀ of what is staged but not yet committed\ngit commit -m “[descriptive message]”\ncommit your staged content as a new commit snapshot\nSETUP\nConﬁguring user information used across all local repositories\ngit config --global user.name “[firstname lastname]”\nset a name that is identiﬁable for credit when review version history\ngit config --global user.email “[valid-email]”\nset an email address that will be associated with eac",
    "metadata": {
      "filename": "git-cheat-sheet-education.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "af4557c9-20bc-4095-8f91-29fd77e429dc",
    "text": "h history marker\ngit config --global color.ui auto\nset automatic command line coloring for Git for easy reviewing\nSETUP & INIT\nConﬁguring user information, initializing and cloning repositories\ngit init\ninitialize an existing directory as a Git repository\ngit clone [url]\nretrieve an entire repository from a hosted location via URL\nBRANCH & MERGE\nIsolating work in branches, changing context, and integrating changes\ngit branch\nlist your branches. a * will appear next to the currently active branch\ngit branch [branch-name]\ncreate a new branch at the current commit\ngit checkout\nswitch to another branch and check it out into your working directory\ngit merge [branch]\nmerge the speciﬁed branch’s history into the current one\ngit log\nshow all commits in the current branch’s history\nGit is the free ",
    "metadata": {
      "filename": "git-cheat-sheet-education.pdf",
      "page": 1,
      "chunk": 1
    }
  },
  {
    "chunk_id": "7fdaec58-c008-4da4-8b89-40936c7d5647",
    "text": "and open source distributed version control system that's responsible for everything GitHub \nrelated that happens locally on your computer. This cheat sheet features the most important and commonly \nused Git commands for easy reference.\nINSTALLATION & GUIS\nWith platform speciﬁc installers for Git, GitHub also provides the \nease of staying up-to-date with the latest releases of the command \nline tool while providing a graphical user interface for day-to-day \ninteraction, review, and repository synchronization.\nGitHub for Windows\nhttps://windows.github.com\nGitHub for Mac\nhttps://mac.github.com\nFor Linux and Solaris platforms, the latest release is available on \nthe oﬃcial Git web site.\nGit for All Platforms\nhttp://git-scm.com",
    "metadata": {
      "filename": "git-cheat-sheet-education.pdf",
      "page": 1,
      "chunk": 2
    }
  },
  {
    "chunk_id": "eadcd24a-cfed-4d45-a1a6-8038ca4f3752",
    "text": "education@github.com \neducation.github.com\nEducation\nTeach and learn better, together. GitHub is free for students and teach-\ners. Discounts available for other educational uses.\nTeach and learn better, together. GitHub is free for students and teach-\ners. Discounts available for other educational uses.\nSHARE & UPDATE\nRetrieving updates from another repository and updating local repos\ngit remote add [alias] [url]\nadd a git URL as an alias\ngit fetch [alias]\nfetch down all the branches from that Git remote\ngit merge [alias]/[branch]\nmerge a remote branch into your current branch to bring it up to date\ngit push [alias] [branch]\nTransmit local branch commits to the remote repository branch\ngit pull\nfetch and merge any commits from the tracking remote branch\nTRACKING PATH CHANGES\nVersioning ﬁle",
    "metadata": {
      "filename": "git-cheat-sheet-education.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e5c6680d-9f86-4ac6-bd54-789d3ac886ed",
    "text": " removes and path changes\ngit rm [file]\ndelete the ﬁle from project and stage the removal for commit\ngit mv [existing-path] [new-path]\nchange an existing ﬁle path and stage the move\ngit log --stat -M\nshow all commit logs with indication of any paths that moved TEMPORARY COMMITS\nTemporarily store modiﬁed, tracked ﬁles  in order to change branches\ngit stash\nSave modiﬁed and staged changes\ngit stash list\nlist stack-order of stashed ﬁle changes\ngit stash pop\nwrite working from top of stash stack\ngit stash drop\ndiscard  the changes from top of stash stack\nREWRITE HISTORY\nRewriting branches, updating commits and clearing history\ngit rebase [branch]\napply any commits of current branch ahead of speciﬁed one\ngit reset --hard [commit]\nclear staging area, rewrite working tree from speciﬁed commit\nINS",
    "metadata": {
      "filename": "git-cheat-sheet-education.pdf",
      "page": 2,
      "chunk": 1
    }
  },
  {
    "chunk_id": "ec50a252-b5e0-472a-a7cf-1f934f9bcdae",
    "text": "PECT & COMPARE\nExamining logs, diﬀs and object information\ngit log\nshow the commit history for the currently active branch\ngit log branchB..branchA\nshow the commits on branchA that are not on branchB\ngit log --follow [file]\nshow the commits that changed ﬁle, even across renames\ngit diff branchB...branchA\nshow the diﬀ of what is in branchA that is not in branchB\ngit show [SHA]\nshow any object in Git in human-readable format\nIGNORING PATTERNS\nPreventing unintentional staging or commiting of ﬁles\ngit config --global core.excludesfile [file]\nsystem wide ignore pattern for all local repositories\nlogs/\n*.notes\npattern*/\nSave a ﬁle with desired patterns as .gitignore with either direct string \nmatches or wildcard globs.",
    "metadata": {
      "filename": "git-cheat-sheet-education.pdf",
      "page": 2,
      "chunk": 2
    }
  },
  {
    "chunk_id": "4e977398-cde8-4010-aadd-f206f45f4bb9",
    "text": " \n \nAI  Engineer  \nRoadmap  2025 🚀  \nBuild  Production  Agentic  AI,  RAG  pipelines,  Fine-Tuning  LLMs,  \nReinforcement\n \nLearning,\n \nwith\n \n11\n \nindustry-level\n \napps.\n \n   \n  Himanshu  Ramchandani                            Microsoft  -  MVP          \n",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 1,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9d5eb4af-d86b-4715-9c1a-4ee77a277cc8",
    "text": "   \nDetailed  Roadmap:  https://god-level-python.notion.site/AI-Engineer-HQ-b3c98407b4ab45819811db081ae9d102?pvs=4  \nCurriculum\n \nPrerequsites  -  7-Step  AI  Prep  Challenge   1.  Foundations  of  AI  Engineering  2.  Mastering  Large  Language  Models  (LLMs)  3.  Retrieval-Augmented  Generation  (RAG)  4.  Fine-Tuning  LLMs  5.  Reinforcement  Learning  and  Ethical  AI  6.  Agentic  Workﬂows  7.  Career  Acceleration  8.  Bonus  \n          ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 2,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f5ba5888-1575-43ae-a201-88830f1cd160",
    "text": "  \nPrerequsites  \n \n7-Step  AI  Prep  Challenge  \nThis  challenge  is  for  you  to  get  started.  You  may  already  know  about  these  topics  \nand\n \ncan\n \ncomplete\n \nthe\n \nchallenge\n \nin\n \n1\n \nday\n \nas\n \nwell.\n \n                  \nAccess  the  Challenge  links  here:  https://god-level-python.notion.site/7-Step-AI-Bootcamp-Prep-Challenge-1c3ffb33c49580ef92eae98681a2ec6b?pvs=4  \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 3,
      "chunk": 0
    }
  },
  {
    "chunk_id": "2909f5d2-67e3-4772-be33-9c8f3f86bf6c",
    "text": " \n[Module  -  1]  \nFoundations  of  AI  Engineering  \n \n1.1  -  Python  \n1.1.1  -  [Hands-On]  Functions  &  Higher  Order  Functions  \n1.1.2  -  [Hands-On]  Modules,  Packages,  Library  &  \nFramework\n \n1.1.3  -  [Hands-On]  OOPs  [Object  Oriented  Programming]  \n1.1.4  -  [Hands-On]  Data  Structures  &  Algorithms  \n1.1.5  -  [Hands-On]  Data  Manipulation  [NumPy  &  Pandas]  \n1.2  -  Mathematics  in  AI  \n1.2.1  -  Linear  Algebra  \n1.2.2  -  Calculus  \n1.2.3  -  Statistics  &  Probability  \n1.3  -  Overview  of  the  AI  Ecosystem  \n1.3.1  -  AI  and  Its  Evolution  \n1.3.2  -  AI  vs  ML  vs  DL  vs  GenAI  vs  LLM  vs  ChatGPT  vs  RL  \n1.3.3  -  LLM  Ecosystem  -  ChatGPT,  Grok,  HuggingFace  \n1.3.4  -  AI  Market  Analysis  &  Career  Opportunity  \n1.3.5  -  AI  Use  Cases  &  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 4,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4c69a0f7-2bba-4135-8047-213b33b0fc69",
    "text": "Tools  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 4,
      "chunk": 1
    }
  },
  {
    "chunk_id": "186e04f9-4b63-4d0b-9009-09b637ea320c",
    "text": "1.4  -  Machine  Learning  as  of  2025  \n1.4.1  -  All  you  need  to  know  about  Machine  Learning  \n1.4.2  -  [Hands-On]  Building  a  Classiﬁcation  Model  \n1.4.3  -  [Hands-On]  Building  Multiple  Linear  Regression  \nmodel\n \n1.4.4  -  When  to  use  Which  ML  Algorithm?  \n1.5  -  Deep  Learning  as  of  2025  \n1.5.1  -  [Hands-On]  Building  Your  First  Neural  Network  \n1.5.2  -  [Hands-On]  Activation  Functions  from  Scratch  \n1.5.3  -  Drawbacks  in  RNN,  CNN,  LSTM  architecture  \n1.6  -  The  Project  Lab  [Build-Deploy-Market]  \n[The  Project  Lab  -  01]  AI-powered  Resume  Analyzer  using  \nPython,\n \nFlask\n \n&\n \nNLP\n \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n1.7  -  Interview  &  Resources  \nTechnical  Interview  Practice  Que",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 5,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ad793404-6be9-40c3-9b8c-91b34367315a",
    "text": "stions  \n[Task]  -  Research  Papers  \n \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 5,
      "chunk": 1
    }
  },
  {
    "chunk_id": "c18a3068-b0a2-4cc3-9871-472969e191a6",
    "text": " \n[Module  -  2]  \nMastering  Large  Language  Models  (LLMs)  \n \n2.1  -  LLM  Ecosystem  and  Access  \n2.1.1  -  Introduction  to  Transformer  Architecture  \n2.1.2  -  LLM  Model  Architectures  \n2.1.3  -  How  to  train  LLMs?  \n2.1.4  -  [Cloud  providers]  Azure  Open  AI,  AWS  Bedrock,  \nGCP\n \nVertex\n \nAI\n \n2.1.5  -  [Open-source  LLMs]  DeepSeek,  LLaMA,  Mistral  7b  \n(via\n \nHugging\n \nFace)\n \n2.1.6  -  [Hands-On]  Setup  LLM  on  your  Local  machine  \nusing\n \nOllama\n \n2.1.7  -  [Hands-On]  Sentiment  classiﬁcation  pipeline  for  \nAmazon\n \nproduct\n \nreviews\n \n2.2  -  Enterprise  Applications  \n2.2.1  -  Business  problems  solved  by  LLMs  \n2.2.2  -  Workﬂow  for  developing  LLM-based  applications  \n2.2.3  -  [Hands-On]  Azure  Open  AI’s  Python  API  to  \ngenerate\n \ntext\n \n2",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 6,
      "chunk": 0
    }
  },
  {
    "chunk_id": "de3ecef7-83db-4b63-b4cb-5ee0cf979813",
    "text": ".2.4  -  [Cost-beneﬁt  analysis]  Cloud  vs.  on-premise  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 6,
      "chunk": 1
    }
  },
  {
    "chunk_id": "08231598-93b2-49c8-8429-0ae93bfeffa5",
    "text": "2.2.5  -  [Hands-On]  HR  query  bot  and  outline  of  workﬂow  \n2.2.6  -  Multimodal  AI  Systems  \n2.2.6  -  Vision  Models  \n2.3  -  Prompt  Engineering  \n2.3.0  -  What  is  prompt  engineering?  \n2.3.1  -  Zero-shot  &  Few-shot  \n2.3.2  -  Chain-of-Thought  &  Tree-of-Thought  \n2.3.3  -  Designing  prompts  for  evaluation  [LLM  as  a  judge]  \n2.3.4  -  [Hands-On]  Design  zero-shot  and  few-shot  \nprompts\n \nusing\n \nAzure\n \nAI\n \n2.3.5  -  [Hands-On]  CoT  prompt  to  solve  a  math  problem  \n2.4  -  System  Design  \n2.4.1  -  The  7  Step  ML  System  Design  Framework  \n2.4.2  -  Pinterest  -  Visual  Search  ML  System  \n2.4.3  -  How  to  build  a  GenerativeAI  Platform?  \n2.5  -  The  Project  Lab  [Build-Deploy-Market]  \n[The  Project  Lab  -  02]  Building  LLM  from  Scr",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 7,
      "chunk": 0
    }
  },
  {
    "chunk_id": "20985129-9ac2-4604-bfc3-ae85b4c1e611",
    "text": "atch  \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n2.6  -  Interview  &  Resources  \nTechnical  Interview  Practice  Questions  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 7,
      "chunk": 1
    }
  },
  {
    "chunk_id": "824928de-2c10-4bd1-af82-422b81b044ee",
    "text": "Resources  \n \n \n \n \n \n[Module  -  3]  \nRetrieval-Augmented  Generation  (RAG)  \n3.1  -  RAG  Fundamentals  &  Workﬂow  \n3.1.1  -  What  is  RAG?  &  Workﬂow  \n3.1.2  -  Why  RAG  matters?  Overcoming  LLM  limitations  \n3.1.3  -  RAG  Architecture  \n3.1.4  -  [Hands-On]  RAG  demo  using  a  pre-built  tool  -  \nLangChain\n \n3.2  -  Embeddings  and  Vector  Databases  \n3.2.1  -  What  are  Vector  representations  of  Text  \n3.2.2  -  How  embeddings  work?  Word2Vec,  BERT  \n3.2.3  -  [Hands-On]  Generate  embeddings  for  sentences  \nusing\n \nHugging\n \nFace\n \n3.2.4  -  Vector  Database  Ecosystem  overview  -  ChromaDB,  \nPinecone,\n \nPostgres\n \nVector\n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 8,
      "chunk": 0
    }
  },
  {
    "chunk_id": "7eb55727-c269-449f-982f-75445eb059ac",
    "text": "3.2.5  -  [Hands-On]  Vector  database  with  Tesla  10-K  \nstatements\n \n3.3  -  Advanced  RAG  \n3.3.0  -  Reranking  and  Structured  Retrieval  \n3.3.1  -  [Hands-On]  Implement  a  basic  RAG  pipeline  \n3.3.2  -  Workﬂow  optimization  -  Balancing  retrieval  quality  \nand\n \ngeneration\n \ncoherence\n \n3.3.3  -  Evaluating  RAG  Outputs  \n3.3.4  -  [Hands-On]  Tesla  RAG  \n3.3.5  -  Hybrid  Search  \n3.3.6  -  RAG  evaluation  [RAGAS]  \n3.4  -  The  Project  Lab  [Build-Deploy-Market]  \n[The  Project  Lab  -  03]  Finance  Annual  Report  RAG  Q&A  \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n3.5  -  Interview  &  Resources  \nTechnical  Interview  Practice  Questions  \n \n \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 9,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5ee3d65b-af22-4dca-a59b-1be3a9c0407b",
    "text": " \n \n[Module  -  4]  \nFine-Tuning  LLMs  \n4.1  -  Fine-tuning  Fundamentals  \n4.1.1  -  Why  ﬁne-tune?  When  is  it  beneﬁcial?  \n4.1.2  -  How  transformers  enable  ﬁne-tuning  [Transfer  \nlearning\n \nprinciples]\n \n4.1.3  -  [Hands-On]  Preparing  Data  for  Fine-Tuning  \n4.1.4  -  [Hands-On]  Fine-tune  Mistral  7b  on  a  \ndomain-speciﬁc\n \ndataset\n \n4.2  -  Parameter-Efﬁcient  Fine-Tuning  (PEFT)  \n4.2.1  -  What  is  PEFT?  \n4.2.2  -  Low-Rank  Adaptation  [LoRA]  \n4.2.3  -  [Hands-On]  Fine-tune  Mistral  7b  with  LoRA  \n4.2.4  -  QLoRA  \n4.3  -  Evaluation  and  Deployment  \n4.3.0  -  Perplexity  [language  modeling],  BERTScore  \n[semantic\n \nsimilarity]\n \n4.3.1  -  [Hands-On]  Evaluating  Mistral  \n4.3.2  -  [Hands-On]  Fine-tuning  job  cost  estimate  using  \nAzure\n \nML\n \npricing",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 10,
      "chunk": 0
    }
  },
  {
    "chunk_id": "6022c538-3efd-4a34-8e50-79488d80066d",
    "text": "\n \ncalculator\n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 10,
      "chunk": 1
    }
  },
  {
    "chunk_id": "35da6c3c-b676-41f0-ad54-c437dce7ca8d",
    "text": "4.3.3  -  [Hands-On]  Deploy  the  ﬁne-tuned  Mistral  7b  locally  \n4.3.4  -  AI  Cost  Optimization  \n4.3.5  -  Quantization  \n4.3.5  -  [Hands-On]  Quantize  Supply  Chain  Forecaster  \n4.4  -  The  Project  Lab  [Build-Deploy-Market]  \n[The  Project  Lab  -  04]  Legal  QnA  -  Domain  Expert  LLM  \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n4.5  -  Interview  &  Resources  \nTechnical  Interview  Practice  Questions  \n \n \n \n \n \n \n \n \n \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 11,
      "chunk": 0
    }
  },
  {
    "chunk_id": "23696d33-2593-4379-aff8-1403c89396b6",
    "text": " \n \n[Module  -  5]  \nReinforcement  Learning  and  Ethical  AI  \n5.1  -  Reinforcement  Learning  with  Human  Feedback  \n(RLHF)\n \n5.1.1  -  What  is  RLHF?  \n5.1.2  -  Reward  model  &  policy  optimization  [PPO]  \n5.1.3  -  Limitations  -  Cost,  subjectivity,  scalability  \n5.1.4  -  [Hands-On]  Pre-trained  RLHF  model  vs  LLM  \n5.2  -  RLHF  Workﬂow  and  Implementation  \n5.2.1  -  RLHF  process  \n5.2.2  -  [Hands-On]  Simulate  a  RLHF  cycle  \n5.3  -  Ethical  and  Enterprise  Considerations  \n5.3.0  -  Bias  and  fairness  \n5.3.1  -  [Hands-On]  Gender  stereotypes  in  text  generation  \n5.3.2  -  Content  ﬁltering  \n5.3.3  -  [Hands-On]  Toxicity  ﬁlter  using  an  LLM  to  ﬂag  \nharmful\n \noutputs\n \n5.3.4  -  [Hands-On]  Create  a  Model  Card  \n5.4  -  The  Project  Lab  [Buil",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 12,
      "chunk": 0
    }
  },
  {
    "chunk_id": "4e920a13-9be7-4ac7-a642-36d9755f36d1",
    "text": "d-Deploy-Market]  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 12,
      "chunk": 1
    }
  },
  {
    "chunk_id": "e869bc30-cb35-4a2e-8fa8-4df33a319fae",
    "text": "[The  Project  Lab  -  05]  Ethical  Chatbot  \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n5.5  -  Interview  &  Resources  \nTechnical  Interview  Practice  Questions  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 13,
      "chunk": 0
    }
  },
  {
    "chunk_id": "14c55fb1-b88b-40d9-a31e-ef27806cbd1e",
    "text": " \n[Module  -  6]  \nAgentic  Workﬂows  \n6.1  -  Agentic  Patterns  \n6.1.1  -  What  are  agentic  workﬂows?  \n6.1.2  -  [Hands-On]  Building  First  AI  Agent  from  Scratch  \n6.1.3  -  What  is  reﬂection?  \n6.1.4  -  [Hands-On]  Build  a  reﬂection  agent  using  \nLangChain\n \n6.2  -  Tool  Use  -  Managing  Agentic  Memory  \n6.2.1  -  Memory  in  agents  \n6.2.2  -  [Hands-On]  Agent  with  MemGPT  to  manage  a  \nconversation\n \nhistory\n \n6.3  -  Tool  Use  -  Function  Calling  with  Agents  \n6.3.0  -  Function  calling  \n6.3.1  -  [Hands-On]  AI  agent  that  calls  a  Hugging  Face  API  \n6.4  -  Planning  with  Agents  [ReAct  Framework]  \n6.4.1  -  What  is  planning?  \n6.4.2  -  [Hands-On]  Implement  a  ReAct  agent  to  plan  a  \ntravel\n \nitinerary\n \n6.5  -  Multi-Agent  Collaborat",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 14,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5b6d1fbb-0db5-4bac-8594-35c93dbac3f9",
    "text": "ion  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 14,
      "chunk": 1
    }
  },
  {
    "chunk_id": "0a2da719-7915-46d4-9749-3e12c8a6f8c3",
    "text": "6.5.1  -  What  is  multi-agent  collaboration?  \n6.5.2  -  Models  -  Open  AI  Swarm  [triage],  Crew  AI  \n[ﬂow-based],\n \nLangGraph\n \n[graph-based]\n \n6.5.3  -  [Hands-On]  Two-agent  system  using  LangChain  \n6.5.4  -  [Hands-On]  Multi-agent  system  with  LangGraph  \nfor\n \na\n \nQ&A\n \ntask\n \n6.5.5  -  [Hands-On]  Autonomous  Systems  \n6.5.6  -  [Hands-On]  Reasoning  Fraud  Agent  \n6.5.7  -  Model  Context  Protocol  [MCP]  \n6.5.8  -  Agent-to-Agent  [A2A]  Protocol  \n6.6  -  The  Project  Lab  [Build-Deploy-Market]  \n[The  Project  Lab  -  06]  Travel  Booking  Agent  \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n6.7  -  Interview  &  Resources  \nTechnical  Interview  Practice  Questions  \n \n \n \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 15,
      "chunk": 0
    }
  },
  {
    "chunk_id": "ceba5c42-606c-48f2-8e3e-aec5a015d0f7",
    "text": " \n \n[Module  -  7]  \nCareer  Acceleration  \n7.1  -  Project  with  Mentoring  \n[The  Project  Lab  -  07]  \n[1  -  AI  Tutor  for  Education]  \n[2  -  AI  Tutor  for  Education]  RAG  +  Agentic  \n[3  -  AI  Tutor  for  Education]  Planning  agent  to  suggest  \nstudy\n \ntopics\n \n[4  -  AI  Tutor  for  Education]  Evaluate  the  tutor  with  \nqueries\n \n[Thus  Showcase]  -  Show  your  project  publicly  \n[Community/YouTube/GitHub]\n \n7.2  -  Portfolio  Building  \n7.2.1  -  GitHub  Proﬁle  &  Repositories  \n7.2.2  -  Personal  Website  Building  &  Deployment  \n7.3  -  Resume  and  Interview  Prep  \n7.3.1  -  Resume  Template  \n7.3.2  -  Resume  Checklist  \n7.3.3  -  Interview  Preparation  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 16,
      "chunk": 0
    }
  },
  {
    "chunk_id": "9f198dc2-5973-45a1-97fa-1047013e06c6",
    "text": "7.3.4  -  AI/ML  Interview  Questions  \n7.3.5  -  LLMs  Interview  Questions  \n7.3.6  -  Machine  Learning  Interview  Questions  \n7.4  -  Networking  \n7.4.1  -  Engaging  in  Following  AI  Communities  \n7.4.2  -  Follow  these  AI  Creators  on  LinkedIn  \n7.4.3  -  Follow  these  AI  Creators  on  YouTube  \n7.5  -  Personal  Branding  [Not  recommended  for  \nEveryone]\n \n7.5.1  -  LinkedIn  Proﬁle  Optimization  \n7.5.2  -  Sharing  your  work  Online  \n7.5.3  -  Cold  Out  Reach  to  Potential  Clients/Recruiters  \n \n \n \n \n \n \n \n \n \n ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 17,
      "chunk": 0
    }
  },
  {
    "chunk_id": "18d2e4c5-a036-47f5-9c6c-e6389f366ba9",
    "text": " \n[Miscellaneous]  \nBonus  \nNote:  These  bonus  bundles  will  not  be  available  anywhere  else,  \nbut\n \nonly\n \ninside\n \nthe\n \ncourse.\n \nAI  Job  Navigator  Toolkit  [$500  Value]  \nFreelance  AI  Proﬁt  Blueprint  [$2000  Value]  \nVIP  Masterclass  Pass  [$2000  Value]  \nPost-Course  Success  Playbook  [$1000  Value]  \nThe  Project  Lab  Bonus  Bundle  [Build-Deploy-Market]  \n[The  Project  Lab  -  08]  Healthcare  Symptom  Diagnostic  \nAgent\n \n[The  Project  Lab  -  09]  E-Commerce  Product  \nRecommendation\n \nEngine\n \n[The  Project  Lab  -  10]  Supply  Chain  Optimization  \nForecaster\n \n[The  Project  Lab  -  11]  Real-Time  Fraud  Detection  System  \nPro  Badge  \nAI  EngineerHQ  Challenge  \nDiscord  Study  Group  Sessions  \nDiscord  mini-Cohorts  and  Study  Groups  \nCertiﬁed  Accre",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 18,
      "chunk": 0
    }
  },
  {
    "chunk_id": "e653e1b5-d825-4e0b-8f55-10b08d66a45b",
    "text": "ditation  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 18,
      "chunk": 1
    }
  },
  {
    "chunk_id": "94bc307a-b18c-46a4-8f81-9ef79909ad8d",
    "text": "AI  EngineerHQ  Certiﬁed  Professional  \n[Hands-On]  Complete  the  Certiﬁcation  Process  \nPersonalized  Mentorship  \nScheduling  Your  1:1  Sessions  \n[Hands-On]  Project  Review  with  Mentor  \n[Career  Strategy  Session]  Job  applications  or  freelancing  \npitches\n \nExclusive  Industry  Access  \nAI  EngineerHQ  Job  Board  \nAI  Engineering  Use  Cases  Bundle  \nAI  Engineering  Case  Study  Hub  \n[Hands-On]  Crafting  a  Winning  Freelancing  Pitch  \nAdvanced  Tools  and  Resources  \nAzure  Credit  [In-process]  \nAI  EngineerHQ  Toolkit  \nLive  Masterclasses  with  Industry  Experts  \nGuest  Speaker  Series  \nPost-Course  Support  \nCommunity  Access  [Discord  weekly  QnA  Calls]  \nRevenue  Generating  Projects  \nMonetizing  Your  Portfolio  [Projects  Into  SaaS]  ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 19,
      "chunk": 0
    }
  },
  {
    "chunk_id": "f7c1c008-e7ca-4d81-823b-7b0735acf4e4",
    "text": "About  Me\n \n \nI’m  Himanshu  Ramchandani,  I  am  from  India.  \nMicrosoft  MVP  \nI  am  an  AI  Consultant  with  close  to  a  decade  of  experience.  \nI  worked  on  over  100  Data  &  AI  projects  in  Energy,  Healthcare,  Law  \nEnforcement\n \n&\n \nDefense.\n \nI  am  the  Founder  of  an  AI  engineering  &  Consulting  company  -  Dextar.  \nI  focus  on  action-oriented  AI  leadership  &  engineering  implementation  \ndrills.\n \nI  provide  AI  Engineering  &  Leadership  training  to  teams  through  AI  \nEngineer\n \nHQ\n \nand\n \nThe\n \nElite\n \n[AI\n \nLeadership\n \nAccelerator]\n \nIn  the  last  decade,  I  have  never  stopped  sharing  my  knowledge  and  \nhave\n \nhelped\n \nover\n \n10000\n \nleaders,\n \nprofessionals,\n \nand\n \nstudents.\n \n \n \n \n \n",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 21,
      "chunk": 0
    }
  },
  {
    "chunk_id": "5fa8d8f7-8620-4784-92cc-1a9f787b9c4e",
    "text": " Detailed  Roadmap:  https://god-level-python.notion.site/AI-Engineer-HQ-b3c98407b4ab45819811db081ae9d102?pvs=4  AI  Newsletter:  \nhttps://newsletter.himanshuramchandani.co/  \n Join  Telegram:  \nhttps://t.me/+sREuRiFssMo4YWJl \n \nJoin  the  Discord  Community:   \nhttps://discord.gg/q3svy4VEEs ",
    "metadata": {
      "filename": "AI Engineer Roadmap 2025.pdf",
      "page": 22,
      "chunk": 0
    }
  },
  {
    "chunk_id": "1d02e7db-6b61-4d27-a3ca-61efa00e86df",
    "text": "The Indus Valley Civilization was one of the earliest urban cultures.\r\nIt flourished around 3300–1300 BCE in present-day India and Pakistan.\r\nMajor cities included Harappa and Mohenjo-Daro.\r\n",
    "metadata": {
      "filename": "history_notes.txt",
      "page": 1,
      "chunk": 0
    }
  }
]